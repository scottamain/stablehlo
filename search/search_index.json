{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#stablehlo","title":"StableHLO","text":"<p>StableHLO is an operation set that expresses ML computations. It has been originally bootstrapped from the MHLO dialect and enhances it with additional functionality, including serialization and versioning.</p> <p>StableHLO is a portability layer between ML frameworks and ML compilers. We are aiming for adoption by a wide variety of ML frameworks including TensorFlow, JAX and PyTorch, as well as ML compilers including XLA and IREE.</p>"},{"location":"#development","title":"Development","text":"<p>We're using GitHub issues / pull requests to organize development and GitHub discussions to have longer discussions. We also have a <code>#stablehlo</code> channel on the OpenXLA Discord server.</p>"},{"location":"#community","title":"Community","text":"<p>With StableHLO, our goal is to create a community to build an amazing portability layer between ML frameworks and ML compilers. Let's work together on figuring out the appropriate governance to make this happen.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Workstream #1: Stable version of HLO/MHLO, including   the spec,   the corresponding dialect with high-quality implementations of   prettyprinting,   verification and   type inference,   and the interpeter.   ETA: H2 2022.</li> <li>Workstream #2: Evolution beyond what's currently in HLO/MHLO.   Ongoing work on dynamism,   sparsity, quantization and extensibility. ETA: H2 2022.</li> <li>Workstream #3: Support for ML frameworks (TensorFlow, JAX, PyTorch) and   ML compilers (XLA and IREE). ETA: H2 2022.</li> </ul>"},{"location":"bytecode/","title":"StableHLO Bytecode","text":""},{"location":"bytecode/#currently-encoded-attributes-types","title":"Currently Encoded Attributes / Types","text":""},{"location":"bytecode/#stablehlo-attributes-and-types","title":"StableHLO Attributes and Types","text":"<p>Documentation on the structure of the encoded attributes and types can be found in the following code comments:</p> <p>Attributes: See <code>stablehlo_encoding::AttributeCode</code> in <code>StablehloBytecode.cpp</code> [link]</p> <p>Types: See <code>stablehlo_encoding::TypeCode</code> in <code>StablehloBytecode.cpp</code> [link]</p>"},{"location":"bytecode/#chlo-attributes-and-types","title":"CHLO Attributes and Types","text":"<p>Documentation on the structure of the encoded attributes and types can be found in the following code comments:</p> <p>Attributes: See <code>chlo_encoding::AttributeCode</code> in <code>ChloBytecode.cpp</code> [link]</p> <p>Types: See <code>chlo_encoding::TypeCode</code> in <code>ChloBytecode.cpp</code> [link]</p>"},{"location":"bytecode/#not-included","title":"Not Included","text":"<p>The following attributes / types are subclasses of builtin machinery and call into the bytecode implementations in the Builtin Dialect.</p> <ul> <li><code>StableHLO_ArrayOfLayoutAttr</code></li> <li><code>StableHLO_BoolElementsAttr</code></li> <li><code>StableHLO_FlatSymbolRefArrayAttr</code></li> <li><code>StableHLO_LayoutAttr</code></li> <li><code>HLO_ComplexTensor</code></li> <li><code>HLO_Complex</code></li> <li><code>HLO_DimensionTensor</code></li> <li><code>HLO_DimensionValue</code></li> <li><code>HLO_Float32Or64</code></li> <li><code>HLO_Float</code></li> <li><code>HLO_Fp32Or64Tensor</code></li> <li><code>HLO_FpOrComplexTensor</code></li> <li><code>HLO_FpTensor</code></li> <li><code>HLO_IntFpOrComplexTensor</code></li> <li><code>HLO_IntOrFpTensor</code></li> <li><code>HLO_IntTensor</code></li> <li><code>HLO_Int</code></li> <li><code>HLO_PredIntOrFpTensor</code></li> <li><code>HLO_PredOrIntTensor</code></li> <li><code>HLO_PredTensor</code></li> <li><code>HLO_Pred</code></li> <li><code>HLO_QuantizedIntTensor</code></li> <li><code>HLO_QuantizedInt</code></li> <li><code>HLO_QuantizedSignedInt</code></li> <li><code>HLO_QuantizedUnsignedInt</code></li> <li><code>HLO_SInt</code></li> <li><code>HLO_ScalarIntTensor</code></li> <li><code>HLO_StaticShapeTensor</code></li> <li><code>HLO_TensorOrTokenOrTuple</code></li> <li><code>HLO_TensorOrToken</code></li> <li><code>HLO_Tensor</code></li> <li><code>HLO_Tuple</code></li> <li><code>HLO_UInt</code></li> </ul> <p>Special Cases:</p> <ul> <li><code>StableHLO_ConvolutionAttributes</code></li> <li>Despite its name,  is not an attribute and is not encoded.     Rather, it is a dag which gets expanded into several attributes     which are all encoded separately.</li> <li><code>StableHLO_CustomCallApiVersionAttr</code></li> <li>This enum is defined strictly as an attribute of <code>I32EnumAttr</code>     and not an <code>EnumAttr</code> of the <code>StablehloDialect</code>. This differs from    <code>FftType</code> and other enum attributes. Because of this, it is handled by     the builtin encoding.</li> </ul>"},{"location":"bytecode/#other-notes","title":"Other Notes","text":""},{"location":"bytecode/#testing-bytecode-with-round-trips","title":"Testing Bytecode with Round Trips","text":"<p>Testing that the round-trip of an MLIR file produces the same results is a good way to test that the bytecode is implemented properly.</p> <pre><code>stablehlo-opt -emit-bytecode stablehlo/tests/print_stablehlo.mlir | stablehlo-opt\n</code></pre>"},{"location":"bytecode/#find-out-what-attributes-or-types-are-not-encoded","title":"Find out what attributes or types are not encoded","text":"<p>Since attributes and types that don't get encoded are instead stored as strings, the <code>strings</code> command can be used to see what attributes were missed:</p> <p>Note: Currently all types/attrs are implemented and log only shows the dialect name <code>stablehlo</code> and the unregistered <code>stablehlo.frontend_attributes</code> and <code>stablehlo.sharding</code> attributes.</p> <pre><code>$ stablehlo-opt -emit-bytecode file.mlir | strings | grep stablehlo\nstablehlo\nstablehlo.frontend_attributes\nstablehlo.sharding\n</code></pre>"},{"location":"bytecode/#debugging-bytecode-with-traces","title":"Debugging Bytecode with Traces","text":"<p>Each read/write function called during bytecoding is traced, and can be viewed using the flag <code>-debug-only=stablehlo-bytecode</code> for StableHLO and <code>-debug-only=chlo-bytecode</code> for CHLO.</p> <pre><code>$ stablehlo-opt -emit-bytecode -debug-only=stablehlo-bytecode ../tmp.mlir\nCalled: writeType(mlir::Type, mlir::DialectBytecodeWriter &amp;)::(anonymous class)::operator()(auto) const [type:auto = mlir::stablehlo::TokenType]\nCalled: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &amp;)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::TransposeAttr]\nCalled: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &amp;)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::RngAlgorithmAttr]\nCalled: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &amp;)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::ChannelHandleAttr]\nCalled: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &amp;)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::ChannelHandleAttr]\nCalled: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &amp;)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::TypeExtensionsAttr]\n...\n\n$ stablehlo-opt -emit-bytecode -debug-only=stablehlo-bytecode bytecoded_file.mlir\nCalled: readComparisonDirectionAttr(mlir::DialectBytecodeReader &amp;) const\nCalled: readTypeExtensionsAttr(mlir::DialectBytecodeReader &amp;) const\nCalled: readChannelHandleAttr(mlir::DialectBytecodeReader &amp;) const\nCalled: readChannelHandleAttr(mlir::DialectBytecodeReader &amp;) const\nCalled: readRngAlgorithmAttr(mlir::DialectBytecodeReader &amp;) const\n</code></pre>"},{"location":"bytecode/#adding-bytecode-for-a-new-type-attribute","title":"Adding Bytecode for a New Type / Attribute","text":"<p>Adding bytecode for a new type or attribute is simple. In the file <code>StablehloBytecode.cpp</code> or <code>ChloBytecode.cpp</code> search for the term <code>TO ADD ATTRIBUTE</code> or <code>TO ADD TYPE</code> depending on the change. Ensure that each location tagged with <code>TO ADD</code> instructions is addressed. If so, bytecode for the attr/type should be generated on next call to <code>stablehlo-opt -emit-bytecode</code>. This can be verified using the proper bytecode trace.</p>"},{"location":"bytecode/#encoding-enum-class-values","title":"Encoding <code>enum class</code> values","text":"<p>Enum class values can be encoded as their underlying numeric types using <code>varint</code>. Currently all enums in StableHLO use <code>uint32_t</code> as the underlying value.</p>"},{"location":"governance/","title":"StableHLO Governance","text":""},{"location":"governance/#future-governance","title":"Future governance","text":"<p>We aim to establish an open governance model drawing from standards such as LLVM, with particular emphasis on open design/roadmap discussions, public process for gaining technical steering rights, and OSS-first docs &amp; repo governance (e.g. location, CLA, etc), repo location.</p>"},{"location":"governance/#near-term-governance","title":"Near-term governance","text":"<p>During the bootstrapping phase of the project in 2022, Google engineers will assume responsibility for the technical leadership of the project. It is a high priority to create a path for community members to take on technical leadership roles.</p>"},{"location":"reference/","title":"Interpreter Design","text":""},{"location":"reference/#data-model","title":"Data Model","text":"<p>StableHLO programs are computations over tensors (n-dimensional arrays), which, in the current model, are implemented using class <code>Tensor</code>. The underlying storage class for a <code>Tensor</code> object, <code>detail::Buffer</code>, stores the <code>mlir::ShapedType</code> of the tensor along with a <code>mlir::HeapAsmResourceBlob</code> object representing a mutable blob of tensor data laid out as contiguous byte array in major-to-minor order. <code>detail::Buffer</code> objects are reference-counted to simplify memory management.</p> <p>Individual elements of a tensor are represented using <code>Element</code> class which uses discriminated union holding one of <code>APInt</code>, <code>APFloat</code> or <code>pair&lt;APFloat,APFloat&gt;</code> for storage. The last one is used for storing elements with complex types.</p> <p><code>Tensor</code> class has the following APIs to interact with its individual elements:</p> <ul> <li><code>Element Tensor::get(llvm::ArrayRef&lt;int64_t&gt; index)</code>: To extract an      individual tensor element at multi-dimensional index <code>index</code> as <code>Element</code>      object.</li> <li><code>void Tensor::set(llvm::ArrayRef&lt;int64_t&gt; index, Element element);</code>:   To update an <code>Element</code> object <code>element</code> into a tensor at multi-dimensional   index <code>index</code>.</li> </ul>"},{"location":"reference/#working-of-the-interpreter","title":"Working of the interpreter","text":"<p>The entry function to the interpreter is</p> <pre><code>SmallVector&lt;Tensor&gt; eval(func::FuncOp func, ArrayRef&lt;Tensor&gt; args);\n</code></pre> <p>which does the following:</p> <ol> <li>Tracks the SSA arguments of <code>func</code> and their associated runtime <code>Tensor</code>    values, provided in <code>args</code>, using a symbol table map, M.</li> <li>Foreach op within <code>func</code> in their SSACFG order:</li> <li>Invokes <code>eval</code> on op. For each SSA operand of the op, extract its      runtime value from M to be provided as argument to the <code>eval</code> invocation.</li> <li>Tracks the SSA result(s) of the op and the evaluated value in M.</li> </ol> <p>The op-level <code>eval</code> as mentioned in (2) is responsible for implementing the execution semantics of the op. Following is an example for <code>stablehlo::AddOp</code>. In the example, individual elements of the <code>lhs</code> and <code>rhs</code> tensors are pairwise extracted as <code>Element</code> objects which are then added. The result of the addition, an <code>Element</code> object, is stored in the final <code>result</code> tensor.</p> <pre><code>Tensor eval(AddOp op, const Tensor &amp;lhs, const Tensor &amp;rhs) {\n  Tensor result(op.getType());\n\n  for (auto it = result.index_begin(); it != result.index_end(); ++it)\n    result.set(*it, lhs.get(*it) + rhs.get(*it));\n\n  return result;\n}\n</code></pre> <p>Overall, the design of the interpreter is optimized for readability of implementations of <code>eval</code> functions for individual ops because it's meant to serve as a reference implementation for StableHLO. For example, instead of defining <code>eval</code> as a template function and parameterizing it with element types, we encapsulate details about how different element types are handled in <code>Element::operator+</code> etc, simplifying the implementation of <code>eval</code>.</p>"},{"location":"reference/#using-interpreter-for-constant-folding","title":"Using interpreter for constant folding","text":"<p>We can use the interpreter mechanism to fold operations with constant operand values. The following code snippet demonstrates an idea of the implementation for folding <code>stablehlo::AddOp</code> with floating-point typed operands:</p> <pre><code>OpFoldResult AddOp::fold(ArrayRef&lt;Attribute&gt; attrs) {\n  DenseElementsAttr lhsData = attrs[0].dyn_cast&lt;DenseElementsAttr&gt;();\n  DenseElementsAttr rhsData = attrs[1].dyn_cast&lt;DenseElementsAttr&gt;();\n  if (!lhsData || !rhsData) return {};\n\n  auto lhs = Tensor(lhsData);\n  auto rhs = Tensor(rhsData);\n  auto result = eval(*this, lhs, rhs);\n\n  SmallVector&lt;APFloat&gt; values;\n  for (auto i = 0; i &lt; result.getNumElements(); ++i) {\n    Element element = result.get(i);\n    values.push_back(element.getValue().cast&lt;FloatAttr&gt;().getValue());\n  }\n\n  return DenseElementsAttr::get(result.getType(), values);\n}\n</code></pre> <p>At the moment, we aren't actively working on integrating the interpreter into constant folding because we aren't planning to implement folder for StableHLO. However, in the future, we are planning to leverage the interpreter for constant folding in MHLO, at which point we'll improve ergonomics of the code snippet above (e.g. we could have a helper function which packs constant operands into <code>Tensor</code> objects and unpacks <code>Tensor</code> results into <code>OpFoldResult</code>).</p>"},{"location":"reference/#testing-the-interpreter","title":"Testing the interpreter","text":"<p>The interpreter takes as inputs (A) a StableHLO program, and (B) data values to be fed to the program, and generates output data values, which are matched against the user-provided expected data values.</p> <p>In the current implementation, we package the inputs (MLIR program + input data values) and outputs in a lit-based test as follows:</p> <pre><code>// CHECK-LABEL: Evaluated results of function: add_op_test_ui4\nfunc.func @add_op_test_ui4() -&gt; tensor&lt;2xui4&gt; {\n  %0 = stablehlo.constant dense&lt;[0, 2]&gt; : tensor&lt;2xui4&gt;\n  %1 = stablehlo.constant dense&lt;[15, 3]&gt; : tensor&lt;2xui4&gt;\n  %2 = stablehlo.add %0, %1 : tensor&lt;2xui4&gt;\n  func.return %2 : tensor&lt;2xui4&gt;\n  // CHECK-NEXT:  tensor&lt;2xui4&gt;\n  // CHECK-NEXT:    15 : ui4\n  // CHECK-NEXT:    5 : ui4\n}\n</code></pre> <p>A test utility <code>stablehlo-interpreter</code> (code) is responsible for parsing the program, interpreting each function, and returning the resulting tensor(s) to be matched against the output tensor provided in FileCheck directives. We have a dedicated test-suite, consisting of several tests exercising various runtime behaviors, for each StableHLO Op. The tests can be found here (e.g. interpret_*.mlir).</p>"},{"location":"spec/","title":"StableHLO Specification","text":"<p>StableHLO is an operation set for high-level operations (HLO) in machine learning (ML) models. StableHLO works as a portability layer between different ML frameworks and ML compilers: ML frameworks that produce StableHLO programs are compatible with ML compilers that consume StableHLO programs.</p> <p>Our goal is to simplify and accelerate ML development by creating more interoperability between various ML frameworks (such as TensorFlow, JAX and PyTorch) and ML compilers (such as XLA and IREE). Towards that end, this document provides a specification for the StableHLO programming language.</p> <p>This specification contains three major sections. First, the \"Programs\" section describes the structure of StableHLO programs which consist of StableHLO functions which themselves consist of StableHLO ops. Within that structure, the \"Ops\" section specifies semantics of individual ops. Finally, the \"Execution\" section provides semantics for all these ops executing together within a program.</p>"},{"location":"spec/#programs","title":"Programs","text":"<pre><code>Program ::= {Func}\n</code></pre> <p>StableHLO programs consist of an arbitrary number of StableHLO functions. Below is an example program with a function <code>@main</code> which has 3 inputs (<code>%image</code>, <code>%weights</code> and <code>%bias</code>) and 1 output. The body of the function has 6 ops.</p> <pre><code>stablehlo.func @main(\n  %image: tensor&lt;28x28xf32&gt;,\n  %weights: tensor&lt;784x10xf32&gt;,\n  %bias: tensor&lt;1x10xf32&gt;\n) -&gt; tensor&lt;1x10xf32&gt; {\n  %0 = \"stablehlo.reshape\"(%image) : (tensor&lt;28x28xf32&gt;) -&gt; tensor&lt;1x784xf32&gt;\n  %1 = \"stablehlo.dot\"(%0, %weights) : (tensor&lt;1x784xf32&gt;, tensor&lt;784x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;\n  %2 = \"stablehlo.add\"(%1, %bias) : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;\n  %3 = \"stablehlo.constant\"() { value = dense&lt;0.0&gt; : tensor&lt;1x10xf32&gt; } : () -&gt; tensor&lt;1x10xf32&gt;\n  %4 = \"stablehlo.maximum\"(%2, %3) : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;\n  \"stablehlo.return\"(%4): (tensor&lt;1x10xf32&gt;) -&gt; ()\n}\n</code></pre>"},{"location":"spec/#functions","title":"Functions","text":"<pre><code>Func        ::= 'stablehlo' '.' 'func' FuncId FuncInputs FuncOutputs '{' FuncBody '}'\nFuncInputs  ::= '(' [FuncInput {',' FuncInput}] `)`\nFuncInput   ::= '%' ValueId ':' ValueType\nFuncOutputs ::= ['-&gt;' FuncOutput, {',' FuncOutput}]\nFuncOutput  ::= ValueType\nFuncBody    ::= {Op}\n</code></pre> <p>StableHLO functions (which are also called named functions) have an identifier, inputs/outputs and a body. In the future, we are planning to introduce additional metadata for functions to achieve better compatibility with HLO (#425).</p>"},{"location":"spec/#identifiers","title":"Identifiers","text":"<pre><code>FuncId  ::= '@' letter {letter | digit}\nValueId ::= '%' digit {digit}\n          | '%' letter {letter | digit}\nletter  ::= 'a' | ... | 'z' | 'A' | ... | 'Z' | '_'\ndigit   ::= '0' | ... | '9'\n</code></pre> <p>StableHLO identifiers are similar to identifiers in many programming languages, with two peculiarities: 1) all identifiers have sigils which distinguish different kinds of identifiers, 2) value identifiers can be completely numeric to simplify generation of StableHLO programs.</p>"},{"location":"spec/#types","title":"Types","text":"<pre><code>Type         ::= ValueType | NonValueType\nValueType    ::= TensorType | TokenType | TupleType\nNonValueType ::= ElementType | FunctionType | StringType\n</code></pre> <p>StableHLO types are categorized into value types (which are also called first-class types) which represent StableHLO values and non-value types which describe other program elements. StableHLO types are similar to types in many programming languages, with the main peculiarity being StableHLO's domain-specific nature which results in some unusual outcomes (e.g. scalar types are not value types).</p> <pre><code>TensorType    ::= 'tensor' '&lt;' TensorShape ElementType '&gt;'\nTensorShape   ::= {DimensionSize 'x'}\nDimensionSize ::= digit {digit}\n</code></pre> <p>Tensor types represent tensors, i.e. multidimensional arrays. They have a shape and an element type, where a shape represents non-negative dimension sizes in the ascending order of the corresponding dimensions (which are also called axes) numbered from <code>0</code> to <code>R-1</code>. The number of dimensions <code>R</code> is called rank. For example, <code>tensor&lt;2x3xf32&gt;</code> is a tensor type with shape <code>2x3</code> and element type <code>f32</code>. It has two dimensions (or, in other words, two axes) - 0th dimension and 1st dimension - whose sizes are 2 and 3. Its rank is 2.</p> <pre><code>TokenType ::= 'token'\n</code></pre> <p>Token types represent tokens, i.e. opaque values produced and consumed by some operations. Tokens are used for imposing execution order on operations as described in the \"Execution\" section.</p> <pre><code>TupleType ::= 'tuple' '&lt;' [ValueType {',' ValueType}] '&gt;'\n</code></pre> <p>Tuple types represent tuples, i.e. heterogeneous lists. Tuples are a legacy feature which only exists for compatibility with HLO. In HLO, tuples are used to represent variadic inputs and outputs. In StableHLO, variadic inputs and outputs are supported natively, and the only use of tuples in StableHLO is to comprehensively represent HLO ABI where e.g. <code>T</code>, <code>tuple&lt;T&gt;</code> and <code>tuple&lt;tuple&lt;T&gt;&gt;</code> may be materially different depending on a particular implementation.</p> <pre><code>ElementType ::= BooleanType | IntegerType | FloatType | ComplexType\nBooleanType ::= 'i1'\nIntegerType ::= 'si4' | 'si8' | 'si16' | 'si32' | 'si64'\n              | 'ui4' | 'ui8' | 'ui16' | 'ui32' | 'ui64'\nFloatType   ::= 'f8E4M3FN' | 'f8E5M2' | 'bf16' | 'f16' | 'f32' | 'f64'\nComplexType ::= 'complex' '&lt;' ('f32' | 'f64') '&gt;'\n</code></pre> <p>Element types represent elements of tensor types. Unlike in many programming languages, these types are not first class in StableHLO. This means that StableHLO programs cannot directly represent values of these types (as a result, it is idiomatic to represent scalar values of type <code>T</code> with 0-dimensional tensor values of type <code>tensor&lt;T&gt;</code>).</p> <ul> <li>Boolean type represents boolean values <code>true</code> and <code>false</code>.</li> <li>Integer types can be either signed (<code>si</code>) or unsigned (<code>ui</code>) and have     one of the supported bit widths (<code>4</code>, <code>8</code>, <code>16</code>, <code>32</code> or <code>64</code>).     Signed <code>siN</code> types represent integer values from <code>-2^(N-1)</code> to <code>2^(N-1)-1</code>     inclusive, and unsigned <code>uiN</code> types represent integer values from <code>0</code> to     <code>2^N-1</code> inclusive.</li> <li>Floating-point types can be one of the following:<ul> <li><code>f8E4M3FN</code> and <code>f8E5M2</code> types corresponding to respectively the <code>E4M3</code> and <code>E5M2</code> encodings of the FP8 format described in FP8 Formats for Deep Learning.</li> <li><code>bf16</code> type corresponding to the <code>bfloat16</code> format described in BFloat16: The secret to high performance on Cloud TPUs.</li> <li><code>f16</code>, <code>f32</code> and <code>f64</code> types corresponding to respectively <code>binary16</code> (\"half precision\"), <code>binary32</code> (\"single precision\") and <code>binary64</code> (\"double precision\") formats described in the IEEE 754 standard.</li> </ul> </li> <li>Complex types represent complex values that have a real part     and an imaginary part of the same element type. Supported complex     types are <code>complex&lt;f32&gt;</code> (both parts are of type <code>f32</code>) and <code>complex&lt;f64&gt;</code>     (both parts are of type <code>f64</code>).</li> </ul> <pre><code>FunctionType ::= '(' [ValueType {',' ValueType}] ')' '-&gt;' '(' [ValueType {',' ValueType}] ')'\n</code></pre> <p>Function types represent both named and anonymous functions. They have input types (the list of types on the left-hand side of <code>-&gt;</code>) and output types (the list of types on the right-hand side of <code>-&gt;</code>). In many programming languages, function types are first class, but not in StableHLO.</p> <pre><code>StringType ::= 'string'\n</code></pre> <p>String type represents sequences of bytes. Unlike in many programming languages, string type is not first class in StableHLO and is only used to specify static metadata for program elements.</p>"},{"location":"spec/#operations","title":"Operations","text":"<p>StableHLO operations (which are also called ops) represent a closed set of high-level operations in machine learning models. As discussed above, StableHLO syntax is heavily inspired by MLIR, which is not necessarily the most ergonomic alternative, but is arguably the best fit for StableHLO's goal of creating more interoperability between ML frameworks and ML compilers.</p> <pre><code>Op            ::= [OpOutputs] OpName OpInputs ':' OpSignature\nOpName        ::= '\"' 'stablehlo' '.' OpMnemonic '\"'\nOpMnemonic    ::= 'abs' | 'add' | ...\n</code></pre> <p>StableHLO operations (which are also called ops) have a name, inputs/outputs and a signature. The name consists of the <code>stablehlo.</code> prefix and a mnemonic which uniquely identifies one of the supported ops. See below for a comprehensive list of all supported ops.</p> <pre><code>OpInputs        ::= OpInputValues OpInputFuncs OpInputAttrs\nOpInputValues   ::= '(' [OpInputValue {',' OpInputValue}] ')'\nOpInputValue    ::= ValueId\nOpInputFuncs    ::= ['(' OpInputFunc {',' OpInputFunc} ')']\nOpInputAttrs    ::= ['{' OpInputAttr {',' OpInputAttr} '}']\nOpOutputs       ::= [OpOutput {',' OpOutput} '=']\nOpOutput        ::= ValueId\n</code></pre> <p>Ops consume inputs and produce outputs. Inputs are categorized into input values (computed during execution), input functions (provided statically, because in StableHLO functions are not first-class values) and input attributes (also provided statically). The kind of inputs and outputs consumed and produced by an op depends on its mnemonic. For example, the <code>add</code> op consumes 2 input values and produces 1 output value. In comparison, the <code>select_and_scatter</code> op consumes 3 input values, 2 input functions and 3 input attributes.</p> <pre><code>OpInputFunc ::= '{' Unused FuncInputs ':' FuncBody '}'\nUnused      ::= '^' digit {digit}\n              | '^' letter {letter | digit}\n</code></pre> <p>Input functions (which are also called anonymous functions) are very similar to named functions except that: 1) they don't have an identifier (hence the name \"anonymous\"), 2) they don't declare output types (output types are inferred from the <code>return</code> op within the function).</p> <p>The syntax for input functions includes a currently unused part (see the <code>Unused</code> production above) which is there for compatibility with MLIR. In MLIR, there is a more general concept of \"regions\" which can have multiple \"blocks\" of ops connected together via jump ops. These blocks have ids which correspond to the <code>Unused</code> production, so that they can be distinguished from each other. StableHLO doesn't have jump ops, so the corresponding part of MLIR syntax is unused (but is still there).</p> <pre><code>OpInputAttr      ::= OpInputAttrName '=' OpInputAttrValue\nOpInputAttrName  ::= letter {letter | digit}\nOpInputAttrValue ::= Constant\n</code></pre> <p>Input attributes have a name and a value which is one of the supported constants. They are the primary way to specify static metadata for program elements. For example, the <code>concatenate</code> op uses the attribute <code>dimension</code> to specify the dimension along which its input values are concatenated. Similarly, the <code>slice</code> op uses multiple attributes like <code>start_indices</code> and <code>limit_indices</code> to specify the bounds that are used to slice the input value.</p> <pre><code>OpSignature ::= '(' [ValueType {',' ValueType}] ')' '-&gt;' '(' [ValueType {',' ValueType}] ')'\n</code></pre> <p>Op signature consists of the types of all input values (the list of types on the left-hand side of <code>-&gt;</code>) and the types of all output values (the list of types on the right-hand side of <code>-&gt;</code>). Strictly speaking, input types are redundant, and output types are almost always redundant as well (because for most StableHLO ops, output types can be inferred from inputs). Nonetheless, op signature is deliberately part of StableHLO syntax for compatibility with MLIR.</p> <p>Below is an example op whose mnemonic is <code>select_and_scatter</code>. It consumes 3 input values (<code>%operand</code>, <code>%source</code> and <code>%init_value</code>), 2 input functions and 3 input attributes (<code>window_dimensions</code>, <code>window_strides</code> and <code>padding</code>). Note how the signature of the op only includes the types of its input values (but not the types of input functions and attributes which are provided inline).</p> <pre><code>%result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.compare\"(%arg0, %arg1) {\n      comparison_direction = #stablehlo&lt;comparison_direction GE&gt;\n    } : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i1&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i1&gt;) -&gt; ()\n}, {\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i32&gt;) -&gt; ()\n}) {\n  window_dimensions = dense&lt;[3, 1]&gt; : tensor&lt;2xi64&gt;,\n  window_strides = dense&lt;[2, 1]&gt; : tensor&lt;2xi64&gt;,\n  padding = dense&lt;[[0, 1], [0, 0]]&gt; : tensor&lt;2x2xi64&gt;\n} : (tensor&lt;4x2xi32&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;4x2xi32&gt;\n</code></pre>"},{"location":"spec/#constants","title":"Constants","text":"<pre><code>Constant ::= BooleanConstant\n           | IntegerConstant\n           | FloatConstant\n           | ComplexConstant\n           | TensorConstant\n           | StringConstant\n           | EnumConstant\n</code></pre> <p>StableHLO constants have a literal and a type which together represent a StableHLO value. Generally, the type is part of the constant syntax, except when it's unambiguous (e.g. a boolean constant unambiguously has type <code>i1</code>, whereas an integer constant can have multiple possible types).</p> <pre><code>BooleanConstant ::= BooleanLiteral\nBooleanLiteral  ::= 'true' | 'false'\n</code></pre> <p>Boolean constants represent boolean values <code>true</code> and <code>false</code>. Boolean constants have type <code>i1</code>.</p> <pre><code>IntegerConstant   ::= IntegerLiteral ':' IntegerType\nIntegerLiteral    ::= ['-' | '+'] DecimalDigits\n                    | ['-' | '+'] '0x' HexadecimalDigits\nDecimalDigits     ::= decimalDigit {decimalDigit}\nHexadecimalDigits ::= hexadecimalDigit {hexadecimalDigit}\ndecimalDigit      ::= '0' | ... | '9'\nhexadecimalDigit  ::= decimalDigit | 'a' | ... | 'f' | 'A' | ... | 'F'\n</code></pre> <p>Integer constants represent integer values via strings that use decimal or hexadecimal notation. Other bases, e.g. binary or octal, are not supported. Integer constants have the following constraints:</p> <ul> <li>(C1) <code>is_wellformed(literal, type)</code>, i.e. <code>literal</code> can be parsed as     a value of type <code>type</code>.</li> </ul> <pre><code>FloatConstant  ::= FloatLiteral ':' FloatType\nFloatLiteral   ::= SignPart IntegerPart FractionalPart ScientificPart\n                 | '0x' [HexadecimalDigits]\nSignPart       ::= ['-' | '+']\nIntegerPart    ::= DecimalDigits\nFractionalPart ::= ['.' [DecimalDigits]]\nScientificPart ::= [('e' | 'E') ['-' | '+'] DecimalDigits]\n</code></pre> <p>Floating-point constants represent floating-point values via strings that use decimal or scientific notation. Additionally, hexadecimal notation can be used to directly specify the underlying bits in the floating-point format of the corresponding type. Floating-point constants have the following constraints:</p> <ul> <li>(C1) If non-hexadecimal notation is used, <code>is_wellformed(literal, type)</code>.</li> <li>(C2) If hexadecimal notation is used,     <code>size(literal) = num_bits(type) / 4 + 2</code>.</li> </ul> <pre><code>ComplexConstant      ::= ComplexLiteral ':' ComplexType\nComplexLiteral       ::= '(' ComplexRealPart ',' ComplexImaginaryPart ')'\nComplexRealPart      ::= FloatLiteral\nComplexImaginaryPart ::= FloatLiteral\n</code></pre> <p>Complex constants represent complex values using lists of a real part (comes first) and an imaginary part (comes second). For example, <code>(1.0, 0.0) : complex&lt;f32&gt;</code> represents <code>1.0 + 0.0i</code>, and <code>(0.0, 1.0) : complex&lt;f32&gt;</code> represents <code>0.0 + 1.0i</code>. The order in which these parts are then stored in memory is implementation-defined. Complex constants have the following constraints:</p> <ul> <li>(C1) <code>is_wellformed(literal[:], element_type(type))</code>.</li> </ul> <pre><code>TensorConstant ::= TensorLiteral ':' TensorType\nTensorLiteral  ::= 'dense' '&lt;' (DenseLiteral | ElementLiteral) '&gt;'\nDenseLiteral   ::= DenseDimension | DenseElements\nDenseDimension ::= '[' [DenseLiteral {',' DenseLiteral}] ']'\nDenseElements  ::= [ElementLiteral {',' ElementLiteral}]\nElementLiteral ::= BooleanLiteral | IntegerLiteral | FloatLiteral | ComplexLiteral\n</code></pre> <p>Tensor constants represent tensor values using nested lists specified via NumPy notation. For example, <code>dense&lt;[[1, 2, 3], [4, 5, 6]]&gt; : tensor&lt;2x3xi32&gt;</code> represents a tensor value with the following mapping from indices to elements: <code>{0, 0} =&gt; 1</code>, <code>{0, 1} =&gt; 2</code>, <code>{0, 2} =&gt; 3</code>, <code>{1, 0} =&gt; 4</code>, <code>{1, 1} =&gt; 5</code>, <code>{1, 2} =&gt; 6</code>. The order in which these elements are then stored in memory is implementation-defined. Tensor constants have the following constraints:</p> <ul> <li>(C1) <code>is_wellformed(element, element_type(type))</code>     for all <code>element</code> in <code>literal</code>.</li> <li>(C2) <code>has_shape(literal, shape(type))</code>, where:<ul> <li><code>has_shape(literal: String, []) = true</code>.</li> <li><code>has_shape(literal: List, shape) = size(literal) == shape[0] and all(has_shape(literal[:], shape[1:]))</code>.</li> <li>otherwise, <code>false</code>.</li> </ul> </li> </ul> <pre><code>StringConstant  ::= StringLiteral\nStringLiteral   ::= '\"' {stringCharacter | escapeSequence} '\"'\nstringCharacter ::= all ASCII characters except '\\00', '\\01', ... '\\1f' and '\"'\nescapeSequence  ::= '\\' ('\"' | '\\' | 'n' | 't' | (hexadecimalDigit hexadecimalDigit))\n</code></pre> <p>String literals consist of bytes specified using ASCII characters and escape sequences. They are encoding-agnostic, so the interpretation of these bytes is implementation-defined. String literals have type <code>string</code>.</p>"},{"location":"spec/#ops","title":"Ops","text":""},{"location":"spec/#abs","title":"abs","text":""},{"location":"spec/#semantics","title":"Semantics","text":"<p>Performs element-wise abs operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For signed integers: integer modulus.</li> <li>For floats: <code>abs</code> from IEEE-754.</li> <li>For complex numbers: complex modulus.</li> </ul>"},{"location":"spec/#inputs","title":"Inputs","text":"Name Type     <code>operand</code> tensor of signed integer, floating-point, or complex type"},{"location":"spec/#outputs","title":"Outputs","text":"Name Type     <code>result</code> tensor of signed integer, floating-point, or complex type"},{"location":"spec/#constraints","title":"Constraints","text":"<ul> <li>(C1)  <code>operand</code> and <code>result</code> have the same shape.</li> <li>(C2)  <code>operand</code> and <code>result</code> have the same element type, except when the   element type of the <code>operand</code> is complex type, in which case the element type   of the <code>result</code> is the element type of the complex type (e.g. the element type   of the <code>result</code> is <code>f64</code> for operand type <code>complex&lt;f64&gt;</code>).</li> </ul>"},{"location":"spec/#examples","title":"Examples","text":"<pre><code>// %operand: [-2, 0, 2]\n%result = \"stablehlo.abs\"(%operand) : (tensor&lt;3xi32&gt;) -&gt; tensor&lt;3xi32&gt;\n// %result: [2, 0, 2]\n</code></pre>"},{"location":"spec/#add","title":"add","text":""},{"location":"spec/#semantics_1","title":"Semantics","text":"<p>Performs element-wise addition of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical OR.</li> <li>For integers: integer addition.</li> <li>For floats: <code>addition</code> from IEEE-754.</li> <li>For complex numbers: complex addition.</li> </ul>"},{"location":"spec/#inputs_1","title":"Inputs","text":"Name Type     <code>lhs</code> tensor   <code>rhs</code> tensor"},{"location":"spec/#outputs_1","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_1","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_1","title":"Examples","text":"<pre><code>// %lhs: [[1, 2], [3, 4]]\n// %rhs: [[5, 6], [7, 8]]\n%result = \"stablehlo.add\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[6, 8], [10, 12]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#after_all","title":"after_all","text":""},{"location":"spec/#semantics_2","title":"Semantics","text":"<p>Ensures that the operations producing the <code>inputs</code> are executed before any operations that depend on <code>result</code>. Execution of this operation does nothing, it only exists to establish data dependencies from <code>result</code> to <code>inputs</code>.</p>"},{"location":"spec/#inputs_2","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of <code>token</code>"},{"location":"spec/#outputs_2","title":"Outputs","text":"Name Type     <code>result</code> <code>token</code>"},{"location":"spec/#examples_2","title":"Examples","text":"<pre><code>%result = \"stablehlo.after_all\"(%input0, %input1) : (!stablehlo.token, !stablehlo.token) -&gt; !stablehlo.token\n</code></pre>"},{"location":"spec/#all_gather","title":"all_gather","text":""},{"location":"spec/#semantics_3","title":"Semantics","text":"<p>Within each process group in the StableHLO grid, concatenates the values of the <code>operand</code> tensor from each process along <code>all_gather_dim</code> and produces a <code>result</code> tensor.</p> <p>The operation splits the StableHLO grid into <code>process_groups</code> as follows:</p> <ul> <li><code>channel_id &lt;= 0</code> and <code>use_global_device_ids = false</code>,     <code>cross_replica(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code> and <code>use_global_device_ids = false</code>,     <code>cross_replica_and_partition(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code> and <code>use_global_device_ids = true</code>,     <code>flattened_ids(replica_groups)</code>.</li> </ul> <p>Afterwards, within each <code>process_group</code>:</p> <ul> <li><code>operands@receiver = [operand@sender for sender in process_group]</code> for all     <code>receiver</code> in <code>process_group</code>.</li> <li><code>result@process = concatenate(operands@process, all_gather_dim)</code> for all     <code>process</code> in <code>process_group</code>.</li> </ul>"},{"location":"spec/#inputs_3","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>all_gather_dim</code> constant of type <code>si64</code>   <code>replica_groups</code> 2-dimensional tensor constant of type <code>si64</code>   <code>channel_id</code> constant of type <code>si64</code>   <code>use_global_device_ids</code> constant of type <code>i1</code>"},{"location":"spec/#outputs_3","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_2","title":"Constraints","text":"<ul> <li>(C1) <code>all_gather_dim</code> \\(\\in\\) [0, rank(<code>operand</code>)).</li> <li>(C2) All values in <code>replica_groups</code> are unique.</li> <li>(C3) <code>size(replica_groups)</code> depends on the process grouping strategy:<ul> <li>If <code>cross_replica</code>, <code>num_replicas</code>.</li> <li>If <code>cross_replica_and_partition</code>, <code>num_replicas</code>.</li> <li>If <code>flattened_ids</code>, <code>num_processes</code>.</li> </ul> </li> <li>(C4) \\(0 \\le\\) <code>replica_groups</code>[i] \\(\\lt\\) size(<code>replica_groups</code>) \\(\\forall i\\)          in <code>indices(replica_groups)</code>.</li> <li>(C5) If <code>use_global_device_ids = true</code>, then <code>channel_id &gt; 0</code>.     todo</li> <li>(C6)<code>type(result) = type(operand)</code> except:<ul> <li><code>dim(result, all_gather_dim)</code> =   <code>dim(operand, all_gather_dim) * dim(process_groups, 1)</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_3","title":"Examples","text":"<pre><code>// num_replicas: 2\n// num_partitions: 1\n// %operand@(0, 0): [[1.0, 2.0], [3.0, 4.0]]\n// %operand@(1, 0): [[5.0, 6.0], [7.0, 8.0]]\n%result = \"stablehlo.all_gather\"(%operand) {\n  all_gather_dim = 1 : i64,\n  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;,\n  // channel_id = 0\n  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;\n  // use_global_device_ids = false\n} : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x4xf32&gt;\n// %result@(0, 0): [[1.0, 2.0, 5.0, 6.0], [3.0, 4.0, 7.0, 8.0]]\n// %result@(1, 0): [[1.0, 2.0, 5.0, 6.0], [3.0, 4.0, 7.0, 8.0]]\n</code></pre>"},{"location":"spec/#all_reduce","title":"all_reduce","text":""},{"location":"spec/#semantics_4","title":"Semantics","text":"<p>Within each process group in the StableHLO grid, applies a reduction function <code>computation</code> to the values of the <code>operand</code> tensor from each process and produces a <code>result</code> tensor.</p> <p>The operation splits the StableHLO grid into process groups as follows:</p> <ul> <li><code>channel_id &lt;= 0</code> and <code>use_global_device_ids = false</code>,     <code>cross_replica(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code> and <code>use_global_device_ids = false</code>,     <code>cross_replica_and_partition(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code> and <code>use_global_device_ids = true</code>,     <code>flattened_ids(replica_groups)</code>.</li> </ul> <p>Afterwards, within each <code>process_group</code>:</p> <ul> <li><code>operands@receiver = [operand@sender for sender in process_group]</code> for all     <code>receiver</code> in <code>process_group</code>.</li> <li>  <pre><code>result@process[i0, i1, ..., iR-1] =\n    reduce_without_init(\n      inputs=operands@process[:][i0, i1, ..., iR-1],\n      dimensions=[0],\n      body=computation\n    )\n</code></pre> <p>where <code>reduce_without_init</code> works exactly like <code>reduce</code>, except that its <code>schedule</code> doesn't include init values.</p> </li> </ul>"},{"location":"spec/#inputs_4","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>replica_groups</code> variadic number of 1-dimensional tensor constants of type <code>si64</code>   <code>channel_id</code> constant of type <code>si64</code>   <code>use_global_device_ids</code> constant of type <code>i1</code>   <code>computation</code> function"},{"location":"spec/#outputs_4","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_3","title":"Constraints","text":"<ul> <li>(C1) All values in <code>replica_groups</code> are unique.</li> <li>(C2) <code>size(replica_groups)</code> depends on the process grouping strategy:<ul> <li>If <code>cross_replica</code>, <code>num_replicas</code>.</li> <li>If <code>cross_replica_and_partition</code>, <code>num_replicas</code>.</li> <li>If <code>flattened_ids</code>, <code>num_processes</code>.</li> </ul> </li> <li>(C3) \\(0 \\le\\) <code>replica_groups</code>[i] \\(\\lt\\) size(<code>replica_groups</code>) \\(\\forall i\\)          in <code>indices(replica_groups)</code>.</li> <li>(C4) If <code>use_global_device_ids = true</code>, then <code>channel_id &gt; 0</code>.          todo</li> <li>(C5) <code>computation</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; (tensor&lt;E&gt;)</code> where          <code>E = element_type(operand)</code>.</li> <li>(C6) type(<code>result</code>) \\(=\\) type(<code>operand</code>).</li> </ul>"},{"location":"spec/#examples_4","title":"Examples","text":"<pre><code>// num_replicas: 2\n// num_partitions: 1\n// %operand@(0, 0): [1.0, 2.0, 3.0, 4.0]\n// %operand@(1, 0): [5.0, 6.0, 7.0, 8.0]\n%result = \"stablehlo.all_reduce\"(%operand) ({\n  ^bb0(%arg0: tensor&lt;f32&gt;, %arg1: tensor&lt;f32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;f32&gt;) -&gt; ()\n}) {\n  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;,\n  // channel_id = 0\n  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;\n  // use_global_device_ids = false\n} : (tensor&lt;4xf32&gt;) -&gt; tensor&lt;4xf32&gt;\n// %result@(0, 0): [6.0, 8.0, 10.0, 12.0]\n// %result@(1, 0): [6.0, 8.0, 10.0, 12.0]\n</code></pre>"},{"location":"spec/#all_to_all","title":"all_to_all","text":""},{"location":"spec/#semantics_5","title":"Semantics","text":"<p>Within each process group in the StableHLO grid, splits the values of the <code>operand</code> tensor along <code>split_dimension</code> into parts, scatters the split parts between the processes, concatenates the scattered parts along <code>concat_dimension</code> and produces a <code>result</code> tensor.</p> <p>The operation splits the StableHLO grid into <code>process_groups</code> as follows:</p> <ul> <li><code>channel_id &lt;= 0</code>,     <code>cross_replica(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code>,     <code>cross_partition(replica_groups)</code>.</li> </ul> <p>Afterwards, within each <code>process_group</code>:</p> <ul> <li>  <pre><code>split_parts@sender = [\n    slice(\n      operand=operand@sender,\n      start_indices=[s0, s1, ..., sR-1],\n        # where\n        #  - sj = 0 if j != split_dimension\n        #  - sj = i * dim(operand, j) / split_count, if j == split_dimension\n        #  - R = rank(operand)\n      limit_indices=[l0, l1, ..., lR-1],\n        # where\n        #   - lj = dim(operand, j) if j != split_dimension\n        #   - lj = (i + 1) * dim(operand, j) / split_count, if j == split_dimension\n      strides=[1, ..., 1]\n    ) for i in range(split_count)\n ]\n</code></pre> <p>for all <code>sender</code> in <code>process_group</code>.   * <code>scattered_parts@receiver = [split_parts@sender[receiver_index] for sender in process_group]</code> where <code>receiver_index = index_of(receiver, process_group)</code>.   * <code>result@process = concatenate(scattered_parts@process, concat_dimension)</code>.</p> </li> </ul>"},{"location":"spec/#inputs_5","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>split_dimension</code> constant of type <code>si64</code>   <code>concat_dimension</code> constant of type <code>si64</code>   <code>split_count</code> constant of type <code>si64</code>   <code>replica_groups</code> 2-dimensional tensor constant of type <code>si64</code>   <code>channel_id</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_5","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_4","title":"Constraints","text":"<ul> <li>(C1) <code>split_dimension</code> \\(\\in\\) [0, rank(<code>operand</code>)).</li> <li>(C2) dim(<code>operand</code>, <code>split_dimension</code>) % <code>split_count</code> \\(=\\) 0.</li> <li>(C3) <code>concat_dimension</code> \\(\\in\\) [0, rank(<code>operand</code>)).</li> <li>(C4) <code>split_count</code> \\(\\gt\\) 0.</li> <li>(C5) All values in <code>replica_groups</code> are unique.</li> <li>(C6) <code>size(replica_groups)</code> depends on the process grouping strategy:<ul> <li>If <code>cross_replica</code>, <code>num_replicas</code>.</li> <li>If <code>cross_partition</code>, <code>num_partitions</code>.</li> </ul> </li> <li>(C7) \\(0 \\le\\) <code>replica_groups</code>[i] \\(\\lt\\) size(<code>replica_groups</code>) \\(\\forall i\\)          in <code>indices(replica_groups)</code>.</li> <li>(C8) <code>type(result) = type(operand)</code> except:<ul> <li><code>dim(result, split_dimension) =   dim(operand, split_dimension) / split_count</code>.</li> <li><code>dim(result, concat_dimension) =   dim(operand, concat_dimension) * split_count</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_5","title":"Examples","text":"<pre><code>// num_replicas: 2\n// num_partitions: 1\n// %operand@(0, 0): [\n//                   [1.0, 2.0, 3.0, 4.0],\n//                   [5.0, 6.0, 7.0, 8.0]\n//                  ]\n// %operand@(1, 0): [\n//                   [9.0, 10.0, 11.0, 12.0],\n//                   [13.0, 14.0, 15.0, 16.0]\n//                  ]\n%result = \"stablehlo.all_to_all\"(%operand) {\n  split_dimension = 1 : i64,\n  concat_dimension = 0 : i64,\n  split_count = 2 : i64,\n  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;\n} : (tensor&lt;2x4xf32&gt;) -&gt; tensor&lt;4x2xf32&gt;\n// %result@(0, 0): [\n//                  [1.0, 2.0],\n//                  [5.0, 6.0],\n//                  [9.0, 10.0],\n//                  [13.0, 14.0]\n//                 ]\n// %result@(1, 0): [\n//                  [3.0, 4.0],\n//                  [7.0, 8.0],\n//                  [11.0, 12.0],\n//                  [15.0, 16.0]\n//                 ]\n</code></pre>"},{"location":"spec/#and","title":"and","text":""},{"location":"spec/#semantics_6","title":"Semantics","text":"<p>Performs element-wise AND of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical AND.</li> <li>For integers: bitwise AND.</li> </ul>"},{"location":"spec/#inputs_6","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of boolean or integer type   <code>rhs</code> tensor of boolean or integer type"},{"location":"spec/#outputs_6","title":"Outputs","text":"Name Type     <code>result</code> tensor of boolean or integer type"},{"location":"spec/#constraints_5","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_6","title":"Examples","text":"<pre><code>// %lhs: [[1, 2], [3, 4]]\n// %rhs: [[5, 6], [7, 8]]\n%result = \"stablehlo.and\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[1, 2], [3, 0]]\n</code></pre>"},{"location":"spec/#atan2","title":"atan2","text":""},{"location":"spec/#semantics_7","title":"Semantics","text":"<p>Performs element-wise atan2 operation on <code>lhs</code> and <code>rhs</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>atan2</code> from IEEE-754.</li> <li>For complex numbers: complex atan2.</li> </ul>"},{"location":"spec/#inputs_7","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of floating-point or complex type   <code>rhs</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_7","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_6","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code>, and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_7","title":"Examples","text":"<pre><code>// %lhs: [0.0, 1.0, -1.0]\n// %rhs: [0.0, 0.0, 0.0]\n%result = \"stablehlo.atan2\"(%lhs, %rhs) : (tensor&lt;3xf32&gt;, tensor&lt;3xf32&gt;) -&gt; tensor&lt;3xf32&gt;\n// %result: [0.0, 1.57079637, -1.57079637] // [0.0, pi/2, -pi/2]\n</code></pre>"},{"location":"spec/#batch_norm_grad","title":"batch_norm_grad","text":""},{"location":"spec/#semantics_8","title":"Semantics","text":"<p>Computes gradients of several inputs of <code>batch_norm_training</code> backpropagating from <code>grad_output</code>, and produces <code>grad_operand</code>, <code>grad_scale</code> and <code>grad_offset</code> tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows:</p> <pre><code>def compute_sum(operand, feature_index):\n  (sum,) = reduce(\n      inputs=[operand],\n      init_values=[0.0],\n      dimensions=[i for i in range(rank(operand)) if i != feature_index],\n      body=lambda x, y: add(x, y))\n  return sum\n\ndef compute_mean(operand, feature_index):\n  sum = compute_sum(operand, feature_index)\n  divisor = constant(num_elements(operand) / dim(operand, feature_index))\n  divisor_bcast = broadcast_in_dim(divisor, [], shape(sum))\n  return divide(sum, divisor_bcast)\n\ndef batch_norm_grad(operand, scale, mean, variance, grad_output, epsilon, feature_index):\n  # Broadcast inputs to shape(operand)\n  scale_bcast = broadcast_in_dim(scale, [feature_index], shape(operand))\n  mean_bcast = broadcast_in_dim(mean, [feature_index], shape(operand))\n  variance_bcast = broadcast_in_dim(variance, [feature_index], shape(operand))\n  epsilon_bcast = broadcast_in_dim(constant(epsilon), [], shape(operand))\n\n  # Perform normalization using the provided `mean` and `variance`\n  # Intermediate values will be useful for computing gradients\n  centered_operand = subtract(operand, mean_bcast)\n  stddev = sqrt(add(variance_bcast, epsilon_bcast))\n  normalized_operand = divide(centered_operand, stddev)\n\n  # Use the implementation from batchnorm_expander.cc in XLA\n  # Temporary variables have exactly the same names as in the C++ code\n  elements_per_feature = constant(\n    divide(size(operand), dim(operand, feature_index)))\n  i1 = multiply(\n    grad_output,\n    broadcast_in_dim(elements_per_feature, [], shape(operand)))\n  i2 = broadcast_in_dim(\n    compute_sum(grad_output, feature_index),\n    [feature_index], shape(operand))\n  i3 = broadcast_in_dim(\n    compute_sum(multiply(grad_output, centered_operand)),\n    [feature_index], shape(operand))\n  i4 = multiply(i3, centered_operand)\n  i5 = divide(i4, add(variance_bcast, epsilon_bcast))\n  grad_operand = multiply(\n    divide(divide(scale_bcast, stddev), elements_per_feature),\n    subtract(subtract(i1, i2), i5))\n  grad_scale = compute_sum(\n    multiply(grad_output, normalized_operand), feature_index)\n  grad_offset = compute_sum(grad_output, feature_index)\n  return grad_operand, grad_scale, grad_offset\n</code></pre>"},{"location":"spec/#inputs_8","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type   <code>scale</code> 1-dimensional tensor of floating-point type   <code>mean</code> 1-dimensional tensor of floating-point type   <code>variance</code> 1-dimensional tensor of floating-point type   <code>grad_output</code> tensor of floating-point type   <code>epsilon</code> constant of type <code>f32</code>   <code>feature_index</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_8","title":"Outputs","text":"Name Type     <code>grad_operand</code> tensor of floating-point type   <code>grad_scale</code> 1-dimensional tensor of floating-point type   <code>grad_offset</code> 1-dimensional tensor of floating-point type"},{"location":"spec/#constraints_7","title":"Constraints","text":"<ul> <li>(C1) 0 \\(\\le\\) <code>feature_index</code> \\(\\lt\\) rank(<code>operand</code>).</li> <li>(C2) <code>operand</code>, <code>scale</code>, <code>mean</code>, <code>variance</code>, <code>grad_output</code>, <code>grad_operand</code> <code>grad_scale</code> and <code>grad_offset</code> have the same element type.</li> <li>(C3) <code>operand</code>, <code>grad_output</code> and <code>grad_operand</code> have the same shape.</li> <li>(C4) <code>scale</code>, <code>mean</code>, <code>variance</code>, <code>grad_scale</code> and <code>grad_offset</code> have the          same shape.</li> <li>(C5) size(<code>scale</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> </ul>"},{"location":"spec/#examples_8","title":"Examples","text":"<pre><code>// %operand: [\n//            [[1.0, 2.0], [3.0, 4.0]],\n//            [[3.0, 4.0], [1.0, 2.0]]\n//           ]\n// %scale: [1.0, 1.0]\n// %mean: [2.0, 3.0]\n// %variance: [1.0, 1.0]\n// %grad_output: [\n//                [[0.1, 0.1], [0.1, 0.1]],\n//                [[0.1, 0.1], [0.1, 0.1]]\n//               ]\n%grad_operand, %grad_scale, %grad_offset =\n\"stablehlo.batch_norm_grad\"(%operand, %scale, %mean, %variance, %grad_output) {\n  epsilon = 0.0 : f32,\n  feature_index = 2 : i64\n} : (tensor&lt;2x2x2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;,\n     tensor&lt;2x2x2xf32&gt;) -&gt; (tensor&lt;2x2x2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;)\n// %grad_operand: [\n//                 [[0.0, 0.0], [0.0, 0.0]],\n//                 [[0.0, 0.0], [0.0, 0.0]]\n//                ]\n// %grad_scale:  [0.0, 0.0]\n// %grad_offset: [0.4, 0.4]\n</code></pre>"},{"location":"spec/#batch_norm_inference","title":"batch_norm_inference","text":""},{"location":"spec/#semantics_9","title":"Semantics","text":"<p>Normalizes the <code>operand</code> tensor across all dimensions except for the <code>feature_index</code> dimension and produces a <code>result</code> tensor. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows:</p> <pre><code>def batch_norm_inference(operand, scale, offset, mean, variance, epsilon, feature_index):\n  # Broadcast inputs to shape(operand)\n  scale_bcast = broadcast_in_dim(scale, [feature_index], shape(operand))\n  offset_bcast = broadcast_in_dim(offset, [feature_index], shape(operand))\n  mean_bcast = broadcast_in_dim(mean, [feature_index], shape(operand))\n  variance_bcast = broadcast_in_dim(variance, [feature_index], shape(operand))\n  epsilon_bcast = broadcast_in_dim(constant(epsilon), [], shape(operand))\n\n  # Perform normalization using the provided `mean` and `variance` instead of\n  # computing them like `batch_norm_training` does.\n  centered_operand = subtract(operand, mean_bcast)\n  stddev = sqrt(add(variance_bcast, epsilon_bcast))\n  normalized_operand = divide(centered_operand, stddev)\n  return add(multiply(scale_bcast, normalized_operand), offset_bcast)\n</code></pre>"},{"location":"spec/#inputs_9","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type   <code>scale</code> 1-dimensional tensor of floating-point type   <code>offset</code> 1-dimensional tensor of floating-point type   <code>mean</code> 1-dimensional tensor of floating-point type   <code>variance</code> 1-dimensional tensor of floating-point type   <code>epsilon</code> constant of type <code>f32</code>   <code>feature_index</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_9","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_8","title":"Constraints","text":"<ul> <li>(C1) 0 \\(\\le\\) <code>feature_index</code> \\(\\lt\\) rank(<code>operand</code>).</li> <li>(C2) <code>operand</code>, <code>scale</code>, <code>offset</code>, <code>mean</code>, <code>variance</code> and <code>result</code> have the     same element type.</li> <li>(C3) size(<code>scale</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C4) size(<code>offset</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C5) size(<code>mean</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C6) size(<code>variance</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C7) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_9","title":"Examples","text":"<pre><code>// %operand: [\n//            [[1.0, 2.0], [3.0, 4.0]],\n//            [[3.0, 4.0], [1.0, 2.0]]\n//           ]\n// %scale: [1.0, 1.0]\n// %offset: [1.0, 1.0]\n// %mean: [2.0, 3.0]\n// %variance: [1.0, 1.0]\n%result = \"stablehlo.batch_norm_inference\"(%operand, %scale, %offset, %mean, %variance) {\n  epsilon = 0.0 : f32,\n  feature_index = 2 : i64\n} : (tensor&lt;2x2x2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;) -&gt; tensor&lt;2x2x2xf32&gt;\n// %result: [\n//           [[0.0, 0.0], [2.0, 2.0]],\n//           [[2.0, 2.0], [0.0, 0.0]]\n//          ]\n</code></pre>"},{"location":"spec/#batch_norm_training","title":"batch_norm_training","text":""},{"location":"spec/#semantics_10","title":"Semantics","text":"<p>Computes mean and variance across all dimensions except for the <code>feature_index</code> dimension and normalizes the <code>operand</code> tensor producing <code>output</code>, <code>batch_mean</code> and <code>batch_var</code> tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows:</p> <pre><code>def compute_mean(operand, feature_index):\n  (sum,) = reduce(\n      inputs=[operand],\n      init_values=[0.0],\n      dimensions=[i for i in range(rank(operand)) if i != feature_index],\n      body=lambda x, y: add(x, y))\n  divisor = constant(num_elements(operand) / dim(operand, feature_index))\n  divisor_bcast = broadcast_in_dim(divisor, [], shape(sum))\n  return divide(sum, divisor_bcast)\n\ndef compute_variance(operand, feature_index):\n  mean = compute_mean(operand, feature_index)\n  mean_bcast = broadcast_in_dim(mean, [feature_index], shape(operand))\n  centered_operand = subtract(operand, mean_bcast)\n  return compute_mean(mul(centered_operand, centered_operand), feature_index)\n\ndef batch_norm_training(operand, scale, offset, epsilon, feature_index):\n  mean = compute_mean(operand, feature_index)\n  variance = compute_variance(operand, feature_index)\n  return batch_norm_inference(operand, scale, offset, mean,\n                              variance, epsilon, feature_index)\n</code></pre>"},{"location":"spec/#inputs_10","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type   <code>scale</code> 1-dimensional tensor of floating-point type   <code>offset</code> 1-dimensional tensor of floating-point type   <code>epsilon</code> constant of type <code>f32</code>   <code>feature_index</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_10","title":"Outputs","text":"Name Type     <code>output</code> tensor of floating-point type   <code>batch_mean</code> 1-dimensional tensor of floating-point type   <code>batch_var</code> 1-dimensional tensor of floating-point type"},{"location":"spec/#constraints_9","title":"Constraints","text":"<ul> <li>(C1) 0 \\(\\le\\) <code>feature_index</code> \\(\\lt\\) rank(<code>operand</code>).</li> <li>(C2) <code>operand</code>, <code>scale</code>, <code>offset</code>, <code>result</code>, <code>batch_mean</code> and <code>batch_var</code>          have the same element type.</li> <li>(C3) size(<code>scale</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C4) size(<code>offset</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C5) size(<code>batch_mean</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C6) size(<code>batch_var</code>) \\(=\\) <code>dim(operand, feature_index)</code>.</li> <li>(C7) <code>operand</code> and <code>output</code> have the same type.</li> </ul>"},{"location":"spec/#examples_10","title":"Examples","text":"<pre><code>// %operand: [\n//            [[1.0, 2.0], [3.0, 4.0]],\n//            [[3.0, 4.0], [1.0, 2.0]]\n//           ]\n// %scale: [1.0, 1.0]\n// %offset: [1.0, 1.0]\n%output, %batch_mean, %batch_var = \"stablehlo.batch_norm_training\"(%operand, %scale, %offset) {\n  epsilon = 0.0 : f32,\n  feature_index = 2 : i64\n} : (tensor&lt;2x2x2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;) -&gt; (tensor&lt;2x2x2xf32&gt;, tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;)\n// %output: [\n//           [[0.0, 0.0], [2.0, 2.0]],\n//           [[2.0, 2.0], [0.0, 0.0]]\n//          ]\n// %batch_mean: [2.0, 3.0]\n// %batch_var: [1.0, 1.0]\n</code></pre>"},{"location":"spec/#bitcast_convert","title":"bitcast_convert","text":""},{"location":"spec/#semantics_11","title":"Semantics","text":"<p>Performs a bitcast operation on <code>operand</code> tensor and produces a <code>result</code> tensor where the bits of the entire <code>operand</code> tensor are reinterpreted using the type of the <code>result</code> tensor.</p> <p>Let <code>E</code> and <code>E'</code> be the <code>operand</code> and <code>result</code> element type respectively, and <code>R = rank(operand)</code>:</p> <ul> <li>If <code>num_bits(E')</code> \\(=\\) <code>num_bits(E)</code>,     <code>bits(result[i0, ..., iR-1]) = bits(operand[i0, ..., iR-1])</code>.</li> <li>If <code>num_bits(E')</code> \\(\\lt\\) <code>num_bits(E)</code>,     <code>bits(result[i0, ..., iR-1, :]) = bits(operand[i0, ..., iR-1])</code>.</li> <li>If <code>num_bits(E')</code> \\(\\gt\\) <code>num_bits(E)</code>,     <code>bits(result[i0, ..., iR-2]) = bits(operand[i0, ..., iR-2, :])</code>.</li> </ul> <p>The behavior of <code>bits</code> is implementation-defined because the exact representation of tensors is implementation-defined, and the exact representation of element types is implementation-defined as well.</p>"},{"location":"spec/#inputs_11","title":"Inputs","text":"Name Type     <code>operand</code> tensor"},{"location":"spec/#outputs_11","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_10","title":"Constraints","text":"<ul> <li>(C1) Let <code>E</code> and <code>E'</code> be the <code>operand</code> and <code>result</code> element type,     respectively and <code>R = rank(operand)</code>:<ul> <li>If <code>num_bits(E')</code> \\(=\\) <code>num_bits(E)</code>, shape(<code>result</code>) \\(=\\) shape(<code>operand</code>).</li> <li>If <code>num_bits(E')</code> \\(\\lt\\) <code>num_bits(E)</code>:</li> <li><code>rank(result) = R+1</code>.</li> <li>dim(<code>result</code>, <code>i</code>) \\(=\\) dim(<code>operand</code>, <code>i</code>) for all <code>i</code> \\(\\in\\) [0, <code>R</code>-1].</li> <li><code>dim(result, R) = num_bits(E)/num_bits(E')</code>.</li> <li>If <code>num_bits(E')</code> \\(\\gt\\) <code>num_bits(E)</code>:</li> <li><code>rank(result) = R-1</code>.</li> <li>dim(<code>result</code>, <code>i</code>) \\(=\\) dim(<code>operand</code>, <code>i</code>) for all <code>i</code> \\(\\in\\) [0, <code>R</code>-1).</li> <li><code>dim(operand, R-1) = num_bits(E')/num_bits(E)</code>.</li> </ul> </li> <li>(C2) Conversion between complex and non-complex types is not permitted.</li> </ul>"},{"location":"spec/#examples_11","title":"Examples","text":"<pre><code>// %operand: [0.0, 1.0]\n%result = \"stablehlo.bitcast_convert\"(%operand) : (tensor&lt;2xf32&gt;) -&gt; tensor&lt;2x4xi8&gt;\n// %result: [\n//           [0, 0, 0, 0],\n//           [0, 0, -128, 63] // little-endian representation of 1.0\n//          ]\n</code></pre>"},{"location":"spec/#broadcast_in_dim","title":"broadcast_in_dim","text":""},{"location":"spec/#semantics_12","title":"Semantics","text":"<p>Expands the dimensions and/or rank of an input tensor by duplicating the data in the <code>operand</code> tensor and produces a <code>result</code> tensor. Formally, <code>result[i0, i1, ..., iR-1]</code> \\(=\\) <code>operand[j0, j1, ..., jR'-1]</code> such that <code>jk</code> \\(=\\) <code>dim(operand, k) == 1 ? 0 : i[broadcast_dimensions[k]]</code> for all dimensions <code>k</code> in <code>operand</code>.</p>"},{"location":"spec/#inputs_12","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>broadcast_dimensions</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_12","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_11","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same element type.</li> <li>(C2) size(<code>broadcast_dimensions</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C3) \\(0 \\le\\) <code>broadcast_dimensions[i]</code> \\(\\lt\\) rank(<code>result</code>) for all          dimensions i in <code>operand</code>.</li> <li>(C4) All dimensions in <code>broadcast_dimensions</code> are unique.</li> <li>(C5) For all dimensions <code>j</code> in <code>operand</code>:<ul> <li><code>dim(operand, j) = 1</code> or</li> <li><code>dim(operand, j) = dim(result, i)</code> where <code>i = broadcast_dimensions[j]</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_12","title":"Examples","text":"<pre><code>// %operand: [\n//            [1, 2, 3]\n//           ]\n%result = \"stablehlo.broadcast_in_dim\"(%operand) {\n  broadcast_dimensions = dense&lt;[2, 1]&gt;: tensor&lt;2xi64&gt;\n} : (tensor&lt;1x3xi32&gt;) -&gt; tensor&lt;2x3x2xi32&gt;\n// %result: [\n//            [\n//             [1, 1],\n//             [2, 2],\n//             [3, 3]\n//            ],\n//            [\n//             [1, 1],\n//             [2, 2],\n//             [3, 3]\n//            ]\n//          ]\n</code></pre>"},{"location":"spec/#case","title":"case","text":""},{"location":"spec/#semantics_13","title":"Semantics","text":"<p>Produces the output from executing exactly one function from <code>branches</code> depending on the value of <code>index</code>. Formally, if \\(0 \\le\\) <code>index</code> \\(\\lt\\) <code>N-1</code>, output of <code>branches[index]</code> is returned, else, output of <code>branches[N-1]</code> is returned.</p>"},{"location":"spec/#inputs_13","title":"Inputs","text":"Name Type     <code>index</code> 1-dimensional tensor of type <code>si32</code>   <code>branches</code> variadic number of functions"},{"location":"spec/#outputs_13","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors or tokens"},{"location":"spec/#constraints_12","title":"Constraints","text":"<ul> <li>(C1) <code>branches</code> have at least one function.</li> <li>(C2) All functions in <code>branches</code> have 0 inputs.</li> <li>(C3) All functions in <code>branches</code> have the same output types.</li> <li>(C4) For all <code>i</code>, <code>type(results[i]) = type(branches[0]).outputs[i]</code>.</li> </ul>"},{"location":"spec/#examples_13","title":"Examples","text":"<pre><code>// %result_branch0: 10\n// %result_branch1: 11\n// %index: 1\n%result = \"stablehlo.case\"(%index) ({\n  \"stablehlo.return\"(%result_branch0) : (tensor&lt;i32&gt;) -&gt; ()\n}, {\n  \"stablehlo.return\"(%result_branch1) : (tensor&lt;i32&gt;) -&gt; ()\n}) : (tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n// %result: 11\n</code></pre>"},{"location":"spec/#cbrt","title":"cbrt","text":""},{"location":"spec/#semantics_14","title":"Semantics","text":"<p>Performs element-wise cubic root operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>rootn(x, 3)</code> from IEEE-754.</li> <li>For complex numbers: complex cubic root.</li> </ul>"},{"location":"spec/#inputs_14","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_14","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_13","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_14","title":"Examples","text":"<pre><code>// %operand: [0.0, 1.0, 8.0, 27.0]\n%result = \"stablehlo.cbrt\"(%operand) : (tensor&lt;4xf32&gt;) -&gt; tensor&lt;4xf32&gt;\n// %result: [0.0, 1.0, 2.0, 3.0]\n</code></pre>"},{"location":"spec/#ceil","title":"ceil","text":""},{"location":"spec/#semantics_15","title":"Semantics","text":"<p>Performs element-wise ceil of <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTowardPositive</code> operation from the IEEE-754 specification.</p>"},{"location":"spec/#inputs_15","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type"},{"location":"spec/#outputs_15","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_14","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_15","title":"Examples","text":"<pre><code>// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0]\n%result = \"stablehlo.ceil\"(%operand) : (tensor&lt;5xf32&gt;) -&gt; tensor&lt;5xf32&gt;\n// %result: [-0.0, -0.0, 1.0, 1.0, 2.0]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#cholesky","title":"cholesky","text":""},{"location":"spec/#semantics_16","title":"Semantics","text":"<p>Computes the Cholesky decomposition of a batch of matrices.</p> <p>More formally, for all <code>i</code>, <code>result[i0, ..., iR-3, :, :]</code> is a Cholesky decomposition of <code>a[i0, ..., iR-3, :, :]</code>, in the form of either of a lower-triangular (if <code>lower</code> is <code>true</code>) or upper-triangular (if <code>lower</code> is <code>false</code>) matrix. The output values in the opposite triangle, i.e. the strict upper triangle or strict lower triangle correspondingly, are implementation-defined.</p> <p>If there exists <code>i</code> where the input matrix is not an Hermitian positive-definite matrix, then the behavior is undefined.</p>"},{"location":"spec/#inputs_16","title":"Inputs","text":"Name Type     <code>a</code> tensor of floating-point or complex type   <code>lower</code> 0-dimensional tensor constant of type <code>i1</code>"},{"location":"spec/#outputs_16","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_15","title":"Constraints","text":"<ul> <li>(C1) <code>a</code> and <code>result</code> have the same type.</li> <li>(C2) rank(<code>a</code>) &gt;= 2.</li> <li>(C3) dim(<code>a</code>, -2) = dim(<code>a</code>, -1).</li> </ul>"},{"location":"spec/#examples_16","title":"Examples","text":"<pre><code>// %a: [\n//      [1.0, 2.0, 3.0],\n//      [2.0, 20.0, 26.0],\n//      [3.0, 26.0, 70.0]\n//     ]\n%result = \"stablehlo.cholesky\"(%a) {\n  lower = true\n} : (tensor&lt;3x3xf32&gt;) -&gt; tensor&lt;3x3xf32&gt;\n// %result: [\n//           [1.0, 0.0, 0.0],\n//           [2.0, 4.0, 0.0],\n//           [3.0, 5.0, 6.0]\n//          ]\n</code></pre>"},{"location":"spec/#clamp","title":"clamp","text":""},{"location":"spec/#semantics_17","title":"Semantics","text":"<p>Clamps every element of the <code>operand</code> tensor between a minimum and maximum value and produces a <code>result</code> tensor. More formally, <code>result[i0, ..., iR-1]</code> = <code>minimum(maximum(operand[i0, ..., iR-1], min_val), max_val)</code>, where <code>min_val = rank(min) == 0 ? min : min[i0, ..., iR-1]</code>, <code>max_val = rank(max) == 0 ? max : max[i0, ..., iR-1]</code>.</p>"},{"location":"spec/#inputs_17","title":"Inputs","text":"Name Type     <code>min</code> tensor   <code>operand</code> tensor   <code>max</code> tensor"},{"location":"spec/#outputs_17","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_16","title":"Constraints","text":"<ul> <li>(C1) Either <code>rank(min)</code> \\(=\\) <code>0</code> or <code>shape(min)</code> \\(=\\) <code>shape(operand)</code>.</li> <li>(C2) Either <code>rank(max)</code> \\(=\\) <code>0</code> or <code>shape(max)</code> \\(=\\) <code>shape(operand)</code>.</li> <li>(C3) <code>min</code>, <code>operand</code>, and <code>max</code> have the same element type.</li> <li>(C4) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_17","title":"Examples","text":"<pre><code>// %min: [5, 10, 15]\n// %operand: [3, 13, 23]\n// %max: [10, 15, 20]\n%result = \"stablehlo.clamp\"(%min, %operand, %max) : (tensor&lt;3xi32&gt;, tensor&lt;3xi32&gt;, tensor&lt;3xi32&gt;) -&gt; tensor&lt;3xi32&gt;\n// %result: [5, 13, 20]\n</code></pre>"},{"location":"spec/#collective_permute","title":"collective_permute","text":""},{"location":"spec/#semantics_18","title":"Semantics","text":"<p>Within each process group in the StableHLO grid, sends the value of the <code>operand</code> tensor from the source process to the target process and produces a <code>result</code> tensor.</p> <p>The operation splits the StableHLO grid into <code>process_groups</code> as follows:</p> <ul> <li><code>channel_id &lt;= 0</code>,     <code>cross_replica(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code>,     <code>cross_partition(replica_groups)</code>.</li> </ul> <p>Afterwards, <code>result@process</code> is given by:</p> <ul> <li><code>operand@process_groups[i, 0]</code>, if there exists an <code>i</code> such that    <code>process_groups[i, 1] = process</code>.</li> <li><code>broadcast_in_dim(0, [], shape(result))</code>, otherwise.</li> </ul>"},{"location":"spec/#inputs_18","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>source_target_pairs</code> 2-dimensional tensor constant of type <code>si64</code>   <code>channel_id</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_18","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_17","title":"Constraints","text":"<ul> <li>(C1) dim(<code>source_target_pairs</code>, 1) \\(=\\) 2.</li> <li>(C2) All values in <code>source_target_pairs[:, 0]</code> are unique.</li> <li>(C3) All values in <code>source_target_pairs[:, 1]</code> are unique.</li> <li>(C4) \\(0 \\le\\) source_target_pairs[i][0], source_target_pairs[i][1] \\(\\lt N\\),          where \\(N\\) depends on the process grouping strategy:<ul> <li>If <code>cross_replica</code>, <code>num_replicas</code>.</li> <li>If <code>cross_partition</code>, <code>num_partitions</code>.</li> </ul> </li> <li>(C5) type(<code>result</code>) \\(=\\) type(<code>operand</code>).</li> </ul>"},{"location":"spec/#examples_18","title":"Examples","text":"<pre><code>// num_replicas: 2\n// num_partitions: 1\n// %operand@(0, 0): [[1, 2], [3, 4]]\n// %operand@(1, 0): [[5, 6], [7, 8]]\n%result = \"stablehlo.collective_permute\"(%operand) {\n  source_target_pairs = dense&lt;[[0, 1]]&gt; : tensor&lt;2x2xi64&gt;,\n  // channel_id = 0\n  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;\n} : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n//\n// %result@(0, 0): [[0, 0], [0, 0]]\n// %result@(1, 0): [[1, 2], [3, 4]]\n</code></pre>"},{"location":"spec/#compare","title":"compare","text":""},{"location":"spec/#semantics_19","title":"Semantics","text":"<p>Performs element-wise comparison of <code>lhs</code> and <code>rhs</code> tensors according to <code>comparison_direction</code> and <code>compare_type</code>, and produces a <code>result</code> tensor.</p> <p>The values of <code>comparison_direction</code> and <code>compare_type</code> have the following semantics:</p> <p>For boolean and integer element types:</p> <ul> <li><code>EQ</code>: <code>lhs</code> \\(=\\) <code>rhs</code>.</li> <li><code>NE</code>: <code>lhs</code> \\(\\ne\\) <code>rhs</code>.</li> <li><code>GE</code>: <code>lhs</code> \\(\\ge\\) <code>rhs</code>.</li> <li><code>GT</code>: <code>lhs</code> \\(\\gt\\) <code>rhs</code>.</li> <li><code>LE</code>: <code>lhs</code> \\(\\le\\) <code>rhs</code>.</li> <li><code>LT</code>: <code>lhs</code> \\(\\lt\\) <code>rhs</code>.</li> </ul> <p>For floating-point element types and <code>compare_type = FLOAT</code>, the op implements the following IEEE-754 operations:</p> <ul> <li><code>EQ</code>: <code>compareQuietEqual</code>.</li> <li><code>NE</code>: <code>compareQuietNotEqual</code>.</li> <li><code>GE</code>: <code>compareQuietGreaterEqual</code>.</li> <li><code>GT</code>: <code>compareQuietGreater</code>.</li> <li><code>LE</code>: <code>compareQuietLessEqual</code>.</li> <li><code>LT</code>: <code>compareQuietLess</code>.</li> </ul> <p>For floating-point element types and <code>compare_type = TOTALORDER</code>, the op uses the combination of <code>totalOrder</code> and <code>compareQuietEqual</code> operations from IEEE-754.</p> <p>For complex element types, lexicographic comparison of <code>(real, imag)</code> pairs is performed using the provided <code>comparison_direction</code> and <code>compare_type</code>.</p>"},{"location":"spec/#inputs_19","title":"Inputs","text":"Name Type     <code>lhs</code> tensor   <code>rhs</code> tensor   <code>comparison_direction</code> enum of <code>EQ</code>, <code>NE</code>, <code>GE</code>, <code>GT</code>, <code>LE</code>, and <code>LT</code>   <code>compare_type</code> enum of <code>FLOAT</code>, <code>TOTALORDER</code>, <code>SIGNED</code>, and <code>UNSIGNED</code>"},{"location":"spec/#outputs_19","title":"Outputs","text":"Name Type     <code>result</code> tensor of boolean type"},{"location":"spec/#constraints_18","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code> and <code>rhs</code> have the same element type.</li> <li>(C2) <code>lhs</code>, <code>rhs</code>, and <code>result</code> have the same shape.</li> <li>(C3) Given <code>E</code> is the <code>lhs</code> element type, the following are legal values of          <code>compare_type</code>:<ul> <li>If <code>E</code> is signed integer type, <code>compare_type</code> = <code>SIGNED</code>.</li> <li>If <code>E</code> is unsigned integer or boolean type, <code>compare_type</code> = <code>UNSIGNED</code>.</li> <li>If <code>E</code> is floating-point type,   <code>compare_type</code> \\(\\in\\) {<code>FLOAT</code>, <code>TOTALORDER</code>}.</li> <li>If <code>E</code> is complex type, <code>compare_type</code> = <code>FLOAT</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_19","title":"Examples","text":"<pre><code>// %lhs: [1.0, 3.0]\n// %rhs: [1.1, 2.9]\n%result = \"stablehlo.compare\"(%lhs, %rhs) {\n  comparison_direction = #stablehlo&lt;comparison_direction LT&gt;,\n  compare_type = #stablehlo&lt;comparison_type FLOAT&gt;\n} : (tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;) -&gt; tensor&lt;2xi1&gt;\n// %result: [true, false]\n</code></pre>"},{"location":"spec/#complex","title":"complex","text":""},{"location":"spec/#semantics_20","title":"Semantics","text":"<p>Performs element-wise conversion to a complex value from a pair of real and imaginary values, <code>lhs</code> and <code>rhs</code>, and produces a <code>result</code> tensor.</p>"},{"location":"spec/#inputs_20","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of type <code>f32</code> or <code>f64</code>   <code>rhs</code> tensor of type <code>f32</code> or <code>f64</code>"},{"location":"spec/#outputs_20","title":"Outputs","text":"Name Type     <code>result</code> tensor of complex type"},{"location":"spec/#constraints_19","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code> and <code>rhs</code> have the same type.</li> <li>(C2) shape(<code>result</code>) \\(=\\) shape(<code>lhs</code>).</li> <li>(C3) element_type(<code>result</code>) = complex_type(element_type(<code>lhs</code>)).</li> </ul>"},{"location":"spec/#examples_20","title":"Examples","text":"<pre><code>// %lhs: [1.0, 3.0]\n// %rhs: [2.0, 4.0]\n%result = \"stablehlo.complex\"(%lhs, %rhs) : (tensor&lt;2xf32&gt;, tensor&lt;2xf32&gt;) -&gt; tensor&lt;2xcomplex&lt;f32&gt;&gt;\n// %result: [(1.0, 2.0), (3.0, 4.0)]\n</code></pre>"},{"location":"spec/#concatenate","title":"concatenate","text":""},{"location":"spec/#semantics_21","title":"Semantics","text":"<p>Concatenates a variadic number of tensors in <code>inputs</code> along <code>dimension</code> dimension in the same order as the given arguments and produces a <code>result</code> tensor. More formally, <code>result[i0, ..., id, ..., iR-1] = inputs[k][i0, ..., kd, ..., iR-1]</code>, where:</p> <ol> <li><code>id = d0 + ... + dk-1 + kd</code>.</li> <li><code>d</code> is equal to <code>dimension</code>, and <code>d0</code>, ... are <code>d</code>th dimension sizes      of <code>inputs</code>.</li> </ol>"},{"location":"spec/#inputs_21","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of tensors   <code>dimension</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_21","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_20","title":"Constraints","text":"<ul> <li>(C1) All tensors in <code>inputs</code> have the same element type.</li> <li>(C2) All tensors in <code>inputs</code> have the same shape except for the size of the   <code>dimension</code>th dimension.</li> <li>(C3) <code>inputs</code> have N tensors where N &gt;= 1.</li> <li>(C4) 0 \\(\\le\\) <code>dimension</code> \\(\\lt\\) <code>rank(inputs[0])</code>.</li> <li>(C5) <code>result</code> has the same element type as the tensors in <code>inputs</code>.</li> <li>(C6) <code>result</code> has the same shape as the tensors in <code>inputs</code> except for the   size of the <code>dimension</code>th dimension, which is calculated as a sum of the size   of <code>inputs[k][dimension]</code> for all <code>k</code> in <code>inputs</code>.</li> </ul>"},{"location":"spec/#examples_21","title":"Examples","text":"<pre><code>// %input0: [[1, 2], [3, 4], [5, 6]]\n// %input1: [[7, 8]]\n%result = \"stablehlo.concatenate\"(%input0, %input1) {\n  dimension = 0 : i64\n} : (tensor&lt;3x2xi32&gt;, tensor&lt;1x2xi32&gt;) -&gt; tensor&lt;4x2xi32&gt;\n// %result: [[1, 2], [3, 4], [5, 6], [7, 8]]\n</code></pre>"},{"location":"spec/#constant","title":"constant","text":""},{"location":"spec/#semantics_22","title":"Semantics","text":"<p>Produces an <code>output</code> tensor from a constant <code>value</code>.</p>"},{"location":"spec/#inputs_22","title":"Inputs","text":"Name Type     <code>value</code> constant"},{"location":"spec/#outputs_22","title":"Outputs","text":"Name Type     <code>output</code> tensor"},{"location":"spec/#constraints_21","title":"Constraints","text":"<ul> <li>(C1) <code>value</code> and <code>output</code> have the same type.</li> </ul>"},{"location":"spec/#examples_22","title":"Examples","text":"<pre><code>%output = \"stablehlo.constant\"() {\n  value = dense&lt;[[0.0, 1.0], [2.0, 3.0]]&gt; : tensor&lt;2x2xf32&gt;\n} : () -&gt; tensor&lt;2x2xf32&gt;\n// %output: [[0.0, 1.0], [2.0, 3.0]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#convert","title":"convert","text":""},{"location":"spec/#semantics_23","title":"Semantics","text":"<p>Performs an element-wise conversion from one element type to another on <code>operand</code> tensor and produces a <code>result</code> tensor.</p> <p>For conversions involving integer-to-integer, if there is an unsigned/signed overflow, the result is implementation-defined and one of the following:</p> <ul> <li>mathematical result modulo \\(2^n\\), where n is the bit width of the result,     for unsigned overflow. For signed integer overflow, wraps the result around     the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\).</li> <li>saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\)) for signed overflow and     saturation to \\(2^n - 1\\) (or \\(0\\)) for unsigned overflow.</li> </ul> <p>For conversions involving floating-point-to-floating-point or integer-to-floating-point, if the source value can be exactly represented in the destination type, the result value is that exact representation. Otherwise, the behavior is TBD.</p> <p>Conversion involving complex-to-complex follows the same behavior of floating-point-to-floating-point conversions for converting real and imaginary parts.</p> <p>For conversions involving floating-point-to-complex or complex-to-floating-point, the destination imaginary value is zeroed or the source imaginary value is ignored, respectively. The conversion of the real part follows the floating-point-to-floating-point conversion.</p> <p>Conversions involving integer-to-complex follows the same behavior as integer-to-floating-point conversion while converting the source integer to destination real part. The destination imaginary part is zeroed.</p> <p>For conversions involving floating-point-to-integer, the fractional part is truncated. If the truncated value cannot be represented in the destination type, the behavior is TBD. Conversions involving complex-to-integer follows the same behavior while converting the source real part to destination integer. The source imaginary part is ignored.</p> <p>For boolean-to-any-supported-type conversions, the value <code>false</code> is converted to zero, and the value <code>true</code> is converted to one. For any-supported-type-to-boolean conversions, a zero value is converted to <code>false</code> and any non-zero value is converted to <code>true</code>.</p>"},{"location":"spec/#inputs_23","title":"Inputs","text":"Name Type     <code>operand</code> tensor"},{"location":"spec/#outputs_23","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_22","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same shape.</li> </ul>"},{"location":"spec/#examples_23","title":"Examples","text":"<pre><code>// %operand: [1, 2, 3]\n%result = \"stablehlo.convert\"(%operand) : (tensor&lt;3xi32&gt;) -&gt; tensor&lt;3xcomplex&lt;f32&gt;&gt;\n// %result: [(1.0, 0.0), (2.0, 0.0), (3.0, 0.0)]\n</code></pre>"},{"location":"spec/#convolution","title":"convolution","text":""},{"location":"spec/#semantics_24","title":"Semantics","text":"<p>Computes dot products between windows of <code>lhs</code> and slices of <code>rhs</code> and produces <code>result</code>. The following diagram shows how elements in <code>result</code> are computed from <code>lhs</code> and <code>rhs</code> using a concrete example.</p>  <p>More formally, consider the following reframing of the inputs in terms of <code>lhs</code> in order to be able to express windows of <code>lhs</code>:</p>  <ul> <li><code>lhs_window_dimensions = lhs_shape(dim(lhs, input_batch_dimension), dim(rhs, kernel_spatial_dimensions), dim(lhs, input_feature_dimension))</code>.</li> <li><code>lhs_window_strides = lhs_shape(1, window_strides, 1)</code>.</li> <li><code>lhs_padding = lhs_shape([0, 0], padding, [0, 0])</code>.</li> <li><code>lhs_base_dilations = lhs_shape(1, lhs_dilation, 1)</code>.</li> <li><code>lhs_window_dilations = lhs_shape(1, rhs_dilation, 1)</code>.</li> </ul> <p>This reframing uses the following helper functions:</p> <ul> <li><code>lhs_shape(n, hw, c) = permute([n] + hw + [c], [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension])</code>.</li> <li><code>result_shape(n1, hw, c1) = permute([n1] + hw + [c1], [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension])</code>.</li> </ul> <p>If <code>feature_group_count = 1</code> and <code>batch_group_count = 1</code>, then for all <code>output_spatial_index</code> in the index space of <code>dim(result, output_spatial_dimensions)</code>, <code>result[result_shape(:, output_spatial_index, :)] = dot_product</code> where:</p> <ul> <li><code>padded_lhs = pad(lhs, 0, lhs_padding[:, 0], lhs_padding[:, 1], lhs_base_dilations)</code>.</li> <li><code>lhs_window_start = lhs_shape(0, output_spatial_index, 0) * lhs_window_strides</code>.</li> <li><code>lhs_window = slice(padded_lhs, lhs_window_start, lhs_window_start + lhs_window_dimensions, lhs_window_dilations)</code>.</li> <li><code>reversed_lhs_window = reverse(lhs_window, [input_spatial_dimensions[dim] for dim in [0, size(window_reversal) and window_reversal[dim] = true])</code>.</li> <li><code>dot_product = dot_general(reversed_lhs_window, rhs,       lhs_batching_dimensions=[],       lhs_contracting_dimensions=input_spatial_dimensions + [input_feature_dimension],       rhs_batching_dimensions=[],       rhs_contracting_dimensions=kernel_spatial_dimensions + [kernel_input_feature_dimension])</code>.</li> </ul> <p>If <code>feature_group_count &gt; 1</code>:</p> <ul> <li><code>lhses = split(lhs, feature_group_count, input_feature_dimension)</code>.</li> <li><code>rhses = split(rhs, feature_group_count, kernel_output_feature_dimension)</code>.</li> <li><code>results[:] = convolution(lhses[:], rhses[:], ..., feature_group_count=1, ...)</code>.</li> <li><code>result = concatenate(results, output_feature_dimension)</code>.</li> </ul> <p>If <code>batch_group_count &gt; 1</code>:</p> <ul> <li><code>lhses = split(lhs, batch_group_count, input_batch_dimension)</code>.</li> <li><code>rhses = split(rhs, batch_group_count, kernel_output_feature_dimension)</code>.</li> <li><code>results[:] = convolution(lhses[:], rhses[:], ..., batch_group_count=1, ...)</code>.</li> <li><code>result = concatenate(results, output_feature_dimension)</code>.</li> </ul>"},{"location":"spec/#inputs_24","title":"Inputs","text":"Name Type Constraints     <code>lhs</code> tensor (C1), (C2), (C11), (C12), (C26), (C27)   <code>rhs</code> tensor (C1), (C2), (C15), (C16), (C17), (C26)   <code>window_strides</code> 1-dimensional tensor constant of type <code>si64</code> (C3), (C4), (C26)   <code>padding</code> 2-dimensional tensor constant of type <code>si64</code> (C5), (C26)   <code>lhs_dilation</code> 1-dimensional tensor constant of type <code>si64</code> (C6), (C7), (C26)   <code>rhs_dilation</code> 1-dimensional tensor constant of type <code>si64</code> (C8), (C9), (C26)   <code>window_reversal</code> 1-dimensional tensor constant of type <code>i1</code> (C10)   <code>input_batch_dimension</code> constant of type <code>si64</code> (C11), (C14), (C26)   <code>input_feature_dimension</code> constant of type <code>si64</code> (C12), (C14)   <code>input_spatial_dimensions</code> 1-dimensional tensor constant of type <code>si64</code> (C13), (C14), (C26)   <code>kernel_input_feature_dimension</code> constant of type <code>si64</code> (C15), (C19)   <code>kernel_output_feature_dimension</code> constant of type <code>si64</code> (C16), (C17), (C19), (C26)   <code>kernel_spatial_dimensions</code> 1-dimensional tensor constant of type <code>si64</code> (C18), (C19), (C26)   <code>output_batch_dimension</code> constant of type <code>si64</code> (C21), (C26)   <code>output_feature_dimension</code> constant of type <code>si64</code> (C21),  (C26)   <code>output_spatial_dimensions</code> 1-dimensional tensor constant of type <code>si64</code> (C20), (C21), (C26)   <code>feature_group_count</code> constant of type <code>si64</code> (C12), (C15), (C17), (C22), (C24)   <code>batch_group_count</code> constant of type <code>si64</code> (C11), (C16), (C23), (C24), (C26)   <code>precision_config</code> variadic number of enum of <code>DEFAULT</code>, <code>HIGH</code>, and <code>HIGHEST</code> (C25)"},{"location":"spec/#outputs_24","title":"Outputs","text":"Name Type Constraints     <code>result</code> tensor (C26), (C27), (C28)"},{"location":"spec/#constraints_23","title":"Constraints","text":"<ul> <li>(C1) \\(N =\\) rank(<code>lhs</code>) \\(=\\) rank(<code>rhs</code>).</li> <li>(C2) element_type(<code>lhs</code>) \\(=\\) element_type(<code>rhs</code>).</li> <li>(C3) size(<code>window_strides</code>) \\(= N - 2\\) .</li> <li>(C4) <code>window_strides[i]</code> \\(\\gt 0\\)  for all i \\(\\in\\) [0, size(<code>window_strides</code>)).</li> <li>(C5) dim(<code>padding</code>, 0) \\(= N - 2\\) and dim(<code>padding</code>, 1) = 2.</li> <li>(C6) size(<code>lhs_dilation</code>) \\(= N - 2\\).</li> <li>(C7) <code>lhs_dilation[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(<code>lhs_dilation</code>)).</li> <li>(C8) size(<code>rhs_dilation</code>) \\(= N - 2\\).</li> <li>(C9) <code>rhs_dilation[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(<code>rhs_dilation</code>)).</li> <li>(C10) size(<code>window_reversal</code>) \\(= N - 2\\).</li> <li>(C11) <code>dim(lhs, input_batch_dimension) % batch_group_count = 0</code>.</li> <li>(C12) `dim(lhs, input_feature_dimension) % feature_group_count = 0.</li> <li>(C13) size(<code>input_spatial_dimensions</code>) \\(= N - 2\\).</li> <li>(C14) Given <code>input_dimensions = [input_batch_dimension] +          input_spatial_dimensions + [input_feature_dimension]</code>.<ul> <li>All dimensions in <code>input_dimensions</code> are unique.</li> <li>For any i \\(\\in\\) <code>input_dimensions</code>, 0 \\(\\le\\) i \\(\\lt\\) N.</li> </ul> </li> <li>(C15) <code>dim(rhs, kernel_input_feature_dimension = dim(lhs, input_feature_dimension) / feature_group_count</code>.</li> <li>(C16) <code>dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0</code>.</li> <li>(C17) <code>dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0</code>.</li> <li>(C18) size(<code>kernel_spatial_dimensions</code>) \\(= N - 2\\).</li> <li>(C19) Given <code>kernel_dimensions = kernel_spatial_dimensions +           [kernel_input_feature_dimension] + [kernel_output_feature_dimension]</code>.<ul> <li>All dimensions in <code>kernel_dimensions</code> are unique.</li> <li>For any i \\(\\in\\) <code>kernel_dimensions</code>, 0 \\(\\le\\) i \\(\\lt\\) N.</li> </ul> </li> <li>(C20) size(<code>output_spatial_dimensions</code>) \\(= N - 2\\).</li> <li>(C21) Given <code>output_dimensions = [output_batch_dimension] +           output_spatial_dimensions + [output_feature_dimension]</code>.<ul> <li>All dimensions in <code>output_dimensions</code> are unique.</li> <li>For any i \\(\\in\\) <code>output_dimensions</code>, 0 \\(\\le\\) i \\(\\lt\\) N.</li> </ul> </li> <li>(C22) <code>feature_group_count &gt; 0</code>.</li> <li>(C23) <code>batch_group_count &gt; 0</code>.</li> <li>(C24) <code>feature_group_count</code> \\(= 1\\) OR  <code>batch_group_count</code> \\(= 1\\).</li> <li>(C25) size(<code>precision_config</code>) \\(=\\) 2.</li> <li>(C26) For result_dim \\(\\in\\) [0, N), <code>dim(result, result_dim)</code> is given by<ul> <li><code>dim(lhs, input_batch_dimension) / batch_group_count</code>, if <code>result_dim = output_batch_dimension</code>.</li> <li><code>dim(rhs, kernel_output_feature_dimension)</code>, if <code>result_dim = output_feature_dimension</code>.</li> <li><code>num_windows</code> otherwise, where:</li> <li><code>output_spatial_dimensions[spatial_dim] = result_dim</code>.</li> <li><code>lhs_dim = input_spatial_dimensions[spatial_dim]</code>.</li> <li><code>rhs_dim = kernel_spatial_dimensions[spatial_dim]</code>.</li> <li><code>dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) == 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1</code>.</li> <li><code>padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1]</code>.</li> <li><code>dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) == 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1</code>.</li> <li><code>num_windows = (padded_input_shape[lhs_dim] == 0 || dilated_window_shape[lhs_dim] &gt; padded_input_shape[lhs_dim]) ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1</code>.</li> </ul> </li> <li>(C27) element_type(<code>result</code>) \\(=\\) element_type(<code>lhs</code>).</li> <li>(C28) rank(<code>result</code>) \\(= N\\).</li> </ul>"},{"location":"spec/#examples_24","title":"Examples","text":"<pre><code>// %lhs: [[\n//        [\n//          [1], [2], [5], [6]\n//        ],\n//        [\n//          [3], [4], [7], [8]\n//        ],\n//        [\n//          [10], [11], [14], [15]\n//        ],\n//        [\n//          [12], [13], [16], [17]\n//        ]\n//      ]]\n//\n// %rhs : [\n//         [[[1]], [[1]], [[1]]],\n//         [[[1]], [[1]], [[1]]],\n//         [[[1]], [[1]], [[1]]]\n//        ]\n%result = \"stablehlo.convolution\"(%lhs, %rhs) {\n  window_strides = dense&lt;4&gt; : tensor&lt;2xi64&gt;,\n  padding = dense&lt;0&gt; : tensor&lt;2x2xi64&gt;,\n  lhs_dilation = dense&lt;2&gt; : tensor&lt;2xi64&gt;,\n  rhs_dilation = dense&lt;1&gt; : tensor&lt;2xi64&gt;,\n  window_reversal = dense&lt;false&gt; : tensor&lt;2xi1&gt;,\n  // In the StableHLO dialect, dimension numbers are encoded via:\n  // `[&lt;input dimensions&gt;]x[&lt;kernel dimensions&gt;]-&gt;[output dimensions]`.\n  // \"b\" is batch dimenion, \"f\" is feature dimension,\n  // \"i\" is input feature dimension, \"o\" is output feature dimension,\n  // \"0/1/etc\" are spatial dimensions.\n  dimension_numbers = #stablehlo.conv&lt;[b, 0, 1, f]x[0, 1, i, o]-&gt;[b, 0, 1, f]&gt;,\n  feature_group_count = 1 : i64,\n  batch_group_count = 1 : i64,\n  precision_config = [#stablehlo&lt;precision DEFAULT&gt;, #stablehlo&lt;precision DEFAULT&gt;]\n} : (tensor&lt;1x4x4x1xi32&gt;, tensor&lt;3x3x1x1xi32&gt;) -&gt; tensor&lt;1x2x2x1xi32&gt;\n// %result: [[\n//            [[10], [26]],\n//            [[46], [62]]\n//          ]]\n</code></pre>"},{"location":"spec/#cosine","title":"cosine","text":""},{"location":"spec/#semantics_25","title":"Semantics","text":"<p>Performs element-wise cosine operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>cos</code> from IEEE-754.</li> <li>For complex numbers: complex cosine.</li> </ul>"},{"location":"spec/#inputs_25","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_25","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_24","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_25","title":"Examples","text":"<pre><code>// %operand: [\n//            [0.0, 1.57079632],       // [0, pi/2]\n//            [3.14159265, 4.71238898] // [pi, 3pi/2]\n//           ]\n%result = \"stablehlo.cosine\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[1.0, 0.0], [-1.0, 0.0]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#count_leading_zeros","title":"count_leading_zeros","text":""},{"location":"spec/#semantics_26","title":"Semantics","text":"<p>Performs element-wise count of the number of leading zero bits in the <code>operand</code> tensor and produces a <code>result</code> tensor.</p>"},{"location":"spec/#inputs_26","title":"Inputs","text":"Name Type     <code>operand</code> tensor of integer type"},{"location":"spec/#outputs_26","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer type"},{"location":"spec/#constraints_25","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_26","title":"Examples","text":"<pre><code>// %operand: [[0, 1], [127, -1]]\n%result = \"stablehlo.count_leading_zeros\"(%operand) : (tensor&lt;2x2xi8&gt;) -&gt; tensor&lt;2x2xi8&gt;\n// %result: [[8, 7], [1, 0]]\n</code></pre>"},{"location":"spec/#custom_call","title":"custom_call","text":""},{"location":"spec/#semantics_27","title":"Semantics","text":"<p>Encapsulates an implementation-defined operation <code>call_target_name</code> that takes <code>inputs</code> and <code>called_computations</code> and produces <code>results</code>. <code>has_side_effect</code>, <code>backend_config</code> and <code>api_version</code> may be used to provide additional implementation-defined metadata.</p>"},{"location":"spec/#inputs_27","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of values   <code>call_target_name</code> constant of type <code>string</code>   <code>has_side_effect</code> constant of type <code>i1</code>   <code>backend_config</code> constant of type <code>string</code>   <code>api_version</code> constant of type <code>si32</code>   <code>called_computations</code> variadic number of functions"},{"location":"spec/#outputs_27","title":"Outputs","text":"Name Type     <code>results</code> variadic number of values"},{"location":"spec/#examples_27","title":"Examples","text":"<pre><code>%results = \"stablehlo.custom_call\"(%input0) {\n  call_target_name = \"foo\",\n  has_side_effect = false,\n  backend_config = \"bar\",\n  api_version = 1 : i32,\n  called_computations = [@foo]\n} : (tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;\n</code></pre>"},{"location":"spec/#divide","title":"divide","text":""},{"location":"spec/#semantics_28","title":"Semantics","text":"<p>Performs element-wise division of dividend <code>lhs</code> and divisor <code>rhs</code> tensors and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For integers: integer division.</li> <li>For floats: <code>division</code> from IEEE-754.</li> <li>For complex numbers: complex division.</li> </ul>"},{"location":"spec/#inputs_28","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer, floating-point or complex type   <code>rhs</code> tensor of integer, floating-point or complex type"},{"location":"spec/#outputs_28","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer, floating-point or complex type"},{"location":"spec/#constraints_26","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_28","title":"Examples","text":"<pre><code>// %lhs: [17.1, -17.1, 17.1, -17.1]\n// %rhs: [3.0, 3.0, -3.0, -3.0]\n%result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor&lt;4xf32&gt;, tensor&lt;4xf32&gt;) -&gt; tensor&lt;4xf32&gt;\n// %result: [5.66666651, -5.66666651, -5.66666651, 5.66666651]\n\n// %lhs: [17, -17, 17, -17]\n// %rhs: [3, 3, -3, -3]\n%result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor&lt;4xi32&gt;, tensor&lt;4xi32&gt;) -&gt; tensor&lt;4xi32&gt;\n// %result: [5, -5, -5, 5]\n</code></pre>"},{"location":"spec/#dot_general","title":"dot_general","text":""},{"location":"spec/#semantics_29","title":"Semantics","text":"<p>Computes dot products between slices of <code>lhs</code> and slices of <code>rhs</code> and produces a <code>result</code> tensor.</p> <p>More formally, <code>result[result_index] = dot_product</code>, where:</p>  <ul> <li><code>lhs_result_dimensions = [d for d in axes(lhs) and d not in lhs_batching_dimensions and d not in lhs_contracting_dimensions]</code>.</li> <li><code>rhs_result_dimensions = [d for d in axes(rhs) and d not in rhs_batching_dimensions and d not in rhs_contracting_dimensions]</code>.</li> <li><code>result_batching_index + result_lhs_index + result_rhs_index = result_index</code>     where <code>size(result_batching_index) = size(lhs_batching_dimensions)</code>,     <code>size(result_lhs_index) = size(lhs_result_dimensions)</code> and     <code>size(result_rhs_index) = size(rhs_result_dimensions)</code>.</li> <li><code>transposed_lhs = transpose(lhs, lhs_batching_dimensions + lhs_result_dimensions + lhs_contracting_dimensions)</code>.</li> <li><code>transposed_lhs_slice = slice(result_batching_index + result_lhs_index + [:, ..., :])</code>.</li> <li><code>reshaped_lhs_slice = reshape(transposed_lhs_slice, dims(lhs, lhs_contracting_dimensions))</code>.</li> <li><code>transposed_rhs = transpose(rhs, rhs_batching_dimensions + rhs_result_dimensions + rhs_contracting_dimensions)</code>.</li> <li><code>transposed_rhs_slice = slice(result_batching_index + result_rhs_index + [:, ..., :])</code>.</li> <li><code>reshaped_rhs_slice = reshape(transposed_rhs_slice, dims(rhs, rhs_contracting_dimensions))</code>.</li> <li><code>dot_product = reduce(     inputs=[multiply(reshaped_lhs_slice, reshaped_rhs_slice)],     init_values=[0],     dimensions=[0, ..., size(lhs_contracting_dimensions) - 1],     body=lambda x, y: add(x, y))</code>.</li> </ul>  <p><code>precision_config</code> controls the tradeoff between speed and accuracy for computations on accelerator backends. This can be one of the following:</p> <ul> <li><code>DEFAULT</code>: Fastest calculation, but least accurate approximation to the     original number.</li> <li><code>HIGH</code>: Slower calculation, but more accurate approximation to the     original number.</li> <li><code>HIGHEST</code>: Slowest calculation, but most accurate approximation to the     original number.</li> </ul>"},{"location":"spec/#inputs_29","title":"Inputs","text":"Name Type     <code>lhs</code> tensor   <code>rhs</code> tensor   <code>lhs_batching_dimensions</code> 1-dimensional tensor constant of type <code>si64</code>   <code>rhs_batching_dimensions</code> 1-dimensional tensor constant of type <code>si64</code>   <code>lhs_contracting_dimensions</code> 1-dimensional tensor constant of type <code>si64</code>   <code>rhs_contracting_dimensions</code> 1-dimensional tensor constant of type <code>si64</code>   <code>precision_config</code> variadic number of enum of <code>DEFAULT</code>, <code>HIGH</code>, and <code>HIGHEST</code>"},{"location":"spec/#outputs_29","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_27","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code> and <code>rhs</code> have the same element type.</li> <li>(C2) size(<code>lhs_batching_dimensions</code>) \\(=\\) size(<code>rhs_batching_dimensions</code>).</li> <li>(C3) size(<code>lhs_contracting_dimensions</code>) \\(=\\)     size(<code>rhs_contracting_dimensions</code>).</li> <li>(C4) <code>lhs_batching_dimensions</code> and <code>lhs_contracting_dimensions</code> combined are     unique.</li> <li>(C5) <code>rhs_batching_dimensions</code> and <code>rhs_contracting_dimensions</code> combined are     unique.</li> <li>(C6) 0 \\(\\le\\) <code>lhs_batching_dimensions[i]</code> \\(\\lt\\) rank(<code>lhs</code>) for all <code>i</code> \\(\\in\\) [0, size(<code>lhs_batching_dimensions</code>)).</li> <li>(C7) 0 \\(\\le\\) <code>lhs_contracting_dimensions[i]</code> \\(\\lt\\) rank(<code>lhs</code>) for all <code>i</code> \\(\\in\\) [0, size(<code>lhs_contracting_dimensions</code>)).</li> <li>(C8) 0 \\(\\le\\) <code>rhs_batching_dimensions[d]</code> \\(\\lt\\) rank(<code>rhs</code>) for all <code>i</code> \\(\\in\\) [0, size(<code>rhs_batching_dimensions</code>)).</li> <li>(C9) 0 \\(\\le\\) <code>rhs_contracting_dimensions[d]</code> \\(\\lt\\) rank(<code>rhs</code>) for all <code>i</code> \\(\\in\\) [0, size(<code>rhs_contracting_dimensions</code>)).</li> <li>(C10) dim(<code>lhs</code>, <code>lhs_batching_dimensions[i]</code>) \\(=\\)     dim(<code>rhs</code>, <code>rhs_batching_dimensions[i]</code>) for all <code>i</code> \\(\\in\\) [0,     size(<code>lhs_batching_dimensions</code>)).</li> <li>(C11) dim(<code>lhs</code>, <code>lhs_contracting_dimensions[i]</code>) \\(=\\)     dim(<code>rhs</code>, <code>rhs_contracting_dimensions[i]</code>) for all <code>i</code> \\(\\in\\) [0,     size(<code>lhs_contracting_dimensions</code>)).</li> <li>(C12) size(<code>precision_config</code>) \\(=\\) 2.</li> <li>(C13) shape(<code>result</code>) \\(=\\) dim(<code>lhs</code>, <code>lhs_batching_dimensions</code>) +     dim(<code>lhs</code>, <code>lhs_result_dimensions</code>) + dim(<code>rhs</code>, <code>rhs_result_dimensions</code>).</li> </ul>"},{"location":"spec/#examples_29","title":"Examples","text":"<pre><code>// %lhs: [\n//        [[1, 2],\n//         [3, 4]],\n//        [[5, 6],\n//         [7, 8]]\n//       ]\n// %rhs: [\n//        [[1, 0],\n//         [0, 1]],\n//        [[1, 0],\n//         [0, 1]]\n//       ]\n%result = \"stablehlo.dot_general\"(%lhs, %rhs) {\n  dot_dimension_numbers = #stablehlo.dot&lt;\n    lhs_batching_dimensions = [0],\n    rhs_batching_dimensions = [0],\n    lhs_contracting_dimensions = [2],\n    rhs_contracting_dimensions = [1]\n  &gt;,\n  precision_config = [#stablehlo&lt;precision DEFAULT&gt;, #stablehlo&lt;precision DEFAULT&gt;]\n} : (tensor&lt;2x2x2xi32&gt;, tensor&lt;2x2x2xi32&gt;) -&gt; tensor&lt;2x2x2xi32&gt;\n// %result: [\n//           [[1, 2],\n//            [3, 4]],\n//           [[5, 6],\n//            [7, 8]]\n//          ]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#dynamic_slice","title":"dynamic_slice","text":""},{"location":"spec/#semantics_30","title":"Semantics","text":"<p>Extracts a slice from the <code>operand</code> using dynamically-computed starting indices and produces a <code>result</code> tensor. <code>start_indices</code> contain the starting indices of the slice for each dimension subject to potential adjustment, and <code>slice_sizes</code> contain the sizes of the slice for each dimension.</p> <p>More formally, <code>result[i0, ..., iR-1] = operand[j0, ..., jR-1]</code> where:</p> <ul> <li><code>jd = adjusted_start_indices[d][] + id</code>.</li> <li><code>adjusted_start_indices = clamp(0, start_indices, shape(operand) -</code> <code>slice_sizes)</code>.</li> </ul>"},{"location":"spec/#inputs_30","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>start_indices</code> variadic number of 0-dimensional tensors of integer type   <code>slice_sizes</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_30","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_28","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same element type.</li> <li>(C2) size(<code>start_indices</code>) \\(=\\) size(<code>slice_sizes</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C3) All <code>start_indices</code> have the same type.</li> <li>(C4) <code>slice_sizes[k]</code> \\(\\in\\) [0, dim(<code>operand</code>, <code>k</code>)) for all <code>k</code> \\(\\in\\) [0,     rank(<code>operand</code>)).</li> <li>(C5) shape(<code>result</code>) \\(=\\) <code>slice_sizes</code>.</li> </ul>"},{"location":"spec/#examples_30","title":"Examples","text":"<pre><code>// %operand: [\n//            [0, 0, 1, 1],\n//            [0, 0, 1, 1],\n//            [0, 0, 0, 0],\n//            [0, 0, 0, 0]\n//           ]\n// %start_indices0: -1\n// %start_indices1: 3\n%result = \"stablehlo.dynamic_slice\"(%operand, %start_indices0, %start_indices1) {\n  slice_sizes = dense&lt;[2, 2]&gt; : tensor&lt;2xi64&gt;\n} : (tensor&lt;4x4xi32&gt;, tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [\n//           [1, 1],\n//           [1, 1]\n//          ]\n</code></pre>"},{"location":"spec/#dynamic_update_slice","title":"dynamic_update_slice","text":""},{"location":"spec/#semantics_31","title":"Semantics","text":"<p>Produces a <code>result</code> tensor which is equal to the <code>operand</code> tensor except that the slice starting at <code>start_indices</code> is updated with the values in <code>update</code>.</p> <p>More formally, <code>result[i0, ..., iR-1]</code> is defined as:</p> <ul> <li><code>update[j0, ..., jR-1]</code> if <code>jd = adjusted_start_indices[d][] + id</code> where     <code>adjusted_start_indices =     clamp(0, start_indices, shape(operand) - shape(update))</code>.</li> <li><code>operand[i0, ..., iR-1]</code> otherwise.</li> </ul>"},{"location":"spec/#inputs_31","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>update</code> tensor   <code>start_indices</code> variadic number of 0-dimensional tensors of integer type"},{"location":"spec/#outputs_31","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_29","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> <li>(C2) element_type(<code>update</code>) \\(=\\) element_type(<code>operand</code>).</li> <li>(C3) rank(<code>update</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C4) size(<code>start_indices</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C5) All <code>start_indices</code> have the same type.</li> <li>(C6) dim(<code>update</code>, <code>k</code>) \\(\\in\\) [0, dim(<code>operand</code>, <code>k</code>)] for all <code>k</code> \\(\\in\\)     [0, rank(<code>operand</code>)).</li> </ul>"},{"location":"spec/#examples_31","title":"Examples","text":"<pre><code>// %operand: [\n//            [1, 1, 0, 0],\n//            [1, 1, 0, 0],\n//            [1, 1, 1, 1],\n//            [1, 1, 1, 1]\n//           ]\n// %update: [\n//           [1, 1],\n//           [1, 1]\n//          ]\n// %start_indices0: -1\n// %start_indices1: 3\n%result = \"stablehlo.dynamic_update_slice\"(%operand, %update, %start_indices0, %start_indices1)\n  : (tensor&lt;4x4xi32&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;i64&gt;, tensor&lt;i64&gt;) -&gt; tensor&lt;4x4xi32&gt;\n// %result: [\n//           [1, 1, 1, 1],\n//           [1, 1, 1, 1],\n//           [1, 1, 1, 1],\n//           [1, 1, 1, 1]\n//          ]\n</code></pre>"},{"location":"spec/#exponential","title":"exponential","text":""},{"location":"spec/#semantics_32","title":"Semantics","text":"<p>Performs element-wise exponential operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>exp</code> from IEEE-754.</li> <li>For complex numbers: complex exponential.</li> </ul>"},{"location":"spec/#inputs_32","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_32","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_30","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_32","title":"Examples","text":"<pre><code>// %operand: [[0.0, 1.0], [2.0, 3.0]]\n%result = \"stablehlo.exponential\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[1.0, 2.71828183], [7.38905610, 20.08553692]]\n\n// %operand: (1.0, 2.0)\n%result = \"stablehlo.exponential\"(%operand) : (tensor&lt;complex&lt;f32&gt;&gt;) -&gt; tensor&lt;complex&lt;f32&gt;&gt;\n// %result: (-1.13120438, 2.47172667)\n</code></pre>"},{"location":"spec/#exponential_minus_one","title":"exponential_minus_one","text":""},{"location":"spec/#semantics_33","title":"Semantics","text":"<p>Performs element-wise exponential minus one operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>expm1</code> from IEEE-754.</li> <li>For complex numbers: complex exponential minus one.</li> </ul>"},{"location":"spec/#inputs_33","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_33","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_31","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_33","title":"Examples","text":"<pre><code>// %operand: [0.0, 1.0]\n%result = \"stablehlo.exponential_minus_one\"(%operand) : (tensor&lt;2xf32&gt;) -&gt; tensor&lt;2xf32&gt;\n// %result: [0.0, 1.71828187]\n</code></pre>"},{"location":"spec/#fft","title":"fft","text":""},{"location":"spec/#semantics_34","title":"Semantics","text":"<p>Performs the forward and inverse Fourier transforms for real and complex inputs/outputs.</p> <p><code>fft_type</code> is one of the following:</p> <ul> <li><code>FFT</code>: Forward complex-to-complex FFT.</li> <li><code>IFFT</code>: Inverse complex-to-complex FFT.</li> <li><code>RFFT</code>: Forward real-to-complex FFT.</li> <li><code>IRFFT</code>: Inverse real-to-complex FFT (i.e. takes complex, returns real).</li> </ul> <p>More formally, given the function <code>fft</code> which takes 1-dimensional tensors of complex types as input, produces 1-dimensional tensors of same types as output and computes the discrete Fourier transform:</p> <p>For <code>fft_type = FFT</code>, <code>result</code> is defined as the final result of a series of L computations where <code>L = size(fft_length)</code>. For example, for <code>L = 3</code>:</p> <ul> <li><code>result1[i0, ..., :]</code> = <code>fft(operand[i0, ..., :])</code> for all <code>i</code>.</li> <li><code>result2[i0, ..., :, iR-1]</code> = <code>fft(result1[i0, ..., :, iR-1])</code> for all <code>i</code>.</li> <li><code>result[i0, ..., :, iR-2, iR-1]</code> = <code>fft(result2[i0, ..., :, iR-2, iR-1])</code>     for all <code>i</code>.</li> </ul> <p>Furthermore, given the function <code>ifft</code> which has the same type signature and computes the inverse of <code>fft</code>:</p> <p>For <code>fft_type = IFFT</code>, <code>result</code> is defined as the inverse of the computations for <code>fft_type = FFT</code>. For example, for <code>L = 3</code>:</p> <ul> <li><code>result1[i0, ..., :, iR-2, iR-1]</code> = <code>ifft(operand[i0, ..., :, iR-2, iR-1])</code>     for all <code>i</code>.</li> <li><code>result2[i0, ..., :, iR-1]</code> = <code>ifft(result1[i0, ..., :, iR-1])</code> for all <code>i</code>.</li> <li><code>result[i0, ..., :]</code> = <code>ifft(result2[i0, ..., :])</code> for all <code>i</code>.</li> </ul> <p>Furthermore, given the function <code>rfft</code> which takes 1-dimensional tensors of floating-point types, produces 1-dimensional tensors of complex types of the same floating-point semantics and works as follows:</p> <ul> <li><code>rfft(real_operand) = truncated_result</code> where</li> <li><code>complex_operand[i] = (real_operand, 0)</code> for all <code>i</code>.</li> <li><code>complex_result = fft(complex_operand)</code>.</li> <li><code>truncated_result = complex_result[:(rank(complex_result) / 2 + 1)]</code>.</li> </ul> <p>(When the discrete Fourier transform is computed for real operands, the first <code>N/2 + 1</code> elements of the result unambiguously define the rest of the result, so the result of <code>rfft</code> is truncated to avoid computing redundant elements).</p> <p>For <code>fft_type = RFFT</code>, <code>result</code> is defined as the final result of a series of L computations where <code>L = size(fft_length)</code>. For example, for <code>L = 3</code>:</p> <ul> <li><code>result1[i0, ..., :]</code> = <code>rfft(operand[i0, ..., :])</code> for all <code>i</code>.</li> <li><code>result2[i0, ..., :, iR-1]</code> = <code>fft(result1[i0, ..., :, iR-1])</code> for all <code>i</code>.</li> <li><code>result[i0, ..., :, iR-2, iR-1]</code> = <code>fft(result2[i0, ..., :, iR-2, iR-1])</code>     for all <code>i</code>.</li> </ul> <p>Finally, given the function <code>irfft</code> which has the same type signature and computes the inverse of <code>rfft</code>:</p> <p>For <code>fft_type = IRFFT</code>, <code>result</code> is defined as the inverse of the computations for <code>fft_type = RFFT</code>. For example, for <code>L = 3</code>:</p> <ul> <li><code>result1[i0, ..., :, iR-2, iR-1]</code> = <code>ifft(operand[i0, ..., :, iR-2, iR-1])</code>     for all <code>i</code>.</li> <li><code>result2[i0, ..., :, iR-1]</code> = <code>ifft(result1[i0, ..., :, iR-1])</code> for all <code>i</code>.</li> <li><code>result[i0, ..., :]</code> = <code>irfft(result2[i0, ..., :])</code> for all <code>i</code>.</li> </ul>"},{"location":"spec/#inputs_34","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type   <code>fft_type</code> enum of <code>FFT</code>, <code>IFFT</code>, <code>RFFT</code>, and <code>IRFFT</code>   <code>fft_length</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_34","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_32","title":"Constraints","text":"<ul> <li>(C1) <code>rank(operand)</code> \\(\\ge\\) <code>size(fft_length)</code>.</li> <li>(C2) The relationship between <code>operand</code> and <code>result</code> element types varies:<ul> <li>If <code>fft_type = FFT</code>, <code>element_type(operand)</code> and <code>element_type(result)</code>   have the same complex type.</li> <li>If <code>fft_type = IFFT</code>, <code>element_type(operand)</code> and <code>element_type(result)</code>   have the same complex type.</li> <li>If <code>fft_type = RFFT</code>, <code>element_type(operand)</code> is a floating-point type and   <code>element_type(result)</code> is a complex type of the same floating-point   semantics.</li> <li>If <code>fft_type = IRFFT</code>, <code>element_type(operand)</code> is a complex type and   <code>element_type(result)</code> is a floating-point type of the same floating-point   semantics.</li> </ul> </li> <li>(C3) 1 \\(\\le\\) <code>size(fft_length)</code> \\(\\le\\) 3.</li> <li>(C4) If among <code>operand</code> and <code>result</code>, there is a tensor <code>real</code> of a   floating-type type, then <code>dims(real)[-size(fft_length):] = fft_length</code>.</li> <li>(C5) <code>dim(result, d) = dim(operand, d)</code> for all <code>d</code>, except for:<ul> <li>If <code>fft_type = RFFT</code>,   <code>dim(result, -1) = dim(operand, -1) == 0 ? 0 : dim(operand, -1) / 2 + 1</code>.</li> <li>If <code>fft_type = IRFFT</code>,   <code>dim(operand, -1) = dim(result, -1) == 0 ? 0 : dim(result, -1) / 2 + 1</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_34","title":"Examples","text":"<pre><code>// %operand: [(1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]\n%result = \"stablehlo.fft\"(%operand) {\n  fft_type = #stablehlo&lt;fft_type FFT&gt;,\n  fft_length = dense&lt;4&gt; : tensor&lt;1xi64&gt;\n} : (tensor&lt;4xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;4xcomplex&lt;f32&gt;&gt;\n// %result: [(1.0, 0.0), (1.0, 0.0), (1.0, 0.0), (1.0, 0.0)]\n</code></pre>"},{"location":"spec/#floor","title":"floor","text":""},{"location":"spec/#semantics_35","title":"Semantics","text":"<p>Performs element-wise floor of <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTowardNegative</code> operation from the IEEE-754 specification.</p>"},{"location":"spec/#inputs_35","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type"},{"location":"spec/#outputs_35","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_33","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_35","title":"Examples","text":"<pre><code>// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0]\n%result = \"stablehlo.floor\"(%operand) : (tensor&lt;5xf32&gt;) -&gt; tensor&lt;5xf32&gt;\n// %result: [-1.0, -1.0, 0.0, 0.0, 2.0]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#gather","title":"gather","text":""},{"location":"spec/#semantics_36","title":"Semantics","text":"<p>Gathers slices from <code>operand</code> tensor from offsets specified in <code>start_indices</code> and produces a <code>result</code> tensor.</p> <p>The following diagram shows how elements in <code>result</code> map on elements in <code>operand</code> using a concrete example. The diagram picks a few example <code>result</code> indices and explains in detail which <code>operand</code> indices they correspond to.</p>  <p>More formally, <code>result[result_index] = operand[operand_index]</code> where:</p> <ul> <li><code>batch_dims</code> = [<code>d</code> for <code>d</code> in <code>axes(result)</code> and <code>d</code> not in <code>offset_dims</code>].</li> <li><code>batch_index</code> = [<code>result_index[d]</code> for <code>d</code> in <code>batch_dims</code>].</li> <li><code>start_index</code> =<ul> <li><code>start_indices[bi0, ..., :, ..., biN]</code> where <code>bi</code> are individual     elements in <code>batch_index</code> and <code>:</code> is inserted at the <code>index_vector_dim</code>     index, if <code>index_vector_dim</code> &lt; <code>rank(start_indices)</code>.</li> <li><code>[start_indices[batch_index]]</code> otherwise.</li> </ul> </li> <li>For <code>do</code> in <code>axes(operand)</code>,<ul> <li><code>full_start_index[do]</code> = <code>start_index[ds]</code> if <code>do = start_index_map[ds]</code>.</li> <li><code>full_start_index[do]</code> = <code>0</code> otherwise.</li> </ul> </li> <li><code>offset_index</code> = [<code>result_index[d]</code> for <code>d</code> in <code>offset_dims</code>].</li> <li><code>full_offset_index</code> = <code>[oi0, ..., 0, ..., oiN]</code> where <code>oi</code> are individual     elements in <code>offset_index</code>, and <code>0</code> is inserted at indices from     <code>collapsed_slice_dims</code>.</li> <li><code>operand_index</code> = <code>add(full_start_index, full_offset_index)</code>.     If <code>operand_index</code> is out of bounds for <code>operand</code>, then the behavior is     implementation-defined.</li> </ul> <p>If <code>indices_are_sorted</code> is <code>true</code> then the implementation can assume that <code>start_indices</code> are sorted with respect to <code>start_index_map</code>, otherwise the behavior is undefined. More formally, for all <code>id &lt; jd</code> from <code>indices(result)</code>, <code>full_start_index(id)</code> &lt;= <code>full_start_index(jd)</code>.</p>"},{"location":"spec/#inputs_36","title":"Inputs","text":"Name Type Constraints     <code>operand</code> tensor (C1), (C10), (C11), (C12), (C15)   <code>start_indices</code> tensor of integer type (C2), (C3), (C13)   <code>offset_dims</code> 1-dimensional tensor constant of type <code>si64</code> (C1), (C4), (C5),   <code>collapsed_slice_dims</code> 1-dimensional tensor constant of type <code>si64</code> (C1), (C6), (C7), (C8), (C13)   <code>start_index_map</code> 1-dimensional tensor constant of type <code>si64</code> (C3), (C9), (C10)   <code>index_vector_dim</code> constant of type <code>si64</code> (C2), (C3), (C13)   <code>slice_sizes</code> 1-dimensional tensor constant of type <code>si64</code> (C7), (C8), (C11), (C12), (C13)   <code>indices_are_sorted</code> constant of type <code>i1</code>"},{"location":"spec/#outputs_36","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_34","title":"Constraints","text":"<ul> <li>(C1) rank(<code>operand</code>) \\(=\\) size(<code>offset_dims</code>) \\(+\\)          size(<code>collapsed_slice_dims</code>).</li> <li>(C2) \\(0 \\le\\) <code>index_vector_dim</code> \\(\\le\\) rank(<code>start_indices</code>).</li> <li>(C3) size(<code>start_index_map</code>) \\(=\\) <code>index_vector_dim</code> \\(\\lt\\) rank(<code>start_indices</code>) ?          dim(<code>start_indices</code>, <code>index_vector_dim</code>) : 1.</li> <li>(C4) All dimensions in <code>offset_dims</code> are unique and sorted in ascending          order.</li> <li>(C5) \\(0 \\le\\) <code>offset_dims</code>[i] \\(\\lt\\) rank(<code>result</code>) \\(\\forall i\\)          such that \\(0 \\le\\) i \\(\\lt\\) size(<code>offset_dims</code>).</li> <li>(C6) All dimensions in <code>collapsed_slice_dims</code> are unique and sorted in          ascending order.</li> <li>(C7) \\(0 \\le\\) <code>collapsed_slice_dims</code>[i] \\(\\lt\\) size(<code>slice_sizes</code>)           \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size(<code>collapsed_slice_dims</code>).</li> <li>(C8) <code>slice_sizes</code>[i] \\(\\le\\) 1 \\(\\forall i \\in\\) <code>collapsed_slice_dims</code>.</li> <li>(C9) All dimensions in <code>start_index_map</code> are unique.</li> <li>(C10) \\(0 \\le\\) <code>start_index_map</code>[i] \\(\\lt\\) rank(<code>operand</code>) \\(\\forall i\\)          such that \\(0 \\le\\) i \\(\\lt\\) size(<code>start_index_map</code>).</li> <li>(C11) size(<code>slice_sizes</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C12) \\(0 \\le\\) <code>slice_sizes</code>[i] \\(\\le\\) dim(<code>operand</code>, i) \\(\\forall i\\)           such that \\(0 \\le\\) i \\(\\lt\\) size(<code>slice_sizes</code>).</li> <li>(C13) <code>shape(result)</code> \\(=\\) <code>combine(batch_dim_sizes, offset_dim_sizes)</code>           where:<ul> <li><code>batch_dim_sizes</code> = <code>shape(start_indices)</code> except that the dimension size   of <code>start_indices</code> corresponding to <code>index_vector_dim</code> is not included.</li> <li><code>offset_dim_sizes</code> = <code>shape(slice_sizes)</code> except that the dimension sizes   in <code>slice_sizes</code> corresponding to <code>collapsed_slice_dims</code> are not included.</li> <li><code>combine</code> puts <code>batch_dim_sizes</code> at axes corresponding to <code>batch_dims</code> and  <code>offset_dim_sizes</code> at axes corresponding to <code>offset_dims</code>.</li> </ul> </li> <li>(C15) <code>operand</code> and <code>result</code> have the same element type.</li> </ul>"},{"location":"spec/#examples_36","title":"Examples","text":"<pre><code>// %operand: [\n//            [[1, 2], [3, 4], [5, 6], [7, 8]],\n//            [[9, 10],[11, 12], [13, 14], [15, 16]],\n//            [[17, 18], [19, 20], [21, 22], [23, 24]]\n//           ]\n// %start_indices: [\n//                  [[0, 0], [1, 0], [2, 1]],\n//                  [[0, 1], [1, 1], [0, 2]]\n//                 ]\n%result = \"stablehlo.gather\"(%operand, %start_indices) {\n  dimension_numbers = #stablehlo.gather&lt;\n    offset_dims = [2, 3],\n    collapsed_slice_dims = [0],\n    start_index_map = [1, 0],\n    index_vector_dim = 2&gt;,\n  slice_sizes = dense&lt;[1, 2, 2]&gt; : tensor&lt;3xi64&gt;,\n  indices_are_sorted = false\n} : (tensor&lt;3x4x2xi32&gt;, tensor&lt;2x3x2xi64&gt;) -&gt; tensor&lt;2x3x2x2xi32&gt;\n// %result: [\n//            [\n//              [[1, 2], [3, 4]],\n//              [[3, 4], [5, 6]],\n//              [[13, 14], [15, 16]]\n//            ],\n//            [\n//              [[9, 10], [11, 12]],\n//              [[11, 12], [13, 14]],\n//              [[17, 18], [19, 20]]\n//            ]\n//          ]\n</code></pre>"},{"location":"spec/#get_dimension_size","title":"get_dimension_size","text":""},{"location":"spec/#semantics_37","title":"Semantics","text":"<p>Produces the size of the given <code>dimension</code> of the <code>operand</code>.</p>"},{"location":"spec/#inputs_37","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>dimension</code> constant of type <code>si64</code>"},{"location":"spec/#outputs_37","title":"Outputs","text":"Name Type     <code>result</code> 0-dimensional tensor of type <code>si32</code>"},{"location":"spec/#constraints_35","title":"Constraints","text":"<ul> <li>(C1) 0 \\(\\le\\) <code>dimension</code> \\(\\lt\\) <code>rank(operand)</code>.     todo</li> </ul>"},{"location":"spec/#examples_37","title":"Examples","text":"<pre><code>// %operand: [[1, 2, 3], [4, 5, 6]]\n%result = \"stablehlo.get_dimension_size\"(%operand) {\n  dimension = 1 : i64\n} : (tensor&lt;2x3xf32&gt;) -&gt; tensor&lt;i32&gt;\n// %result: 3\n</code></pre>"},{"location":"spec/#get_tuple_element","title":"get_tuple_element","text":""},{"location":"spec/#semantics_38","title":"Semantics","text":"<p>Extracts element at <code>index</code> position of the <code>operand</code> tuple and produces a <code>result</code>.</p>"},{"location":"spec/#inputs_38","title":"Inputs","text":"Name Type     <code>operand</code> tuple   <code>index</code> constant of type <code>si32</code>"},{"location":"spec/#outputs_38","title":"Outputs","text":"Name Type     <code>result</code> any supported type"},{"location":"spec/#constraints_36","title":"Constraints","text":"<ul> <li>(C1) 0 \\(\\le\\) <code>index</code> \\(\\lt\\) size(<code>operand</code>).</li> <li>(C2) type(<code>operand[index]</code>) \\(=\\) type(<code>result</code>).</li> </ul>"},{"location":"spec/#examples_38","title":"Examples","text":"<pre><code>// %operand: ([1.0, 2.0], (3))\n%result = \"stablehlo.get_tuple_element\"(%operand) {\n  index = 0 : i32\n} : (tuple&lt;tensor&lt;2xf32&gt;, tuple&lt;tensor&lt;i32&gt;&gt;&gt;) -&gt; tensor&lt;2xf32&gt;\n// %result: [1.0, 2.0]\n</code></pre>"},{"location":"spec/#if","title":"if","text":""},{"location":"spec/#semantics_39","title":"Semantics","text":"<p>Produces the output from executing exactly one function from <code>true_branch</code> or <code>false_branch</code> depending on the value of <code>pred</code>. Formally, if <code>pred</code> is <code>true</code>, output of <code>true_branch</code> is returned, else if pred is <code>false</code>, output of <code>false_branch</code> is returned.</p>"},{"location":"spec/#inputs_39","title":"Inputs","text":"Name Type     <code>pred</code> 1-dimensional tensor constant of type <code>i1</code>   <code>true_branch</code> function   <code>false_branch</code> function"},{"location":"spec/#outputs_39","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors or tokens"},{"location":"spec/#constraints_37","title":"Constraints","text":"<ul> <li>(C1) <code>true_branch</code> and <code>false_branch</code> have 0 inputs.</li> <li>(C2) <code>true_branch</code> and <code>false_branch</code> have the same output types.</li> <li>(C3) For all <code>i</code>, <code>type(results[i]) = type(true_branch).outputs[i]</code>.</li> </ul>"},{"location":"spec/#examples_39","title":"Examples","text":"<pre><code>// %result_true_branch: 10\n// %result_false_branch: 11\n// %pred: true\n%result = \"stablehlo.if\"(%pred) ({\n  \"stablehlo.return\"(%result_true_branch) : (tensor&lt;i32&gt;) -&gt; ()\n}, {\n  \"stablehlo.return\"(%result_false_branch) : (tensor&lt;i32&gt;) -&gt; ()\n}) : (tensor&lt;i1&gt;) -&gt; tensor&lt;i32&gt;\n// %result: 10\n</code></pre>"},{"location":"spec/#imag","title":"imag","text":""},{"location":"spec/#semantics_40","title":"Semantics","text":"<p>Extracts the imaginary part, element-wise, from the <code>operand</code> and produces a <code>result</code> tensor. More formally, for each element <code>x</code>: <code>imag(x) = is_complex(x) ? x.imag : 0.0</code>.</p>"},{"location":"spec/#inputs_40","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_40","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_38","title":"Constraints","text":"<ul> <li>(C1) shape(<code>result</code>) = shape(<code>operand</code>).</li> <li>(C2) element_type(<code>result</code>) \\(=\\)<ul> <li>element_type(<code>operand</code>) if it's a floating-point type.</li> <li>real_type(element_type(<code>operand</code>)) otherwise.</li> </ul> </li> </ul>"},{"location":"spec/#examples_40","title":"Examples","text":"<pre><code>// %operand: [(1.0, 2.0), (3.0, 4.0)]\n%result = \"stablehlo.imag\"(%operand) : (tensor&lt;2xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;2xf32&gt;\n// %result: [2.0, 4.0]\n</code></pre>"},{"location":"spec/#infeed","title":"infeed","text":""},{"location":"spec/#semantics_41","title":"Semantics","text":"<p>Reads data from the infeed and produces <code>results</code>.</p> <p>Semantics of <code>infeed_config</code> is implementation-defined.</p> <p><code>results</code> consist of payload values which come first and a token which comes last. The operation produces a token to reify the side effect of this operation as a value that other operations can take a data dependency on.</p>"},{"location":"spec/#inputs_41","title":"Inputs","text":"Name Type     <code>token</code> <code>token</code>   <code>infeed_config</code> constant of type <code>string</code>"},{"location":"spec/#outputs_41","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors or tokens"},{"location":"spec/#constraints_39","title":"Constraints","text":"<ul> <li>(C1) size(<code>results</code>) \\(\\ge\\) 1.</li> <li>(C2) type(<code>results</code>[-1]) \\(=\\) <code>token</code>.</li> <li>-- Verify layout in     InfeedOp --</li> </ul>"},{"location":"spec/#examples_41","title":"Examples","text":"<pre><code>%results0, %results1 = \"stablehlo.infeed\"(%token) {\n  infeed_config = \"\"\n} : (!stablehlo.token) -&gt; (tensor&lt;3x3x3xi32&gt;, !stablehlo.token)\n</code></pre>"},{"location":"spec/#iota","title":"iota","text":""},{"location":"spec/#semantics_42","title":"Semantics","text":"<p>Fills an <code>output</code> tensor with values in increasing order starting from zero along the <code>iota_dimension</code> dimension. More formally, <code>output[i0, ..., id, ..., iR-1] = id</code>, where <code>d</code> is equal to <code>iota_dimension</code>.</p>"},{"location":"spec/#inputs_42","title":"Inputs","text":"Name Type     <code>iota_dimension</code> <code>si64</code>"},{"location":"spec/#outputs_42","title":"Outputs","text":"Name Type     <code>output</code> tensor of integer, floating-point or complex type"},{"location":"spec/#constraints_40","title":"Constraints","text":"<ul> <li>(C1) 0 \\(\\le\\) <code>iota_dimension</code> \\(\\lt\\) <code>rank(output)</code>.</li> </ul>"},{"location":"spec/#examples_42","title":"Examples","text":"<pre><code>%output = \"stablehlo.iota\"() {\n  iota_dimension = 0 : i64\n} : () -&gt; tensor&lt;4x5xi32&gt;\n// %output: [\n//           [0, 0, 0, 0, 0],\n//           [1, 1, 1, 1, 1],\n//           [2, 2, 2, 2, 2],\n//           [3, 3, 3, 3, 3]\n//          ]\n\n%output = \"stablehlo.iota\"() {\n  iota_dimension = 1 : i64\n} : () -&gt; tensor&lt;4x5xi32&gt;\n// %output: [\n//           [0, 1, 2, 3, 4],\n//           [0, 1, 2, 3, 4],\n//           [0, 1, 2, 3, 4],\n//           [0, 1, 2, 3, 4]\n//          ]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#is_finite","title":"is_finite","text":""},{"location":"spec/#semantics_43","title":"Semantics","text":"<p>Performs element-wise check whether the value in <code>x</code> is finite (i.e. is neither +Inf, -Inf, nor NaN) and produces a <code>y</code> tensor. Implements the <code>isFinite</code> operation from the IEEE-754 specification.</p>"},{"location":"spec/#inputs_43","title":"Inputs","text":"Name Type     <code>x</code> tensor of floating-point type"},{"location":"spec/#outputs_43","title":"Outputs","text":"Name Type     <code>y</code> tensor of boolean type"},{"location":"spec/#constraints_41","title":"Constraints","text":"<ul> <li>(C1) <code>x</code> and <code>y</code> have the same shape.</li> </ul>"},{"location":"spec/#examples_43","title":"Examples","text":"<pre><code>// Logical values: -Inf, +Inf, NaN, ...\n// %x: [0xFF800000, 0x7F800000, 0x7FFFFFFF, -10.0, -0.0, 0.0, 10.0]\n%y = \"stablehlo.is_finite\"(%x) : (tensor&lt;7xf32&gt;) -&gt; tensor&lt;7xi1&gt;\n// %y: [false, false, false, true, true, true, true]\n</code></pre>"},{"location":"spec/#log","title":"log","text":""},{"location":"spec/#semantics_44","title":"Semantics","text":"<p>Performs element-wise logarithm operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>log</code> from IEEE-754.</li> <li>For complex numbers: complex logarithm.</li> </ul>"},{"location":"spec/#inputs_44","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_44","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_42","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_44","title":"Examples","text":"<pre><code>// %operand: [[1.0, 2.0], [3.0, 4.0]]\n%result = \"stablehlo.log\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[0.0, 0.69314718], [1.09861229, 1.38629436]]\n\n// %operand: (1.0, 2.0)\n%result = \"stablehlo.log\"(%operand) : (tensor&lt;complex&lt;f32&gt;&gt;) -&gt; tensor&lt;complex&lt;f32&gt;&gt;\n// %result: (0.80471896, 1.10714871)\n</code></pre>"},{"location":"spec/#log_plus_one","title":"log_plus_one","text":""},{"location":"spec/#semantics_45","title":"Semantics","text":"<p>Performs element-wise logarithm plus one operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>logp1</code> from IEEE-754.</li> <li>For complex numbers: complex logarithm plus one.</li> </ul>"},{"location":"spec/#inputs_45","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_45","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_43","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_45","title":"Examples","text":"<pre><code>// %operand: [-2.0, -0.0, -0.999, 7.0, 6.38905621, 15.0]\n%result = \"stablehlo.log_plus_one\"(%operand) : (tensor&lt;6xf32&gt;) -&gt; tensor&lt;6xf32&gt;\n// %result: [-nan, 0.0, -6.90776825, 2.07944155, 2.0, 2.77258873]\n</code></pre>"},{"location":"spec/#logistic","title":"logistic","text":""},{"location":"spec/#semantics_46","title":"Semantics","text":"<p>Performs element-wise logistic operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>division(1, addition(1, exp(-x)))</code> from IEEE-754.</li> <li>For complex numbers: complex logistic.</li> </ul>"},{"location":"spec/#inputs_46","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_46","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_44","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_46","title":"Examples","text":"<pre><code>// %operand: [[0.0, 1.0], [2.0, 3.0]]\n%result = \"stablehlo.logistic\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[0.5, 0.73105858], [0.88079708, 0.95257413]]\n\n// %operand: (1.0, 2.0)\n%result = \"stablehlo.logistic\"(%operand) : (tensor&lt;complex&lt;f32&gt;&gt;) -&gt; tensor&lt;complex&lt;f32&gt;&gt;\n// %result: (1.02141536, 0.40343871)\n</code></pre>"},{"location":"spec/#map","title":"map","text":""},{"location":"spec/#semantics_47","title":"Semantics","text":"<p>Applies a map function <code>computation</code> to <code>inputs</code> along the <code>dimensions</code> and produces a <code>result</code> tensor.</p> <p>More formally, <code>result[i0, ..., iR-1] = computation(inputs[0][i0, ..., iR-1],</code> <code>..., inputs[N-1][i0, ..., iR-1])</code>.</p>"},{"location":"spec/#inputs_47","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of tensors   <code>dimensions</code> 1-dimensional tensor constant of type <code>si64</code>   <code>computation</code> function"},{"location":"spec/#outputs_47","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_45","title":"Constraints","text":"<ul> <li>(C1) All <code>inputs</code> and <code>result</code> have the same shape.</li> <li>(C2) size(<code>inputs</code>) \\(=\\) N \\(\\ge\\) 1.</li> <li>(C3) <code>dimensions = [0, ..., R-1]</code>, where <code>R</code> \\(=\\) rank(<code>inputs[0]</code>).</li> <li>(C4) <code>computation</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;) -&gt; tensor&lt;E'&gt;</code>     where <code>Ek</code> \\(=\\) element_type(<code>inputs[k]</code>) and <code>E'</code> \\(=\\)     element_type(<code>result</code>).</li> </ul>"},{"location":"spec/#examples_47","title":"Examples","text":"<pre><code>// %input0: [[0, 1], [2, 3]]\n// %input1: [[4, 5], [6, 7]]\n%result = \"stablehlo.map\"(%input0, %input1) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor&lt;i32&gt;\n    stablehlo.return %0 : tensor&lt;i32&gt;\n}) {\n  dimensions = dense&lt;[0, 1]&gt; : tensor&lt;2xi64&gt;\n} : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[0, 5], [12, 21]]\n</code></pre>"},{"location":"spec/#maximum","title":"maximum","text":""},{"location":"spec/#semantics_48","title":"Semantics","text":"<p>Performs element-wise max operation on tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical OR.</li> <li>For integers: integer maximum.</li> <li>For floats: <code>maximum</code> from IEEE-754.</li> <li>For complex numbers: lexicographic maximum for the <code>(real, imaginary)</code> pair.</li> </ul>"},{"location":"spec/#inputs_48","title":"Inputs","text":"Name Type     <code>lhs</code> tensor   <code>rhs</code> tensor"},{"location":"spec/#outputs_48","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_46","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_48","title":"Examples","text":"<pre><code>// %lhs: [[1, 2], [7, 8]]\n// %rhs: [[5, 6], [3, 4]]\n%result = \"stablehlo.maximum\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[5, 6], [7, 8]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#minimum","title":"minimum","text":""},{"location":"spec/#semantics_49","title":"Semantics","text":"<p>Performs element-wise min operation on tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical AND.</li> <li>For integers: integer minimum.</li> <li>For floats: <code>minimum</code> from IEEE-754.</li> <li>For complex numbers: lexicographic minimum for the <code>(real, imaginary)</code> pair.</li> </ul>"},{"location":"spec/#inputs_49","title":"Inputs","text":"Name Type     <code>lhs</code> tensor   <code>rhs</code> tensor"},{"location":"spec/#outputs_49","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_47","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_49","title":"Examples","text":"<pre><code>// %lhs: [[1, 2], [7, 8]]\n// %rhs: [[5, 6], [3, 4]]\n%result = \"stablehlo.minimum\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[1, 2], [3, 4]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#multiply","title":"multiply","text":""},{"location":"spec/#semantics_50","title":"Semantics","text":"<p>Performs element-wise product of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical AND.</li> <li>For integers: integer multiplication.</li> <li>For floats: <code>multiplication</code> from IEEE-754.</li> <li>For complex numbers: complex multiplication.</li> </ul>"},{"location":"spec/#inputs_50","title":"Inputs","text":"Name Type     <code>lhs</code> tensor   <code>rhs</code> tensor"},{"location":"spec/#outputs_50","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_48","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_50","title":"Examples","text":"<pre><code>// %lhs: [[1, 2], [3, 4]]\n// %rhs: [[5, 6], [7, 8]]\n%result = \"stablehlo.multiply\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[5, 12], [21, 32]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#negate","title":"negate","text":""},{"location":"spec/#semantics_51","title":"Semantics","text":"<p>Performs element-wise negation of <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For signed integers: integer negation.</li> <li>For unsigned integers: bitcast to signed integer, integer negation, bitcast     back to unsigned integer.</li> <li>For floats: <code>negate</code> from IEEE-754.</li> <li>For complex numbers: complex negation.</li> </ul>"},{"location":"spec/#inputs_51","title":"Inputs","text":"Name Type     <code>operand</code> tensor of integer, floating-point, or complex type"},{"location":"spec/#outputs_51","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer, floating-point, or complex type"},{"location":"spec/#constraints_49","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_51","title":"Examples","text":"<pre><code>// Negation operation with integer Tensors\n// %operand: [0, -2]\n%result = \"stablehlo.negate\"(%operand) : (tensor&lt;2xi32&gt;) -&gt; tensor&lt;2xi32&gt;\n// %result: [0, 2]\n\n// Negation operation with with complex tensors\n// %operand: (2.5, 0.0)\n%result = \"stablehlo.negate\"(%operand) : (tensor&lt;1xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;1xcomplex&lt;f32&gt;&gt;\n// %result: [-2.5, -0.0]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#not","title":"not","text":""},{"location":"spec/#semantics_52","title":"Semantics","text":"<p>Performs element-wise NOT of tensor <code>operand</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical NOT.</li> <li>For integers: bitwise NOT.</li> </ul>"},{"location":"spec/#arguments","title":"Arguments","text":"Name Type     <code>operand</code> tensor of boolean or integer type"},{"location":"spec/#outputs_52","title":"Outputs","text":"Name Type     <code>result</code> tensor of boolean or integer type"},{"location":"spec/#constraints_50","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_52","title":"Examples","text":"<pre><code>// Bitwise operation with with integer tensors\n// %operand: [[1, 2], [3, 4]]\n%result = \"stablehlo.not\"(%operand) : (tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[-2, -3], [-4, -5]]\n\n// Bitwise operation with with boolean tensors\n// %operand: [true, false]\n%result = \"stablehlo.not\"(%operand) : (tensor&lt;2xi1&gt;) -&gt; tensor&lt;2xi1&gt;\n// %result: [false, true]\n</code></pre>"},{"location":"spec/#optimization_barrier","title":"optimization_barrier","text":""},{"location":"spec/#semantics_53","title":"Semantics","text":"<p>Ensures that the operations that produce the <code>operand</code> are executed before any operations that depend on the <code>result</code> and prevents compiler transformations from moving operations across the barrier. Other than that, the operation is an identity, i.e. <code>result</code> = <code>operand</code>.</p>"},{"location":"spec/#arguments_1","title":"Arguments","text":"Name Type     <code>operand</code> variadic number of tensors or tokens"},{"location":"spec/#outputs_53","title":"Outputs","text":"Name Type     <code>result</code> variadic number of tensors or tokens"},{"location":"spec/#constraints_51","title":"Constraints","text":"<ul> <li>(C1) size(<code>operand</code>) \\(=\\) size(<code>result</code>).</li> <li>(C2) type(<code>operand[i]</code>) \\(=\\) type(<code>result[i]</code>) for all i.</li> </ul>"},{"location":"spec/#examples_53","title":"Examples","text":"<pre><code>// %operand0: 0.0\n// %operand1: 1.0\n%result0, %result1 = \"stablehlo.optimization_barrier\"(%operand0, %operand1) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; (tensor&lt;f32&gt;, tensor&lt;f32&gt;)\n// %result0: 0.0\n// %result1: 1.0\n</code></pre>"},{"location":"spec/#or","title":"or","text":""},{"location":"spec/#semantics_54","title":"Semantics","text":"<p>Performs element-wise OR of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical OR.</li> <li>For integers: bitwise OR.</li> </ul>"},{"location":"spec/#inputs_52","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer or boolean type   <code>rhs</code> tensor of integer or boolean type"},{"location":"spec/#outputs_54","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer or boolean type"},{"location":"spec/#constraints_52","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_54","title":"Examples","text":"<pre><code>// Bitwise operation with with integer tensors\n// %lhs: [[1, 2], [3, 4]]\n// %rhs: [[5, 6], [7, 8]]\n%result = \"stablehlo.or\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[5, 6], [7, 12]]\n\n// Logical operation with with boolean tensors\n// %lhs: [[false, false], [true, true]]\n// %rhs: [[false, true], [false, true]]\n%result = \"stablehlo.or\"(%lhs, %rhs) : (tensor&lt;2x2xi1&gt;, tensor&lt;2x2xi1&gt;) -&gt; tensor&lt;2x2xi1&gt;\n// %result: [[false, true], [true, true]]\n</code></pre>"},{"location":"spec/#outfeed","title":"outfeed","text":""},{"location":"spec/#semantics_55","title":"Semantics","text":"<p>Writes <code>inputs</code> to the outfeed and produces a <code>result</code> token.</p> <p>Semantics of <code>outfeed_config</code> is implementation-defined.</p> <p>The operation takes a token and produces a token to reify its side effects as a value that other operations can take a data dependency on.</p>"},{"location":"spec/#inputs_53","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of tensors   <code>token</code> <code>token</code>   <code>outfeed_config</code> constant of type <code>string</code>"},{"location":"spec/#outputs_55","title":"Outputs","text":"Name Type     <code>result</code> <code>token</code>"},{"location":"spec/#examples_55","title":"Examples","text":"<pre><code>%result = \"stablehlo.outfeed\"(%input0, %token) {\n  outfeed_config = \"\"\n} : (tensor&lt;3x3x3xi32&gt;, !stablehlo.token) -&gt; !stablehlo.token\n</code></pre>"},{"location":"spec/#pad","title":"pad","text":""},{"location":"spec/#semantics_56","title":"Semantics","text":"<p>Expands <code>operand</code> by padding around the tensor as well as between the elements of the tensor with the given <code>padding_value</code>.</p> <p><code>edge_padding_low</code> and <code>edge_padding_high</code> specify the amount of padding added at the low-end (next to index 0) and the high-end (next to the highest index) of each dimension respectively. The amount of padding can be negative, where the absolute value of negative padding indicates the number of elements to remove from the specified dimension.</p> <p><code>interior_padding</code> specifies the amount of padding added between any two elements in each dimension which may not be negative. Interior padding occurs before edge padding such that negative edge padding will remove elements from the interior-padded operand.</p> <p>More formally, <code>result[i0, ..., iR-1]</code> is equal to:</p> <ul> <li><code>operand[j0, ..., jR-1]</code> if     <code>id = edge_padding_low[d] + jd * (interior_padding[d] + 1)</code>.</li> <li><code>padding_value[]</code> otherwise.</li> </ul>"},{"location":"spec/#inputs_54","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>padding_value</code> 0-dimensional tensor   <code>edge_padding_low</code> 1-dimensional tensor constant of type <code>si64</code>   <code>edge_padding_high</code> 1-dimensional tensor constant of type <code>si64</code>   <code>interior_padding</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_56","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_53","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code>, <code>padding_value</code>, <code>result</code> have the same element type.</li> <li>(C2) <code>edge_padding_low</code>, <code>edge_padding_high</code>, <code>interior_padding</code> have the   size equal to <code>operand</code>'s rank.</li> <li>(C3) 0 \\(\\le\\) <code>interior_padding[i]</code> for all <code>i</code> values in <code>interior_padding</code>.</li> <li>(C4) 0 \\(\\le\\) <code>dim(result, i)</code> for all <code>i</code>th dimension of <code>operand</code>, where   <code>dim(result, i) = di + max(di - 1, 0) * interior_padding[i] + edge_padding_low[i] + edge_padding_high[i]</code>   and <code>di = dim(operand, i)</code>.</li> </ul>"},{"location":"spec/#examples_56","title":"Examples","text":"<pre><code>// %operand: [\n//            [1, 2, 3],\n//            [4, 5, 6]\n//           ]\n// %padding_value: 0\n%result = \"stablehlo.pad\"(%operand, %padding_value) {\n  edge_padding_low = dense&lt;[0, 1]&gt; : tensor&lt;2xi64&gt;,\n  edge_padding_high = dense&lt;[2, 1]&gt; : tensor&lt;2xi64&gt;,\n  interior_padding = dense&lt;[1, 2]&gt; : tensor&lt;2xi64&gt;\n} : (tensor&lt;2x3xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;5x9xi32&gt;\n// %result: [\n//           [0, 1, 0, 0, 2, 0, 0, 3, 0],\n//           [0, 0, 0, 0, 0, 0, 0, 0, 0],\n//           [0, 4, 0, 0, 5, 0, 0, 6, 0],\n//           [0, 0, 0, 0, 0, 0, 0, 0, 0],\n//           [0, 0, 0, 0, 0, 0, 0, 0, 0]\n//          ]\n</code></pre>"},{"location":"spec/#partition_id","title":"partition_id","text":""},{"location":"spec/#semantics_57","title":"Semantics","text":"<p>Produces <code>partition_id</code> of the current process.</p>"},{"location":"spec/#outputs_57","title":"Outputs","text":"Name Type     <code>result</code> 0-dimensional tensor of type <code>ui32</code>"},{"location":"spec/#examples_57","title":"Examples","text":"<pre><code>%result = \"stablehlo.partition_id\"() : () -&gt; tensor&lt;ui32&gt;\n</code></pre>"},{"location":"spec/#popcnt","title":"popcnt","text":""},{"location":"spec/#semantics_58","title":"Semantics","text":"<p>Performs element-wise count of the number of bits set in the <code>operand</code> tensor and produces a <code>result</code> tensor.</p>"},{"location":"spec/#inputs_55","title":"Inputs","text":"Name Type     <code>operand</code> tensor of integer type"},{"location":"spec/#outputs_58","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer type"},{"location":"spec/#constraints_54","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_58","title":"Examples","text":"<pre><code>// %operand: [0, 1, 2, 127]\n%result = \"stablehlo.popcnt\"(%operand) : (tensor&lt;4xi8&gt;) -&gt; tensor&lt;4xi8&gt;\n// %result: [0, 1, 1, 7]\n</code></pre>"},{"location":"spec/#power","title":"power","text":""},{"location":"spec/#semantics_59","title":"Semantics","text":"<p>Performs element-wise exponentiation of <code>lhs</code> tensor by <code>rhs</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For integers: integer exponentiation.</li> <li>For floats: <code>pow</code> from IEEE-754.</li> <li>For complex numbers: complex exponentiation.</li> </ul>"},{"location":"spec/#inputs_56","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer, floating-point, or complex type   <code>rhs</code> tensor of integer, floating-point, or complex type"},{"location":"spec/#outputs_59","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer, floating-point, or complex type"},{"location":"spec/#constraints_55","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code>, and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_59","title":"Examples","text":"<pre><code>// %lhs: [-2.0, -0.0, -36.0, 5.0, 3.0, 10000.0]\n// %rhs: [2.0, 2.0, 1.1, 2.0, -1.0, 10.0]\n%result = \"stablehlo.power\"(%lhs, %rhs) : (tensor&lt;6xf32&gt;, tensor&lt;6xf32&gt;) -&gt; tensor&lt;6xf32&gt;\n// %result: [4.0, 0.0, -nan, 25.0, 0.333333343, inf]\n</code></pre>"},{"location":"spec/#real","title":"real","text":""},{"location":"spec/#semantics_60","title":"Semantics","text":"<p>Extracts the real part, element-wise, from the <code>operand</code> and produces a <code>result</code> tensor. More formally, for each element <code>x</code>: <code>real(x) = is_complex(x) ? x.real : x</code>.</p>"},{"location":"spec/#inputs_57","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_60","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_56","title":"Constraints","text":"<ul> <li>(C1) shape(<code>result</code>) = shape(<code>operand</code>).</li> <li>(C2) element_type(<code>result</code>) \\(=\\)<ul> <li>element_type(<code>operand</code>) if it's a floating-point type.</li> <li>real_type(element_type(<code>operand</code>)) otherwise.</li> </ul> </li> </ul>"},{"location":"spec/#examples_60","title":"Examples","text":"<pre><code>// %operand: [(1.0, 2.0), (3.0, 4.0)]\n%result = \"stablehlo.real\"(%operand) : (tensor&lt;2xcomplex&lt;f32&gt;&gt;) -&gt; tensor&lt;2xf32&gt;\n// %result: [1.0, 3.0]\n</code></pre>"},{"location":"spec/#recv","title":"recv","text":""},{"location":"spec/#semantics_61","title":"Semantics","text":"<p>Receives data from a channel with <code>channel_id</code> and produces <code>results</code>.</p> <p>If <code>is_host_transfer</code> is <code>true</code>, then the operation transfers data from the host. Otherwise, it transfers data from another device. What this means is implementation-defined.</p> <p><code>results</code> consist of payload values which come first and a token which comes last. The operation produces a token to reify its side effects as a value that other operations can take a data dependency on.</p>"},{"location":"spec/#inputs_58","title":"Inputs","text":"Name Type     <code>token</code> <code>token</code>   <code>channel_id</code> constant of type <code>si64</code>   <code>channel_type</code> enum of <code>DEVICE_TO_DEVICE</code> and <code>HOST_TO_DEVICE</code>   <code>is_host_transfer</code> constant of type <code>i1</code>"},{"location":"spec/#outputs_61","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors or tokens"},{"location":"spec/#constraints_57","title":"Constraints","text":"<ul> <li>(C1) todo <code>channel_type</code> must be<ul> <li><code>HOST_TO_DEVICE</code>, if <code>is_host_transfer</code> \\(=\\) <code>true</code>,</li> <li><code>DEVICE_TO_DEVICE</code>, otherwise.</li> </ul> </li> <li>(C2) size(<code>results</code>) \\(\\ge\\) 1.</li> <li>(C3) type(<code>results</code>[-1]) \\(=\\) <code>token</code>.</li> </ul>"},{"location":"spec/#examples_61","title":"Examples","text":"<pre><code>%results0, %results1 = \"stablehlo.recv\"(%token) {\n  // channel_id = 5 : i64,\n  // channel_type = #stablehlo&lt;channel_type HOST_TO_DEVICE&gt;,\n  channel_handle = #stablehlo.channel_handle&lt;handle = 5, type = 3&gt;,\n  is_host_transfer = true\n} : (!stablehlo.token) -&gt; (tensor&lt;3x4xi32&gt;, !stablehlo.token)\n</code></pre>"},{"location":"spec/#reduce","title":"reduce","text":""},{"location":"spec/#semantics_62","title":"Semantics","text":"<p>Applies a reduction function <code>body</code> to <code>inputs</code> and <code>init_values</code> along the <code>dimensions</code> and produces a <code>result</code> tensor.</p> <p>The order of reductions is implementation-defined, which means that <code>body</code> and <code>init_values</code> must form a monoid to guarantee that the operation produces the same results for all inputs on all implementations.</p> <p>However, this condition doesn't hold for many popular reductions. E.g. floating-point addition for <code>body</code> and zero for <code>init_values</code> don't actually form a monoid because floating-point addition is not associative.</p> <p>More formally, <code>results[:][j0, ..., jR-1] = reduce(input_slices)</code> where:</p> <ul> <li><code>input_slices</code> = <code>inputs[:][j0, ..., :, ..., jR-1]</code>, where <code>:</code> are inserted     at <code>dimensions</code>.</li> <li><code>reduce(input_slices)</code> = <code>exec(schedule)</code> for some binary tree <code>schedule</code>     where:<ul> <li><code>exec(node)</code> = <code>body(exec(node.left), exec(node.right))</code>.</li> <li><code>exec(leaf)</code> = <code>leaf.value</code>.</li> </ul> </li> <li><code>schedule</code> is an implementation-defined full binary tree whose in-order     traversal consists of:<ul> <li><code>input_slices[:][index]</code> values, for all <code>index</code> in the index space   of <code>input_slices</code>, in the ascending lexicographic order of <code>index</code>.</li> <li>Interspersed with an implementation-defined amount of <code>init_values</code>   at implementation-defined positions.</li> </ul> </li> </ul>"},{"location":"spec/#inputs_59","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of tensors   <code>init_values</code> variadic number of 0-dimensional tensors   <code>dimensions</code> 1-dimensional tensor constant of type <code>si64</code>   <code>body</code> function"},{"location":"spec/#outputs_62","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors"},{"location":"spec/#constraints_58","title":"Constraints","text":"<ul> <li>(C1) All <code>inputs</code> have the same shape.</li> <li>(C2) element_type(<code>inputs[k]</code>) \\(=\\) element_type(<code>init_values[k]</code>) \\(=\\)   element_type(<code>results[k]</code>) for all <code>k</code> \\(\\in\\) [0, N).</li> <li>(C3) size(<code>inputs</code>) \\(=\\) size(<code>init_values</code>) \\(=\\) size(<code>results</code>) \\(=\\) N where   N &gt;= 1.</li> <li>(C4) 0 \\(\\le\\) <code>dimensions[d]</code> \\(\\lt\\) rank(<code>inputs[0][d]</code>) for all dimension   <code>d</code>.</li> <li>(C5) All dimensions in <code>dimensions</code> are unique.</li> <li>(C6) <code>body</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;E0&gt;, ...,</code> <code>tensor&lt;EN-1&gt;) -&gt; (tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;)</code> where   <code>Ek = element_type(inputs[k])</code>.</li> <li>(C7) shape(<code>results[k]</code>) \\(=\\) shape(<code>inputs[k]</code>) except that the dimension   sizes of <code>inputs[k]</code> corresponding to <code>dimensions</code> are not included.</li> </ul>"},{"location":"spec/#examples_62","title":"Examples","text":"<pre><code>// %input = [[0, 1, 2, 3, 4, 5]]\n// %init_value = 0\n%result = \"stablehlo.reduce\"(%input, %init_value) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i32&gt;) -&gt; ()\n}) {\n  dimensions = dense&lt;1&gt; : tensor&lt;1xi64&gt;\n} : (tensor&lt;1x6xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;1xi32&gt;\n// %result = [15]\n</code></pre>"},{"location":"spec/#reduce_precision","title":"reduce_precision","text":""},{"location":"spec/#semantics_63","title":"Semantics","text":"<p>Performs element-wise conversion of <code>operand</code> to another floating-point type that uses <code>exponent_bits</code> and <code>mantissa_bits</code> and back to the original floating-point type and produces a <code>result</code> tensor.</p> <p>More formally:</p> <ul> <li>The mantissa bits of the original value are updated to round the original     value to the nearest value representable with <code>mantissa_bits</code> using     <code>roundToIntegralTiesToEven</code> semantics.</li> <li>Then, if <code>mantissa_bits</code> are smaller than the number of mantissa bits of     the original value, the mantissa bits are truncated to <code>mantissa_bits</code>.</li> <li>Then, if the exponent bits of the intermediate result don't fit into the     range provided by <code>exponent_bits</code>, the intermediate result overflows to     infinity using the original sign or underflows to zero using the     original sign.</li> </ul>"},{"location":"spec/#inputs_60","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type   <code>exponent_bits</code> constant of type <code>si32</code>   <code>mantissa_bits</code> constant of type <code>si32</code>"},{"location":"spec/#outputs_63","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_59","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> <li>(C2) <code>exponent_bits</code> \\(\\ge\\) 1.</li> <li>(C3) <code>mantissa_bits</code> \\(\\ge\\) 0.</li> </ul>"},{"location":"spec/#examples_63","title":"Examples","text":"<pre><code>// Logical values: -Inf, +Inf, NaN, ...\n// %operand: [0xFF800000, 0x7F800000, 0x7FFFFFFF, 0.0, 1000.0, 1000000.0]\n%result = \"stablehlo.reduce_precision\"(%operand) {\n  exponent_bits = 5 : i32,\n  mantissa_bits = 2 : i32\n} : (tensor&lt;6xf32&gt;) -&gt; tensor&lt;6xf32&gt;\n// Logical values: -Inf, +Inf, NaN, NaN, 0.0, 1024.0, +Inf\n// %result: [0xFF800000, 0x7F800000, 0x7FFFFFFF, 0.0, 1024.0, 0x7F800000]\n</code></pre>"},{"location":"spec/#reduce_scatter","title":"reduce_scatter","text":""},{"location":"spec/#semantics_64","title":"Semantics","text":"<p>Within each process group in the StableHLO grid, performs reduction, using <code>computations</code>, over the values of the <code>operand</code> tensor from each process, splits the reduction result along <code>scatter_dimension</code> into parts, and scatters the split parts between the processes to produce the <code>result</code>.</p> <p>The operation splits the StableHLO grid into <code>process_groups</code> as follows:</p> <ul> <li><code>channel_id &lt;= 0</code> and <code>use_global_device_ids = false</code>,     <code>cross_replica(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code> and <code>use_global_device_ids = false</code>,     <code>cross_replica_and_partition(replica_groups)</code>.</li> <li><code>channel_id &gt; 0</code> and <code>use_global_device_ids = true</code>,     <code>flattened_ids(replica_groups)</code>.</li> </ul> <p>Afterwards, within each <code>process_group</code>:</p>  <ul> <li><code>reduced_value = all_reduce(operand, replica_groups, channel_id, use_global_device_ids, computation)</code>.</li> <li><code>parts@sender = split(reduced_value@sender, dim(process_groups, 1), split_dimension)</code>.</li> <li><code>result@receiver = parts@sender[receiver_index]</code> for any sender in process_group,     where <code>receiver_index = index_of(receiver, process_group)</code>.</li> </ul>"},{"location":"spec/#inputs_61","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>scatter_dimension</code> constant of type <code>si64</code>   <code>replica_groups</code> 2-dimensional tensor constant of type <code>si64</code>   <code>channel_id</code> constant of type <code>si64</code>   <code>use_global_device_ids</code> constant of type <code>i1</code>   <code>computation</code> function"},{"location":"spec/#outputs_64","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_60","title":"Constraints","text":"<ul> <li>(C1) dim(<code>operand</code>, <code>scatter_dimension</code>) % dim(<code>process_groups</code>, 1) \\(=\\) 0.</li> <li>(C2) <code>scatter_dimension</code> \\(\\in\\) [0, rank(<code>operand</code>)).</li> <li>(C3) All values in <code>replica_groups</code> are unique.</li> <li>(C4) <code>size(replica_groups)</code> depends on the process grouping strategy:<ul> <li>If <code>cross_replica</code>, <code>num_replicas</code>.</li> <li>If <code>cross_replica_and_partition</code>, <code>num_replicas</code>.</li> <li>If <code>flattened_ids</code>, <code>num_processes</code>.</li> </ul> </li> <li>(C5) \\(0 \\le\\) <code>replica_groups[i]</code> \\(\\lt\\) size(<code>replica_groups</code>) \\(\\forall i\\)          in <code>indices(replica_groups)</code>.</li> <li>(C6) If <code>use_global_device_ids = true</code>, then <code>channel_id &gt; 0</code>.          todo</li> <li>(C7) <code>computation</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; (tensor&lt;E&gt;)</code> where          <code>E = element_type(operand)</code>.</li> <li>(C8) <code>type(result) = type(operand)</code> except:<ul> <li><code>dim(result, scatter_dimension) = dim(operand, scatter_dimension) / dim(process_groups, 1)</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_64","title":"Examples","text":"<pre><code>// num_replicas: 2\n// num_partitions: 1\n// %operand@(0, 0): [\n//                   [1.0, 2.0, 3.0, 4.0],\n//                   [5.0, 6.0, 7.0, 8.0]\n//                  ]\n// %operand@(1, 0): [\n//                   [9.0, 10.0, 11.0, 12.0],\n//                   [13.0, 14.0, 15.0, 16.0]\n//                  ]\n%result = \"stablehlo.reduce_scatter\"(%operand) ({\n  ^bb0(%arg0: tensor&lt;f32&gt;, %arg1: tensor&lt;f32&gt;):\n  %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;f32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;f32&gt;\n  \"stablehlo.return\"(%0) : (tensor&lt;f32&gt;) -&gt; ()\n}) {\n  scatter_dimension = 1 : i64,\n  replica_groups = dense&lt;[[0, 1]]&gt; : tensor&lt;1x2xi64&gt;,\n  // channel_id = 0\n  channel_handle = #stablehlo.channel_handle&lt;handle = 0, type = 0&gt;\n  // use_global_device_ids = false\n} : (tensor&lt;2x4xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n//\n// %result@(0, 0): [\n//                  [10.0, 12.0],\n//                  [18.0, 20.0]\n//                 ]\n// %result@(1, 0): [\n//                  [14.0, 16.0],\n//                  [22.0, 24.0]\n//                 ]\n</code></pre>"},{"location":"spec/#reduce_window","title":"reduce_window","text":""},{"location":"spec/#semantics_65","title":"Semantics","text":"<p>Applies a reduction function <code>body</code> to windows of <code>inputs</code> and <code>init_values</code> and produces <code>results</code>.</p> <p>The following diagram shows how elements in <code>results[k]</code> are computed from <code>inputs[k]</code> using a concrete example.</p>  <p>More formally, <code>results[:][result_index] = reduce(windows, init_values, axes(inputs[:]), body)</code> where:</p>  <ul> <li><code>padded_inputs = pad(inputs[:], init_values[:], padding[:, 0], padding[:, 1], base_dilations)</code>.</li> <li><code>window_start = result_index * window_strides</code>.</li> <li><code>windows = slice(padded_inputs[:], window_start, window_start + window_dimensions, window_dilations)</code>.</li> </ul>"},{"location":"spec/#inputs_62","title":"Inputs","text":"Name Type Constraints     <code>inputs</code> variadic number of tensors (C1-C4), (C6), (C8), (C10), (C12), (C13), (C15)   <code>init_values</code> variadic number of 0-dimensional tensors (C1), (C13), (C16)   <code>window_dimensions</code> 1-dimensional tensor constant of type <code>si64</code> (C4), (C5), (C15)   <code>window_strides</code> 1-dimensional tensor constant of type <code>si64</code> (C6), (C7), (C15)   <code>base_dilations</code> 1-dimensional tensor constant of type <code>si64</code> (C8), (C9), (C15)   <code>window_dilations</code> 1-dimensional tensor constant of type <code>si64</code> (C10), (C11), (C15)   <code>padding</code> 2-dimensional tensor constant of type <code>si64</code> (C12), (C15)   <code>body</code> function (C13)"},{"location":"spec/#outputs_65","title":"Outputs","text":"Name Type Constraints     <code>results</code> variadic number of tensors (C1), (C14-C16)"},{"location":"spec/#constraints_61","title":"Constraints","text":"<ul> <li>(C1) size(<code>inputs</code>) \\(=\\) size(<code>init_values</code>) \\(=\\) size(<code>results</code>) \\(=\\) N and          N \\(\\ge\\) 1.</li> <li>(C2) All <code>inputs</code> have the same shape.</li> <li>(C3) <code>element_type(inputs[k]) = element_type(init_values[k])</code> for any k       \\(\\in\\) [0, N).</li> <li>(C4) size(<code>window_dimensions</code>) \\(=\\) rank(<code>inputs[0]</code>).</li> <li>(C5) <code>window_dimensions[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(<code>window_dimensions</code>)).</li> <li>(C6) size(<code>window_strides</code>) \\(=\\) rank(<code>inputs[0]</code>).</li> <li>(C7) <code>window_strides[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(<code>window_strides</code>)).</li> <li>(C8) size(<code>base_dilations</code>) \\(=\\) rank(<code>inputs[0]</code>).</li> <li>(C9) <code>base_dilations[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(<code>base_dilations</code>)).</li> <li>(C10) size(<code>window_dilations</code>) \\(=\\) rank(<code>inputs[0]</code>).</li> <li>(C11) <code>window_dilations[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(<code>window_dilations</code>)).</li> <li>(C12) dim(<code>padding</code>, 0) \\(=\\) rank(<code>inputs[0]</code>) and dim(<code>padding</code>, 1) = 2.</li> <li>(C13) <code>body</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;) -&gt; (tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;)</code>           where <code>Ek = element_type(inputs[0])</code>.</li> <li>(C14) All <code>results</code> have the same shape.</li> <li>(C15) <code>shape(results[0]) = num_windows</code><ul> <li><code>dilated_input_shape = shape(inputs[0]) == 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1</code>.</li> <li><code>padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1]</code>.</li> <li><code>dilated_window_shape = window_dimensions == 0 ? 0 : (window_dimensions - 1) * window_dilations + 1</code>.</li> <li><code>num_windows = (padded_input_shape == 0 || dilated_window_shape &gt; padded_input_shape) ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1</code>.</li> </ul> </li> <li>(C16) <code>element_type(results[k]) = element_type(init_values[k])</code> for any k       \\(\\in\\) [0, N).</li> </ul>"},{"location":"spec/#examples_65","title":"Examples","text":"<pre><code>// %input = [[1, 2], [3, 4], [5, 6]]\n// %init_value = 0\n%result = \"stablehlo.reduce_window\"(%input, %init_value) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i32&gt;) -&gt; ()\n}) {\n  window_dimensions = dense&lt;[2, 1]&gt; : tensor&lt;2xi64&gt;,\n  window_strides = dense&lt;[4, 1]&gt; : tensor&lt;2xi64&gt;,\n  base_dilations = dense&lt;[2, 1]&gt; : tensor&lt;2xi64&gt;,\n  window_dilations = dense&lt;[3, 1]&gt; : tensor&lt;2xi64&gt;,\n  padding = dense&lt;[[2, 1], [0, 0]]&gt; : tensor&lt;2x2xi64&gt;\n} : (tensor&lt;3x2xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result = [[0, 0], [3, 4]]\n</code></pre>"},{"location":"spec/#remainder","title":"remainder","text":""},{"location":"spec/#semantics_66","title":"Semantics","text":"<p>Performs element-wise remainder of dividend <code>lhs</code> and divisor <code>rhs</code> tensors and produces a <code>result</code> tensor.</p> <p>More formally, the sign of the result is taken from the dividend, and the absolute value of the result is always less than the divisor's absolute value. The remainder is calculated as <code>lhs - d * rhs</code>, where <code>d = stablehlo.divide</code>. For floating-point element types, this is in contrast with the <code>remainder</code> operation from IEEE-754 specification where <code>d</code> is an integral value nearest to the exact value of <code>lhs/rhs</code> with ties to even.</p>"},{"location":"spec/#inputs_63","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer, floating-point or complex type   <code>rhs</code> tensor of integer, floating-point or complex type"},{"location":"spec/#outputs_66","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer, floating-point or complex type"},{"location":"spec/#constraints_62","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_66","title":"Examples","text":"<pre><code>// %lhs: [17.1, -17.1, 17.1, -17.1]\n// %rhs: [3.0, 3.0, -3.0, -3.0]\n%result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor&lt;4xf32&gt;, tensor&lt;4xf32&gt;) -&gt; tensor&lt;4xf32&gt;\n// %result: [2.1, -2.1, 2.1, -2.1]\n\n// %lhs: [17, -17, 17, -17]\n// %rhs: [3, 3, -3, -3]\n%result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor&lt;4xi32&gt;, tensor&lt;4xi32&gt;) -&gt; tensor&lt;4xi32&gt;\n// %result: [2, -2, 2, -2]\n</code></pre>"},{"location":"spec/#replica_id","title":"replica_id","text":""},{"location":"spec/#semantics_67","title":"Semantics","text":"<p>Produces <code>replica_id</code> of the current process.</p>"},{"location":"spec/#outputs_67","title":"Outputs","text":"Name Type     <code>result</code> 0-dimensional tensor of type <code>ui32</code>"},{"location":"spec/#examples_67","title":"Examples","text":"<pre><code>%result = \"stablehlo.replica_id\"() : () -&gt; tensor&lt;ui32&gt;\n</code></pre>"},{"location":"spec/#reshape","title":"reshape","text":""},{"location":"spec/#semantics_68","title":"Semantics","text":"<p>Performs reshape of <code>operand</code> tensor to a <code>result</code> tensor. Conceptually, it amounts to keeping the same canonical representation but potentially changing the shape, e.g. from <code>tensor&lt;2x3xf32&gt;</code> to <code>tensor&lt;3x2xf32&gt;</code> or <code>tensor&lt;6xf32&gt;</code>.</p> <p>More formally, <code>result[i0, ..., iR-1] = operand[j0, ..., jR'-1]</code> where <code>i</code> and <code>j</code> have the same position in the lexicographic ordering of the index spaces of <code>result</code> and <code>operand</code>.</p>"},{"location":"spec/#inputs_64","title":"Inputs","text":"Name Type     <code>operand</code> tensor"},{"location":"spec/#outputs_68","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_63","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same element type.</li> <li>(C2) <code>operand</code> and <code>result</code> have the same number of elements.</li> </ul>"},{"location":"spec/#examples_68","title":"Examples","text":"<pre><code>// %operand: [[1, 2, 3], [4, 5, 6]]]\n%result = \"stablehlo.reshape\"(%operand) : (tensor&lt;2x3xi32&gt;) -&gt; tensor&lt;3x2xi32&gt;\n// %result: [[1, 2], [3, 4], [5, 6]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#reverse","title":"reverse","text":""},{"location":"spec/#semantics_69","title":"Semantics","text":"<p>Reverses the order of elements in the <code>operand</code> along the specified <code>dimensions</code> and produces a <code>result</code> tensor. More formally, <code>result[i0, ..., ik,..., iR-1] = operand[i0, ..., ik',..., iR-1]</code> where <code>ik + ik' = dk - 1</code> for all dimensions <code>k</code> in <code>dimensions</code>.</p>"},{"location":"spec/#inputs_65","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>dimensions</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_69","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_64","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> <li>(C2) All dimensions in <code>dimensions</code> are unique.</li> <li>(C3) For all dimensions <code>k</code> in <code>dimensions</code>, 0 \\(\\le\\) <code>dimensions[k]</code> \\(\\lt\\) <code>rank(result)</code>.</li> </ul>"},{"location":"spec/#examples_69","title":"Examples","text":"<pre><code>// Reverse along dimension 0\n\n// %operand = [[1, 2], [3, 4], [5, 6]]\n%result = \"stablehlo.reverse\"(%operand) {\n  dimensions = dense&lt;0&gt; : tensor&lt;i64&gt;\n} : (tensor&lt;3x2xi32&gt;) -&gt; tensor&lt;3x2xi32&gt;\n// %result: [[5, 6], [3, 4], [1, 2]]\n\n// Reverse along dimension 1\n\n// %operand = [[1, 2], [3, 4], [5, 6]]\n%result = \"stablehlo.reverse\"(%operand) {\n  dimensions = dense&lt;1&gt; : tensor&lt;i64&gt;\n} : (tensor&lt;3x2xi32&gt;) -&gt; tensor&lt;3x2xi32&gt;\n// %result: [[2, 1], [4, 3], [6, 5]]\n</code></pre>"},{"location":"spec/#rng","title":"rng","text":""},{"location":"spec/#semantics_70","title":"Semantics","text":"<p>Generates random numbers using the <code>rng_distribution</code> algorithm and produces a <code>result</code> tensor of a given shape <code>shape</code>.</p> <p>If <code>rng_distribution</code> \\(=\\) <code>UNIFORM</code>, then the random numbers are generated following the uniform distribution over the interval [<code>a</code>, <code>b</code>). If <code>a</code> \\(\\ge\\) <code>b</code>, the behavior is undefined.</p> <p>If <code>rng_distribution</code> \\(=\\) <code>NORMAL</code>, then the random numbers are generated following the normal distribution with mean = <code>a</code> and standard deviation = <code>b</code>. If <code>b</code> \\(\\lt\\) 0, the behavior is undefined.</p> <p>The exact way how random numbers are generated is implementation-defined. For example, they may or may not be deterministic, and they may or may not use hidden state.</p>"},{"location":"spec/#inputs_66","title":"Inputs","text":"Name Type     <code>a</code> 0-dimensional tensor of integer, boolean, or floating-point type   <code>b</code> 0-dimensional tensor of integer, boolean, or floating-point type   <code>shape</code> 1-dimensional tensor constant of type <code>si64</code>   <code>rng_distribution</code> enum of <code>UNIFORM</code> and <code>NORMAL</code>"},{"location":"spec/#outputs_70","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer, boolean, or floating-point type"},{"location":"spec/#constraints_65","title":"Constraints","text":"<ul> <li>(C1) <code>a</code>, <code>b</code>, and <code>result</code> have the same element type.</li> <li>(C2) If <code>rng_distribution = NORMAL</code>, <code>a</code>, <code>b</code>, and <code>result</code> have the same     floating-point element type.</li> <li>(C3) shape(<code>result</code>) = <code>shape</code>.</li> </ul>"},{"location":"spec/#examples_70","title":"Examples","text":"<pre><code>// %a = 0\n// %b = 2\n// %shape = [3, 3]\n%result = \"stablehlo.rng\"(%a, %b, %shape) {\n  rng_distribution = #stablehlo&lt;rng_distribution UNIFORM&gt;\n} : (tensor&lt;i32&gt;, tensor&lt;i32&gt;, tensor&lt;2xi64&gt;) -&gt; tensor&lt;3x3xi32&gt;\n// %result: [\n//           [1, 0, 1],\n//           [1, 1, 1],\n//           [0, 0, 0]\n//          ]\n</code></pre>"},{"location":"spec/#rng_bit_generator","title":"rng_bit_generator","text":""},{"location":"spec/#semantics_71","title":"Semantics","text":"<p>Returns an <code>output</code> filled with uniform random bits and an updated output state <code>output_state</code> given an initial state <code>initial_state</code> using the pseudorandom number generator algorithm <code>rng_algorithm</code>. The output is guaranteed to be deterministic function of <code>initial_state</code>, but it is not guaranteed to be deterministic between implementations.</p> <p><code>rng_algorithm</code> is one of the following:</p> <ul> <li><code>DEFAULT</code>: Implementation-defined algorithm.</li> <li><code>THREE_FRY</code>: Implementation-defined variant of the Threefry algorithm.*</li> <li><code>PHILOX</code>: Implementation-defined variant of the Philox algorithm.*</li> </ul> <p>* See: Salmon et al. SC 2011. Parallel random numbers: as easy as 1, 2, 3. </p>"},{"location":"spec/#inputs_67","title":"Inputs","text":"Name Type     <code>initial_state</code> 1-dimensional tensor of type <code>ui64</code>   <code>rng_algorithm</code> enum of <code>DEFAULT</code>, <code>THREE_FRY</code>, and <code>PHILOX</code>"},{"location":"spec/#outputs_71","title":"Outputs","text":"Name Type     <code>output_state</code> 1-dimensional tensor of type <code>ui64</code>   <code>output</code> tensor of integer or floating-point type"},{"location":"spec/#constraints_66","title":"Constraints","text":"<ul> <li>(C1) type(<code>initial_state</code>) \\(=\\) type(<code>output_state</code>).</li> <li>(C2) size(<code>initial_state</code>) depends on <code>rng_algorithm</code>:<ul> <li><code>DEFAULT</code>: implementation-defined.</li> <li><code>THREE_FRY</code>: <code>2</code>.</li> <li><code>PHILOX</code>: <code>2</code> or <code>3</code>.</li> </ul> </li> </ul>"},{"location":"spec/#examples_71","title":"Examples","text":"<pre><code>// %initial_state: [1, 2]\n%output_state, %output = \"stablehlo.rng_bit_generator\"(%initial_state) {\n  rng_algorithm = #stablehlo&lt;rng_algorithm THREE_FRY&gt;\n} : (tensor&lt;2xui64&gt;) -&gt; (tensor&lt;2xui64&gt;, tensor&lt;2x2xui64&gt;)\n// %output_state: [1, 6]\n// %output: [\n//           [9236835810183407956, 16087790271692313299],\n//           [18212823393184779219, 2658481902456610144]\n//          ]\n</code></pre>"},{"location":"spec/#round_nearest_afz","title":"round_nearest_afz","text":""},{"location":"spec/#semantics_72","title":"Semantics","text":"<p>Performs element-wise rounding towards the nearest integer, breaking ties away from zero, on the <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTiesToAway</code> operation from the IEEE-754 specification.</p>"},{"location":"spec/#inputs_68","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type"},{"location":"spec/#outputs_72","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_67","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_72","title":"Examples","text":"<pre><code>// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5]\n%result = \"stablehlo.round_nearest_afz\"(%operand) : (tensor&lt;5xf32&gt;) -&gt; tensor&lt;5xf32&gt;\n// %result: [-3.0, 0.0, 1.0, 1.0, 3.0]\n</code></pre>"},{"location":"spec/#round_nearest_even","title":"round_nearest_even","text":""},{"location":"spec/#semantics_73","title":"Semantics","text":"<p>Performs element-wise rounding towards the nearest integer, breaking ties towards the even integer, on the <code>operand</code> tensor and produces a <code>result</code> tensor. Implements the <code>roundToIntegralTiesToEven</code> operation from the IEEE-754 specification.</p>"},{"location":"spec/#inputs_69","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point type"},{"location":"spec/#outputs_73","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point type"},{"location":"spec/#constraints_68","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_73","title":"Examples","text":"<pre><code>// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5]\n%result = \"stablehlo.round_nearest_even\"(%operand) : (tensor&lt;5xf32&gt;) -&gt; tensor&lt;5xf32&gt;\n// %result: [-2.0, 0.0, 0.0, 1.0, 2.0]\n</code></pre>"},{"location":"spec/#rsqrt","title":"rsqrt","text":""},{"location":"spec/#semantics_74","title":"Semantics","text":"<p>Performs element-wise reciprocal square root operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>rSqrt</code> from IEEE-754.</li> <li>For complex numbers: complex reciprocal square root.</li> </ul>"},{"location":"spec/#inputs_70","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_74","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_69","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_74","title":"Examples","text":"<pre><code>// %operand: [[1.0, 4.0], [9.0, 25.0]]\n%result = \"stablehlo.rsqrt\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[1.0, 0.5], [0.33333343, 0.2]]\n\n// %operand: [(1.0, 2.0)]\n%result = \"stablehlo.rsqrt\"(%operand) : (tensor&lt;complex&lt;f32&gt;&gt;) -&gt; tensor&lt;complex&lt;f32&gt;&gt;\n// %result: [(0.56886448, -0.35157758)]\n</code></pre>"},{"location":"spec/#scatter","title":"scatter","text":""},{"location":"spec/#semantics_75","title":"Semantics","text":"<p>Produces <code>results</code> tensors which are equal to <code>inputs</code> tensors except that several slices specified by <code>scatter_indices</code> are updated with the values <code>updates</code> using <code>update_computation</code>.</p> <p>The following diagram shows how elements in <code>updates[k]</code> map on elements in <code>results[k]</code> using a concrete example. The diagram picks a few example <code>updates[k]</code> indices and explains in detail which <code>results[k]</code> indices they correspond to.</p>  <p>More formally, for all <code>update_index</code> from the index space of <code>updates[0]</code>:</p> <ul> <li><code>update_scatter_dims</code> = [<code>d</code> for <code>d</code> in <code>axes(updates[0])</code> and <code>d</code> not in     <code>update_window_dims</code>].</li> <li><code>update_scatter_index</code> = [<code>update_index[d]</code> for <code>d</code> in     <code>update_scatter_dims</code>].</li> <li><code>start_index</code> =<ul> <li><code>scatter_indices[si0, ..., :, ..., siN]</code> where <code>si</code> are individual     elements in <code>update_scatter_index</code> and <code>:</code> is inserted at the     <code>index_vector_dim</code> index, if <code>index_vector_dim</code> &lt;     <code>rank(scatter_indices)</code>.</li> <li><code>[scatter_indices[update_scatter_index]]</code> otherwise.</li> </ul> </li> <li>For <code>do</code> in <code>axes(inputs[0])</code>,<ul> <li><code>full_start_index[do]</code> = <code>start_index[ds]</code> if     <code>do = scatter_dims_to_operand_dims[ds]</code>.</li> <li><code>full_start_index[do]</code> = <code>0</code> otherwise.</li> </ul> </li> <li><code>update_window_index</code> = [<code>update_index[d]</code> for <code>d</code> in <code>update_window_dims</code>].</li> <li><code>full_window_index</code> = <code>[oi0, ..., 0, ..., oiN]</code> where <code>oi</code> are individual     elements in <code>update_window_index</code>, and <code>0</code> is inserted at indices from     <code>inserted_window_dims</code>.</li> <li><code>result_index</code> = <code>add(full_start_index, full_window_index)</code>.</li> </ul> <p>Given that, <code>results = exec(schedule, inputs)</code>, where:</p> <ul> <li><code>schedule</code> is an implementation-defined permutation of the index space     of <code>updates[0]</code>.</li> <li><code>exec([update_index, ...], results) = exec([...], updated_results)</code> where:<ul> <li><code>updated_values =   update_computation(results[:][result_index], updates[:][update_index])</code>.</li> <li><code>updated_results</code> is a copy of <code>results</code> with <code>results[:][result_index]</code>   set to <code>updated_values[:]</code>.</li> <li>If <code>result_index</code> is out of bounds for <code>shape(results[:])</code>, the behavior   is implementation-defined.</li> </ul> </li> <li><code>exec([], results) = results</code>.</li> </ul> <p>If <code>indices_are_sorted</code> is <code>true</code> then the implementation can assume that <code>scatter_indices</code> are sorted with respect to <code>scatter_dims_to_operand_dims</code>, otherwise the behavior is undefined. More formally, for all <code>id &lt; jd</code> from <code>indices(result)</code>, <code>full_start_index(id)</code> &lt;= <code>full_start_index(jd)</code>.</p> <p>If <code>unique_indices</code> is <code>true</code> then the implementation can assume that all <code>result_index</code> indices being scattered to are unique. If <code>unique_indices</code> is <code>true</code> but the indices being scattered to are not unique then the behavior is undefined.</p>"},{"location":"spec/#inputs_71","title":"Inputs","text":"Name Type Constraints     <code>inputs</code> variadic number of tensors (C1), (C2), (C4), (C5), (C6), (C10), (C13), (C15), (C16)   <code>scatter_indices</code> tensor of integer type (C4), (C11), (C14)   <code>updates</code> variadic number of tensors (C3), (C4), (C5), (C6), (C8)   <code>update_window_dims</code> 1-dimensional tensor constant of type <code>si64</code> (C2), (C4), (C7), (C8)   <code>inserted_window_dims</code> 1-dimensional tensor constant of type <code>si64</code> (C2), (C4), (C9), (C10)   <code>scatter_dims_to_operand_dims</code> 1-dimensional tensor constant of type <code>si64</code> (C11),(C12), (C13)   <code>index_vector_dim</code> constant of type <code>si64</code> (C4), (C11), (C14)   <code>indices_are_sorted</code> constant of type <code>i1</code>    <code>unique_indices</code> constant of type <code>i1</code>    <code>update_computation</code> function (C15)"},{"location":"spec/#outputs_75","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors"},{"location":"spec/#constraints_70","title":"Constraints","text":"<ul> <li>(C1) All <code>inputs</code> have the same shape.</li> <li>(C2) rank(<code>inputs</code>[0]) = size(<code>update_window_dims</code>) +          size(<code>inserted_window_dims</code>).</li> <li>(C3) All <code>updates</code> have the same shape.</li> <li>(C4) <code>shape(updates[0])</code> \\(=\\) <code>combine(update_scatter_dim_sizes, update_window_dim_sizes)</code> where:<ul> <li><code>update_scatter_dim_sizes</code> = <code>shape(scatter_indices)</code> except that   the dimension size of <code>scatter_indices</code> corresponding to   <code>index_vector_dim</code> is not included.</li> <li><code>update_window_dim_sizes</code> \\(\\le\\) <code>shape(inputs[0])</code> except that   the dimension sizes in <code>inputs[0]</code> corresponding to <code>inserted_window_dims</code>   are not included.</li> <li><code>combine</code> puts <code>update_scatter_dim_sizes</code> at axes corresponding to  <code>update_scatter_dims</code> and <code>update_window_dim_sizes</code> at axes corresponding  to <code>update_window_dims</code>.</li> </ul> </li> <li>(C5) N \\(=\\) size(<code>inputs</code>) = size(<code>updates</code>) and N \\(\\ge\\) 1.</li> <li>(C6) <code>element_type(updates[k]) = element_type(inputs[k])</code> for any k \\(\\in\\)          [0, N).</li> <li>(C7) All dimensions in <code>update_window_dims</code> are unique and sorted.</li> <li>(C8) For all i \\(\\in\\) [0, size(<code>update_window_dims</code>)), \\(0 \\le\\) <code>update_window_dims</code>[i] \\(\\lt\\) rank(<code>updates</code>[0]).</li> <li>(C9) All dimensions in <code>inserted_window_dims</code> are unique and sorted.</li> <li>(C10) For all i \\(\\in\\) [0, size(<code>inserted_window_dims</code>)), \\(0 \\le\\) <code>inserted_window_dims</code>[i] \\(\\lt\\) rank(<code>inputs</code>[0]).</li> <li>(C11) size(<code>scatter_dims_to_operand_dims</code>) \\(=\\) <code>index_vector_dim</code> \\(\\lt\\) rank(<code>scatter_indices</code>) ?          dim(<code>scatter_indices</code>, <code>index_vector_dim</code>) : 1.</li> <li>(C12) All dimensions in <code>scatter_dims_to_operand_dims</code> are unique.</li> <li>(C13) For all i \\(\\in\\) [0, size(<code>scatter_dims_to_operand_dims</code>)), \\(0 \\le\\) <code>scatter_dims_to_operand_dims</code>[i] \\(\\lt\\) rank(<code>inputs</code>[0]).</li> <li>(C14) \\(0 \\le\\) <code>index_vector_dim</code> \\(\\le\\) rank(<code>scatter_indices</code>).</li> <li>(C15) <code>update_computation</code> has type <code>(tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;) -&gt; (tensor&lt;E0&gt;, ..., tensor&lt;EN-1&gt;)</code>           where <code>Ek = element_type(inputs[k])</code> for any k \\(\\in\\) [0, N).</li> <li>(C16) <code>inputs[k]</code> and <code>result[k]</code> have the same type for any k \\(\\in\\) [0, N).</li> </ul>"},{"location":"spec/#examples_75","title":"Examples","text":"<pre><code>// %input: [\n//          [[1, 2], [3, 4], [5, 6], [7, 8]],\n//          [[9, 10], [11, 12], [13, 14], [15, 16]],\n//          [[17, 18], [19, 20], [21, 22], [23, 24]]\n//         ]\n// %scatter_indices: [[[0, 2], [1, 0], [2, 1]], [[0, 1], [1, 0], [2, 0]]]\n// %update: [\n//           [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 1], [1, 1]]],\n//           [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 1], [1, 1]]]\n//          ]\n%result = \"stablehlo.scatter\"(%input, %scatter_indices, %update) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i32&gt;) -&gt; ()\n}) {\n  scatter_dimension_numbers = #stablehlo.scatter&lt;\n    update_window_dims = [2,3],\n    inserted_window_dims = [0],\n    scatter_dims_to_operand_dims = [1, 0],\n    index_vector_dim = 2&gt;,\n  indices_are_sorted = false,\n  unique_indices = false\n} : (tensor&lt;3x4x2xi32&gt;, tensor&lt;2x3x2xi64&gt;, tensor&lt;2x3x2x2xi32&gt;) -&gt; tensor&lt;3x4x2xi32&gt;\n// %result: [\n//           [[1, 2], [5, 6], [8, 9], [8, 9]],\n//           [[10, 11], [12, 13], [14, 15], [16, 17]],\n//           [[18, 19], [20, 21], [21, 22], [23, 24]]\n//          ]\n</code></pre>"},{"location":"spec/#select","title":"select","text":""},{"location":"spec/#semantics_76","title":"Semantics","text":"<p>Produces a <code>result</code> tensor where each element is selected from <code>on_true</code> or <code>on_false</code> tensor based on the value of the corresponding element of <code>pred</code>. More formally, <code>result[i0, ..., iR-1] = pred_val ? on_true[i0, ..., iR-1] : on_false[i0, ..., iR-1]</code>, where <code>pred_val = rank(pred) == 0 ? pred : pred[i0, ..., iR-1]</code>.</p>"},{"location":"spec/#inputs_72","title":"Inputs","text":"Name Type     <code>pred</code> tensor of type <code>i1</code>   <code>on_true</code> tensor   <code>on_false</code> tensor"},{"location":"spec/#outputs_76","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_71","title":"Constraints","text":"<ul> <li>(C1) Either <code>rank(pred)</code> \\(=\\) <code>0</code> or <code>shape(pred)</code> \\(=\\) <code>shape(on_true)</code>.</li> <li>(C2) <code>on_true</code>, <code>on_false</code> and <code>result</code> have same type.</li> </ul>"},{"location":"spec/#examples_76","title":"Examples","text":"<pre><code>// %pred: [[false, true], [true, false]]\n// %on_true: [[1, 2], [3, 4]]\n// %on_false: [[5, 6], [7, 8]]\n%result = \"stablehlo.select\"(%pred, %on_true, %on_false) : (tensor&lt;2x2xi1&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[5, 2], [3, 8]]\n</code></pre>"},{"location":"spec/#select_and_scatter","title":"select_and_scatter","text":""},{"location":"spec/#semantics_77","title":"Semantics","text":"<p>Scatters the values from the <code>source</code> tensor using <code>scatter</code> based on the outcome of <code>reduce_window</code> of the <code>input</code> tensor using <code>select</code> and produces a <code>result</code> tensor.</p> <p>The following diagram shows how elements in <code>result</code> are computed from <code>operand</code> and <code>source</code> using a concrete example.</p>  <p>More formally:</p> <ul> <li> <p><code>selected_values = reduce_window_without_init(...)</code> with the following inputs:</p> <ul> <li><code>inputs</code> \\(=\\) [ <code>operand</code> ].</li> <li><code>window_dimensions</code>, <code>window_strides</code>, and <code>padding</code> which are used as is.</li> <li><code>base_dilations</code> \\(=\\) <code>windows_dilations</code> \\(=\\) <code>[1, ..., 1]</code>.</li> <li><code>body</code> defined as:</li> </ul> <pre><code>(tensor&lt;E&gt; arg0, tensor&lt;E&gt; arg1) -&gt; tensor&lt;E&gt; {\n return select(arg0, arg1) ? arg0 : arg1;\n}\n</code></pre> <p>where <code>E = element_type(operand)</code>.    where <code>reduce_window_without_init</code> works exactly like <code>reduce_window</code>,    except that the <code>schedule</code> of the underlying <code>reduce</code> doesn't include    init values.   * <code>result[result_index] = reduce([source_values], [init_value], [0], scatter)</code>    where: * <code>source_values</code> \\(=\\) [<code>source[source_index]</code> for <code>source_index</code> in  <code>source_indices</code>]. * <code>source_indices</code> \\(=\\) [<code>source_index</code> for <code>source_index</code> in  <code>indices(source)</code> if <code>selected_index(source_index) = result_index</code>]. * <code>selected_index(source_index) = operand_index</code> if  <code>selected_values[source_index]</code> has the <code>operand</code> element  from <code>operand_index</code>.</p> </li> </ul>"},{"location":"spec/#inputs_73","title":"Inputs","text":"Name Type Constraints     <code>operand</code> tensor (C1-C5), (C7), (C9), (C10-C12)   <code>source</code> tensor (C2), (C3)   <code>init_value</code> 0-dimensional tensor (C4)   <code>window_dimensions</code> 1-dimensional tensor constant of type <code>si64</code> (C1), (C3), (C5), (C6)   <code>window_strides</code> 1-dimensional tensor constant of type <code>si64</code> (C3), (C7), (C8)   <code>padding</code> 2-dimensional tensor constant of type <code>si64</code> (C3), (C9)   <code>select</code> function (C10)   <code>scatter</code> function (C11)"},{"location":"spec/#outputs_77","title":"Outputs","text":"Name Type Constraints     <code>result</code> tensor (C12)"},{"location":"spec/#constraints_72","title":"Constraints","text":"<ul> <li>(C1) rank(<code>operand</code>) \\(=\\) size(<code>window_dimensions</code>).</li> <li>(C2) <code>operand</code> and <code>source</code> have the same element type.</li> <li>(C3) <code>shape(source) = (padded_operand_shape == 0 || window_dimensions &gt; padded_operand_shape) ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1:</code><ul> <li><code>padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1]</code>.</li> </ul> </li> <li>(C4) element_type(<code>init_value</code>) \\(=\\) element_type(<code>operand</code>).</li> <li>(C5) size(<code>window_dimensions</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C6) <code>window_dimensions[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(window_dimensions)).</li> <li>(C7) size(<code>window_strides</code>) \\(=\\) rank(<code>operand</code>).</li> <li>(C8) <code>window_strides[i]</code> \\(\\gt 0\\) for all i \\(\\in\\) [0, size(window_strides)).</li> <li>(C9) dim(<code>padding</code>, 0) \\(=\\) rank(<code>operand</code>) and dim(<code>padding</code>, 1) = 2.</li> <li>(C10) <code>select</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; tensor&lt;i1&gt;</code> where           <code>E = element_type(operand)</code>.</li> <li>(C11) <code>scatter</code> has type <code>(tensor&lt;E&gt;, tensor&lt;E&gt;) -&gt; tensor&lt;E&gt;</code> where           <code>E = element_type(operand)</code>.</li> <li>(C12) type(<code>operand</code>) \\(=\\) type(<code>result</code>).</li> </ul>"},{"location":"spec/#examples_77","title":"Examples","text":"<pre><code>// %operand: [[1, 5], [2, 5], [3, 6], [4, 4]]\n// %source: [[5, 6], [7, 8]]\n// %init_value: 0\n%result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.compare\"(%arg0, %arg1) {\n      comparison_direction = #stablehlo&lt;comparison_direction GE&gt;\n    } : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i1&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i1&gt;) -&gt; ()\n}, {\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i32&gt;) -&gt; ()\n}) {\n  window_dimensions = dense&lt;[3, 1]&gt; : tensor&lt;2xi64&gt;,\n  window_strides = dense&lt;[2, 1]&gt; : tensor&lt;2xi64&gt;,\n  padding = dense&lt;[[0, 1], [0, 0]]&gt; : tensor&lt;2x2xi64&gt;\n} : (tensor&lt;4x2xi32&gt;, tensor&lt;2x2xi32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;4x2xi32&gt;\n// %result: [[0, 0], [0, 0], [5, 14], [7, 0]]\n</code></pre>"},{"location":"spec/#send","title":"send","text":""},{"location":"spec/#semantics_78","title":"Semantics","text":"<p>Sends <code>inputs</code> to a channel <code>channel_id</code> and produces a <code>result</code> token.</p> <p>The operation takes a token and produces a token to reify its side effects as a value that other operations can take a data dependency on.</p> <p>If <code>is_host_transfer</code> is <code>true</code>, then the operation transfers data to the host. Otherwise, it transfers data to another device. What this means is implementation-defined.</p>"},{"location":"spec/#inputs_74","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of tensors   <code>token</code> <code>token</code>   <code>channel_id</code> constant of type <code>si64</code>   <code>channel_type</code> enum of <code>DEVICE_TO_DEVICE</code> and <code>DEVICE_TO_HOST</code>   <code>is_host_transfer</code> constant of type <code>i1</code>"},{"location":"spec/#outputs_78","title":"Outputs","text":"Name Type     <code>result</code> <code>token</code>"},{"location":"spec/#constraints_73","title":"Constraints","text":"<ul> <li>(C1) todo <code>channel_type</code>     must be<ul> <li><code>DEVICE_TO_HOST</code>, if <code>is_host_transfer</code> \\(=\\) <code>true</code>,</li> <li><code>DEVICE_TO_DEVICE</code>, otherwise.</li> </ul> </li> </ul>"},{"location":"spec/#examples_78","title":"Examples","text":"<pre><code>%result = \"stablehlo.send\"(%operand, %token) {\n  // channel_id = 5 : i64,\n  // channel_type = #stablehlo&lt;channel_type DEVICE_TO_HOST&gt;,\n  channel_handle = #stablehlo.channel_handle&lt;handle = 5, type = 2&gt;,\n  is_host_transfer = true\n} : (tensor&lt;3x4xi32&gt;, !stablehlo.token) -&gt; !stablehlo.token\n</code></pre>"},{"location":"spec/#shift_left","title":"shift_left","text":""},{"location":"spec/#semantics_79","title":"Semantics","text":"<p>Performs element-wise left-shift operation on the <code>lhs</code> tensor by <code>rhs</code> number of bits and produces a <code>result</code> tensor.</p>"},{"location":"spec/#inputs_75","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer type   <code>rhs</code> tensor of integer type"},{"location":"spec/#outputs_79","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer type"},{"location":"spec/#constraints_74","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code>, and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_79","title":"Examples","text":"<pre><code>// %lhs: [-1, -2, 3, 4, 7, 7]\n// %rhs: [1, 2, 3, 6, 7, 8]\n%result = \"stablehlo.shift_left\"(%lhs, %rhs): (tensor&lt;6xi8&gt;, tensor&lt;6xi8&gt;) -&gt; tensor&lt;6xi8&gt;\n// %result: [-2, -8, 24, 0, -128, 0]\n</code></pre>"},{"location":"spec/#shift_right_arithmetic","title":"shift_right_arithmetic","text":""},{"location":"spec/#semantics_80","title":"Semantics","text":"<p>Performs element-wise arithmetic right-shift operation on the <code>lhs</code> tensor by <code>rhs</code> number of bits and produces a <code>result</code> tensor.</p>"},{"location":"spec/#inputs_76","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer type   <code>rhs</code> tensor of integer type"},{"location":"spec/#outputs_80","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer type"},{"location":"spec/#constraints_75","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code>, and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_80","title":"Examples","text":"<pre><code>// %lhs: [-1, -128, -36, 5, 3, 7]\n// %rhs: [1, 2, 3, 2, 1, 3]\n%result = \"stablehlo.shift_right_arithmetic\"(%lhs, %rhs): (tensor&lt;6xi8&gt;, tensor&lt;6xi8&gt;) -&gt; tensor&lt;6xi8&gt;\n// %result: [-1, -32, -5, 1, 1, 0]\n</code></pre>"},{"location":"spec/#shift_right_logical","title":"shift_right_logical","text":""},{"location":"spec/#semantics_81","title":"Semantics","text":"<p>Performs element-wise logical right-shift operation on the <code>lhs</code> tensor by <code>rhs</code> number of bits and produces a <code>result</code> tensor.</p>"},{"location":"spec/#inputs_77","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer type   <code>rhs</code> tensor of integer type"},{"location":"spec/#outputs_81","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer type"},{"location":"spec/#constraints_76","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code>, and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_81","title":"Examples","text":"<pre><code>// %lhs: [-1, -128, -36, 5, 3, 7]\n// %rhs: [1, 2, 3, 2, 1, 3]\n%result = \"stablehlo.shift_right_logical\"(%lhs, %rhs): (tensor&lt;6xi8&gt;, tensor&lt;6xi8&gt;) -&gt; tensor&lt;6xi8&gt;\n// %result: [127, 32, 27, 1, 1, 0]\n</code></pre>"},{"location":"spec/#sign","title":"sign","text":""},{"location":"spec/#semantics_82","title":"Semantics","text":"<p>Returns the sign of the <code>operand</code> element-wise and produces a <code>result</code> tensor. More formally, for each element <code>x</code>, the semantics can be expressed using Python-like syntax as follows:</p> <pre><code>def sign(x):\n  if is_integer(x):\n    if compare(x, 0, LT, SIGNED): return -1\n    if compare(x, 0, EQ, SIGNED): return 0\n    if compare(x, 0, GT, SIGNED): return 1\n  elif is_float(x):\n    if x is NaN:\n      return NaN\n    else:\n      if compare(x, 0.0, LT, FLOAT): return -1.0\n      if compare(x, -0.0, EQ, FLOAT): return -0.0\n      if compare(x, +0.0, EQ, FLOAT): return +0.0\n      if compare(x, 0.0, GT, FLOAT): return 1.0\n  elif is_complex(x):\n    if x.real is NaN or x.imag is NaN:\n      return NaN\n    else:\n      return divide(x, abs(x))\n</code></pre>"},{"location":"spec/#inputs_78","title":"Inputs","text":"Name Type     <code>operand</code> tensor of signed integer, floating-point, or complex type"},{"location":"spec/#outputs_82","title":"Outputs","text":"Name Type     <code>result</code> tensor of signed integer, floating-point, or complex type"},{"location":"spec/#constraints_77","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_82","title":"Examples","text":"<pre><code>// Logical values: -Inf, +Inf, NaN, ...\n// %operand: [0xFF800000, 0x7F800000, 0x7FFFFFFF, -10.0, -0.0, 0.0, 10.0]\n%result = \"stablehlo.sign\"(%operand) : (tensor&lt;7xf32&gt;) -&gt; tensor&lt;7xf32&gt;\n// %result: [-1.0, 1.0, 0x7FFFFFFF, -1.0, -0.0, 0.0, 1.0]\n</code></pre>"},{"location":"spec/#sine","title":"sine","text":""},{"location":"spec/#semantics_83","title":"Semantics","text":"<p>Performs element-wise sine operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>sin</code> from IEEE-754.</li> <li>For complex numbers: complex sine.</li> </ul>"},{"location":"spec/#inputs_79","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_83","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_78","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_83","title":"Examples","text":"<pre><code>// %operand: [\n//            [0.0, 1.57079632],       // [0, pi/2]\n//            [3.14159265, 4.71238898] // [pi, 3pi/2]\n//           ]\n%result = \"stablehlo.sine\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[0.0, 1.0], [0.0, -1.0]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#slice","title":"slice","text":""},{"location":"spec/#semantics_84","title":"Semantics","text":"<p>Extracts a slice from the <code>operand</code> using statically-computed starting indices and produces a <code>result</code> tensor. <code>start_indices</code> contain the starting indices of the slice for each dimension, <code>limit_indices</code> contain the ending indices (exclusive) for the slice for each dimension, and <code>strides</code> contain the strides for each dimension.</p> <p>More formally, <code>result[i0, ..., iR-1] = operand[j0, ..., jR-1]</code> where <code>jd = start_indices[d] + id * strides[d]</code>.</p>"},{"location":"spec/#inputs_80","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>start_indices</code> 1-dimensional tensor constant of type <code>si64</code>   <code>limit_indices</code> 1-dimensional tensor constant of type <code>si64</code>   <code>strides</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_84","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_79","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same element type.</li> <li>(C2) size(<code>start_indices</code>) = size(<code>limit_indices</code>) = size(<code>strides</code>) =   rank(<code>operand</code>).</li> <li>(C3) 0 \\(\\le\\) <code>start_indices[d]</code> \\(\\le\\) <code>limit_indices[d]</code> \\(\\le\\) <code>dim(operand, d)</code> for all dimension <code>d</code>.</li> <li>(C4) 0 \\(\\lt\\) <code>strides[d]</code> for all dimension <code>d</code>.</li> <li>(C5) <code>dim(result, d)</code> =   \\(\\lceil\\)<code>(limit_indices[d]-start_indices[d])/stride[d]</code>\\(\\rceil\\) for all   dimension <code>d</code> in <code>operand</code>.</li> </ul>"},{"location":"spec/#examples_84","title":"Examples","text":"<pre><code>// 1-dimensional slice\n\n// %operand: [0, 1, 2, 3, 4]\n%result = \"stablehlo.slice\"(%operand) {\n  start_indices = dense&lt;2&gt; : tensor&lt;1xi64&gt;,\n  limit_indices = dense&lt;4&gt; : tensor&lt;1xi64&gt;,\n  strides = dense&lt;1&gt; : tensor&lt;1xi64&gt;\n} : (tensor&lt;5xi64&gt;) -&gt; tensor&lt;2xi64&gt;\n// %result: [2, 3]\n\n// 2-dimensional slice\n\n// %operand: [\n//            [0, 0, 0, 0],\n//            [0, 0, 1, 1],\n//            [0, 0, 1, 1]\n//           ]\n%result = \"stablehlo.slice\"(%operand) {\n  start_indices = dense&lt;[1, 2]&gt; : tensor&lt;2xi64&gt;,\n  limit_indices = dense&lt;[3, 4]&gt; : tensor&lt;2xi64&gt;,\n  strides = dense&lt;1&gt; : tensor&lt;2xi64&gt;\n} : (tensor&lt;3x4xi64&gt;) -&gt; tensor&lt;2x2xi64&gt;\n// % result: [\n//            [1, 1],\n//            [1, 1]\n//           ]\n</code></pre>"},{"location":"spec/#sort","title":"sort","text":""},{"location":"spec/#semantics_85","title":"Semantics","text":"<p>Sorts a variadic number of tensors in <code>inputs</code> together, according to a custom <code>comparator</code>, along the given <code>dimension</code> and produces a variadic number of tensors as <code>results</code>. If <code>is_stable</code> is true, then the sorting is stable, that is, relative order of elements considered to be equal by the comparator is preserved. Two elements <code>e1</code> and <code>e2</code> are considered to be equal by the comparator if and only if <code>comparator(e1, e2) = comparator(e2, e1) = false</code>.</p> <p>More formally, for all <code>0 &lt;= id &lt; jd &lt; dim(inputs[0], d)</code>, either <code>compare_i_j = compare_j_i = false</code> or <code>compare_i_j = true</code>, where:</p> <ol> <li><code>compare_i_j</code> \\(=\\) <code>comparator(inputs[0][i], inputs[0][j], inputs[1][i], inputs[1][j], ...)</code>.</li> <li>For all indices <code>i = [i0, ..., iR-1]</code> and <code>j = [j0, ..., jR-1]</code>.</li> <li>Where <code>i</code> \\(=\\) <code>j</code> everywhere except for the <code>d</code>th dimension.</li> <li>Where <code>d</code> \\(=\\) <code>dimension &gt;= 0 ? dimension : rank(inputs[0]) + dimension</code>.</li> </ol>"},{"location":"spec/#inputs_81","title":"Inputs","text":"Name Type     <code>inputs</code> variadic number of tensors   <code>dimension</code> constant of type <code>si64</code>   <code>is_stable</code> constant of type <code>i1</code>   <code>comparator</code> function"},{"location":"spec/#outputs_85","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors"},{"location":"spec/#constraints_80","title":"Constraints","text":"<ul> <li>(C1) <code>inputs</code> have at least 1 tensor.</li> <li>(C2) For all <code>i</code>, <code>type(inputs[i])</code> = <code>type(results[i])</code>.</li> <li>(C3) All tensors in <code>inputs</code> and <code>results</code> have the same shape.</li> <li>(C4) <code>-R</code> \\(\\le\\) <code>dimension</code> \\(\\lt\\) <code>R</code>, where <code>R</code> is rank of <code>inputs[0]</code>.</li> <li>(C5) <code>comparator</code> has type     <code>(tensor&lt;E1&gt;, tensor&lt;E1&gt;, ..., tensor&lt;EN-1&gt;, tensor&lt;EN-1&gt;) -&gt; tensor&lt;i1&gt;</code>,     where <code>Ei</code> is element type of <code>inputs[i]</code>.</li> </ul>"},{"location":"spec/#examples_85","title":"Examples","text":"<pre><code>// Sort along dimension 0\n\n// %input0 = [[1, 2, 3], [3, 2, 1]]\n// %input1 = [[3, 2, 1], [1, 2, 3]]\n%result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;, %arg2: tensor&lt;i32&gt;, %arg3: tensor&lt;i32&gt;):\n    %predicate = \"stablehlo.compare\"(%arg0, %arg1) {\n      comparison_direction = #stablehlo&lt;comparison_direction GT&gt;\n    } : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i1&gt;\n    \"stablehlo.return\"(%predicate) : (tensor&lt;i1&gt;) -&gt; ()\n}) {\n  dimension = 0 : i64,\n  is_stable = true\n} : (tensor&lt;2x3xi32&gt;, tensor&lt;2x3xi32&gt;) -&gt; (tensor&lt;2x3xi32&gt;, tensor&lt;2x3xi32&gt;)\n// %result0 = [[3, 2, 3], [1, 2, 1]]\n// %result1 = [[1, 2, 1], [3, 2, 3]]\n\n\n// Sort along dimension 1\n\n// %input0 = [[1, 2, 3], [3, 2, 1]]\n// %input1 = [[3, 2, 1], [1, 2, 3]]\n%result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;, %arg2: tensor&lt;i32&gt;, %arg3: tensor&lt;i32&gt;):\n    %predicate = \"stablehlo.compare\"(%arg0, %arg1) {\n      comparison_direction = #stablehlo&lt;comparison_direction GT&gt;\n    } : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i1&gt;\n    \"stablehlo.return\"(%predicate) : (tensor&lt;i1&gt;) -&gt; ()\n}) {\n  dimension = 1 : i64,\n  is_stable = true\n} : (tensor&lt;2x3xi32&gt;, tensor&lt;2x3xi32&gt;) -&gt; (tensor&lt;2x3xi32&gt;, tensor&lt;2x3xi32&gt;)\n// %result0 = [[3, 2, 1], [3, 2, 1]]\n// %result1 = [[1, 2, 3], [1, 2, 3]]\n</code></pre>"},{"location":"spec/#sqrt","title":"sqrt","text":""},{"location":"spec/#semantics_86","title":"Semantics","text":"<p>Performs element-wise square root operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>squareRoot</code> from IEEE-754.</li> <li>For complex numbers: complex square root.</li> </ul>"},{"location":"spec/#inputs_82","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_86","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_81","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_86","title":"Examples","text":"<pre><code>// %operand: [[0.0, 1.0], [4.0, 9.0]]\n%result = \"stablehlo.sqrt\"(%operand) : (tensor&lt;2x2xf32&gt;) -&gt; tensor&lt;2x2xf32&gt;\n// %result: [[0.0, 1.0], [2.0, 3.0]]\n\n// %operand: [(1.0, 2.0)]\n%result = \"stablehlo.sqrt\"(%operand) : (tensor&lt;complex&lt;f32&gt;&gt;) -&gt; tensor&lt;complex&lt;f32&gt;&gt;\n// %result: [(1.27201965, 0.78615138)]\n</code></pre>"},{"location":"spec/#subtract","title":"subtract","text":""},{"location":"spec/#semantics_87","title":"Semantics","text":"<p>Performs element-wise subtraction of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For integers: integer subtraction.</li> <li>For floats: <code>subtraction</code> from IEEE-754.</li> <li>For complex numbers: complex subtraction.</li> </ul>"},{"location":"spec/#inputs_83","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of integer, floating-point, or complex type   <code>rhs</code> tensor of integer, floating-point, or complex type"},{"location":"spec/#outputs_87","title":"Outputs","text":"Name Type     <code>result</code> tensor of integer, floating-point, or complex type"},{"location":"spec/#constraints_82","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_87","title":"Examples","text":"<pre><code>// %lhs: [[6, 8], [10, 12]]\n// %rhs: [[5, 6], [7, 8]]\n%result = \"stablehlo.subtract\"(%lhs, %rhs) : (tensor&lt;2x2xf32&gt;, tensor&lt;2x2xf32&gt;) -&gt; (tensor&lt;2x2xf32&gt;)\n// %result: [[1, 2], [3, 4]]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#tanh","title":"tanh","text":""},{"location":"spec/#semantics_88","title":"Semantics","text":"<p>Performs element-wise hyperbolic tangent operation on <code>operand</code> tensor and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For floats: <code>tanh</code> from IEEE-754.</li> <li>For complex numbers: complex hyperbolic tangent.</li> </ul>"},{"location":"spec/#inputs_84","title":"Inputs","text":"Name Type     <code>operand</code> tensor of floating-point or complex type"},{"location":"spec/#outputs_88","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_83","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_88","title":"Examples","text":"<pre><code>// %operand: [-1.0, 0.0, 1.0]\n%result = \"stablehlo.tanh\"(%operand) : (tensor&lt;3xf32&gt;) -&gt; tensor&lt;3xf32&gt;\n// %result: [-0.76159416, 0.0, 0.76159416]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#transpose","title":"transpose","text":""},{"location":"spec/#semantics_89","title":"Semantics","text":"<p>Permutes the dimensions of <code>operand</code> tensor using <code>permutation</code> and produces a <code>result</code> tensor. More formally, <code>result[i0, ..., iR-1] = operand[j0, ..., jR-1]</code> where <code>i[d] = j[permutation[d]]</code>.</p>"},{"location":"spec/#inputs_85","title":"Inputs","text":"Name Type     <code>operand</code> tensor   <code>permutation</code> 1-dimensional tensor constant of type <code>si64</code>"},{"location":"spec/#outputs_89","title":"Outputs","text":"Name Type     <code>result</code> tensor"},{"location":"spec/#constraints_84","title":"Constraints","text":"<ul> <li>(C1) <code>operand</code> and <code>result</code> have the same element type.</li> <li>(C2) <code>permutation</code> is a permutation of <code>[0, 1, ..., R-1]</code> where <code>R</code> is the   rank of <code>operand</code>.</li> <li>(C3) For all dimensions <code>i</code> in <code>operand</code>, <code>dim(operand, i) = dim(result, j)</code>   where <code>j = permutation[i]</code>.</li> </ul>"},{"location":"spec/#examples_89","title":"Examples","text":"<pre><code>// %operand: [\n//            [[1,2], [3,4], [5,6]],\n//            [[7,8], [9,10], [11,12]]\n//           ]\n%result = \"stablehlo.transpose\"(%operand) {\n  permutation = dense&lt;[2, 1, 0]&gt; : tensor&lt;3xi64&gt;\n} : (tensor&lt;2x3x2xi32&gt;) -&gt; tensor&lt;2x3x2xi32&gt;\n// %result: [\n//           [[1,7], [3,9], [5,11]],\n//           [[2,8], [4,10], [6,12]]\n//          ]\n</code></pre> <p> More Examples</p>"},{"location":"spec/#triangular_solve","title":"triangular_solve","text":""},{"location":"spec/#semantics_90","title":"Semantics","text":"<p>Solves batches of systems of linear equations with lower or upper triangular coefficient matrices.</p> <p>More formally, given <code>a</code> and <code>b</code>, <code>result[i0, ..., iR-3, :, :]</code> is the solution to <code>op(a[i0, ..., iR-3, :, :]) * x = b[i0, ..., iR-3, :, :]</code> when <code>left_side</code> is <code>true</code> or <code>x * op(a[i0, ..., iR-3, :, :]) = b[i0, ..., iR-3, :, :]</code> when <code>left_side</code> is <code>false</code>, solving for the variable <code>x</code> where <code>op(a)</code> is determined by <code>transpose_a</code>, which can be one of the following:</p> <ul> <li><code>NO_TRANSPOSE</code>: Perform operation using <code>a</code> as-is.</li> <li><code>TRANSPOSE</code>: Perform operation on transpose of <code>a</code>.</li> <li><code>ADJOINT</code>: Perform operation on conjugate transpose of <code>a</code>.</li> </ul> <p>Input data is read only from the lower triangle of <code>a</code>, if <code>lower</code> is <code>true</code> or upper triangle of <code>a</code>, otherwise. Output data is returned in the same triangle; the values in the other triangle are implementation-defined.</p> <p>If <code>unit_diagonal</code> is true, then the implementation can assume that the diagonal elements of <code>a</code> are equal to 1, otherwise the behavior is undefined.</p>"},{"location":"spec/#inputs_86","title":"Inputs","text":"Name Type     <code>a</code> tensor of floating-point or complex type   <code>b</code> tensor of floating-point or complex type   <code>left_side</code> constant of type <code>i1</code>   <code>lower</code> constant of type <code>i1</code>   <code>unit_diagonal</code> constant of type <code>i1</code>   <code>transpose_a</code> enum of <code>NO_TRANSPOSE</code>, <code>TRANSPOSE</code>, and <code>ADJOINT</code>"},{"location":"spec/#outputs_90","title":"Outputs","text":"Name Type     <code>result</code> tensor of floating-point or complex type"},{"location":"spec/#constraints_85","title":"Constraints","text":"<ul> <li>(C1) <code>a</code> and <code>b</code> have the same element type</li> <li>(C2) rank(<code>a</code>) \\(=\\) rank(<code>b</code>) \\(\\ge\\) 2.</li> <li>(C3) The relationship between shape(<code>a</code>) and shape(<code>b</code>) is as follows:<ul> <li>For all <code>i</code> \\(\\in\\) [0, R-3], dim(<code>a</code>, <code>i</code>) \\(=\\) dim(<code>b</code>, <code>i</code>).</li> <li><code>dim(a, R-2)</code> \\(=\\) <code>dim(a, R-1)</code> \\(=\\) <code>dim(b, left_side ? R-2 : R-1)</code>.</li> </ul> </li> <li>(C4) <code>b</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_90","title":"Examples","text":"<pre><code>// %a = [\n//       [1.0, 0.0, 0.0],\n//       [2.0, 4.0, 0.0],\n//       [3.0, 5.0, 6.0]\n//      ]\n// %b = [\n//       [2.0, 0.0, 0.0],\n//       [4.0, 8.0, 0.0],\n//       [6.0, 10.0, 12.0]\n//      ]\n%result = \"stablehlo.triangular_solve\"(%a, %b) {\n  left_side = true,\n  lower = true,\n  unit_diagonal = false,\n  transpose_a = #stablehlo&lt;transpose NO_TRANSPOSE&gt;\n} : (tensor&lt;3x3xf32&gt;, tensor&lt;3x3xf32&gt;) -&gt; tensor&lt;3x3xf32&gt;\n// %result: [\n//           [2.0, 0.0, 0.0],\n//           [0.0, 2.0, 0.0],\n//           [0.0, 0.0, 2.0]\n//          ]\n</code></pre>"},{"location":"spec/#tuple","title":"tuple","text":""},{"location":"spec/#semantics_91","title":"Semantics","text":"<p>Produces a <code>result</code> tuple from values <code>val</code>.</p>"},{"location":"spec/#inputs_87","title":"Inputs","text":"Name Type     <code>val</code> variadic number of values"},{"location":"spec/#outputs_91","title":"Outputs","text":"Name Type     <code>result</code> tuple"},{"location":"spec/#constraints_86","title":"Constraints","text":"<ul> <li>(C1) size(<code>val</code>) \\(=\\) size(<code>result</code>) \\(=\\) N.</li> <li>(C2) <code>type(val[i])</code> \\(=\\) <code>type(result[i])</code>, for all <code>i</code> \\(\\in\\) range [0, N).</li> </ul>"},{"location":"spec/#examples_91","title":"Examples","text":"<pre><code>// %val0: [1.0, 2.0]\n// %val1: (3)\n%result = \"stablehlo.tuple\"(%val0, %val1) : (tensor&lt;2xf32&gt;, tuple&lt;tensor&lt;i32&gt;&gt;) -&gt; tuple&lt;tensor&lt;2xf32&gt;, tuple&lt;tensor&lt;i32&gt;&gt;&gt;\n// %result: ([1.0, 2.0], (3))\n</code></pre>"},{"location":"spec/#while","title":"while","text":""},{"location":"spec/#semantics_92","title":"Semantics","text":"<p>Produces the output from executing <code>body</code> function 0 or more times while the <code>cond</code> function outputs <code>true</code>. More formally, the semantics can be expressed using Python-like syntax as follows:</p> <pre><code>internal_state = operands\nwhile cond(internal_state) == True:\n  internal_state = body(internal_state)\nresults = internal_state\n</code></pre> <p>The behavior of an infinite loop is TBD.</p>"},{"location":"spec/#inputs_88","title":"Inputs","text":"Name Type     <code>operands</code> variadic number of tensors or tokens   <code>cond</code> function   <code>body</code> function"},{"location":"spec/#outputs_92","title":"Outputs","text":"Name Type     <code>results</code> variadic number of tensors or tokens"},{"location":"spec/#constraints_87","title":"Constraints","text":"<ul> <li>(C1) <code>cond</code> has type <code>(T0, ..., TN-1) -&gt; tensor&lt;i1&gt;</code>, where          <code>Ti</code> = <code>type(operands[i])</code>.</li> <li>(C2) <code>body</code> has type <code>(T0, ..., TN-1) -&gt; (T0, ..., TN-1)</code>, where          <code>Ti</code> = <code>type(operands[i])</code>.</li> <li>(C3) For all <code>i</code>, <code>type(results[i])</code> = <code>type(operands[i])</code>.</li> </ul>"},{"location":"spec/#examples_92","title":"Examples","text":"<pre><code>// %constant0: 1\n// %input0: 0\n// %input1: 10\n%results0, %results1 = \"stablehlo.while\"(%input0, %input1) ({\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.compare\"(%arg0, %arg1) {\n      comparison_direction = #stablehlo&lt;comparison_direction LT&gt;\n    } : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i1&gt;\n    \"stablehlo.return\"(%0) : (tensor&lt;i1&gt;) -&gt; ()\n}, {\n  ^bb0(%arg0: tensor&lt;i32&gt;, %arg1: tensor&lt;i32&gt;):\n    %0 = \"stablehlo.add\"(%arg0, %constant0) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;i32&gt;\n    \"stablehlo.return\"(%0, %arg1) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; ()\n}) : (tensor&lt;i32&gt;, tensor&lt;i32&gt;) -&gt; (tensor&lt;i32&gt;, tensor&lt;i32&gt;)\n// %results0: 10\n// %results1: 10\n</code></pre>"},{"location":"spec/#xor","title":"xor","text":""},{"location":"spec/#semantics_93","title":"Semantics","text":"<p>Performs element-wise XOR of two tensors <code>lhs</code> and <code>rhs</code> and produces a <code>result</code> tensor. Depending on the element type, does the following:</p> <ul> <li>For booleans: logical XOR.</li> <li>For integers: bitwise XOR.</li> </ul>"},{"location":"spec/#inputs_89","title":"Inputs","text":"Name Type     <code>lhs</code> tensor of boolean or integer type   <code>rhs</code> tensor of boolean or integer type"},{"location":"spec/#outputs_93","title":"Outputs","text":"Name Type     <code>result</code> tensor of boolean or integer type"},{"location":"spec/#constraints_88","title":"Constraints","text":"<ul> <li>(C1) <code>lhs</code>, <code>rhs</code> and <code>result</code> have the same type.</li> </ul>"},{"location":"spec/#examples_93","title":"Examples","text":"<pre><code>// Bitwise operation with with integer tensors\n// %lhs: [[1, 2], [3, 4]]\n// %rhs: [[5, 6], [7, 8]]\n%result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor&lt;2x2xi32&gt;, tensor&lt;2x2xi32&gt;) -&gt; tensor&lt;2x2xi32&gt;\n// %result: [[4, 4], [4, 12]]\n\n// Logical operation with with boolean tensors\n// %lhs: [[false, false], [true, true]]\n// %rhs: [[false, true], [false, true]]\n%result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor&lt;2x2xi1&gt;, tensor&lt;2x2xi1&gt;) -&gt; tensor&lt;2x2xi1&gt;\n// %result: [[false, true], [true, false]]\n</code></pre>"},{"location":"spec/#execution","title":"Execution","text":""},{"location":"spec/#sequential-execution","title":"Sequential execution","text":"<p>A StableHLO program is executed by providing input values to the <code>main</code> function and computing output values. Output values of a function are computed by executing the graph of ops rooted in the corresponding <code>return</code> op.</p> <p>The execution order is implementation-defined, as long as ops are executed before their uses. Possible execution orders of the example program above are <code>%0</code> \u2192 <code>%1</code> \u2192 <code>%2</code> \u2192 <code>%3</code> \u2192 <code>%4</code> \u2192 <code>return</code> or <code>%3</code> \u2192 <code>%0</code> \u2192 <code>%1</code> \u2192 <code>%2</code> \u2192 <code>%4</code> \u2192 <code>return</code>.</p> <p>More formally, a StableHLO process is a combination of: 1) a StableHLO program, 2) operation statuses (not executed yet, already executed), and 3) intermediate values that the process is working on. The process starts with input values to the <code>main</code> function, progresses through the graph of ops updating operation statuses and intermediate values and finishes with output values. Further formalization is TBD.</p>"},{"location":"spec/#parallel-execution","title":"Parallel execution","text":"<p>StableHLO programs can be executed in parallel, organized into a 2D grid of <code>num_replicas</code> by <code>num_partitions</code> which both have type <code>ui32</code>.</p> <p>In the StableHLO grid, <code>num_replicas * num_partitions</code> of StableHLO processes are executing at the same time. Each process has a unique <code>process_id = (replica_id, partition_id)</code>, where <code>replica_id \u220a replica_ids = [0, ..., num_replicas-1]</code> and <code>partition_id \u220a partition_ids = [0, ..., num_partitions-1]</code> which both have type <code>ui32</code>.</p> <p>The size of the grid is known statically for every program, and the position within the grid is known statically for every process. Each process has access to its position within the grid via the <code>replica_id</code> and <code>partition_id</code> ops.</p> <p>Within the grid, the programs can all be the same (in the \"Single Program, Multiple Data\" style), can all be different (in the \"Multiple Program, Multiple Data\" style) or something in between.</p> <p>Within the grid, the processes are mostly independent from each other - they have separate operation statuses, separate input/intermediate/output values and most of the ops are executed separately between processes, with the exception of a small number of collective ops described below.</p> <p>Given that execution of most of the ops is only using values from the same process, it is usually unambiguous to refer to these values by their names. However, when describing semantics of collective ops, that is insufficient, and that gives rise to the notation <code>name@process_id</code> to refer to the value <code>name</code> within a particular process. (From that perspective, unqualified <code>name</code> can be viewed as a shorthand for <code>name@(replica_id(), partition_id())</code>).</p> <p>The execution order across processes is implementation-defined, except for the synchronization introduced by point-to-point communication and collective ops as described below.</p>"},{"location":"spec/#point-to-point-communication","title":"Point-to-point communication","text":"<p>StableHLO processes can communicate with each other through StableHLO channels. A channel is represented by a positive id of type <code>si64</code>. Through various ops, it is possible to send values to channels and receive them from channels.</p> <p>Further formalization, e.g. where these channel ids are coming from, how processes programs become aware of them and what kind of synchronization is introduced by them, is TBD.</p>"},{"location":"spec/#streaming-communication","title":"Streaming communication","text":"<p>Every StableHLO process has access to two streaming interfaces:</p> <ul> <li>Infeed that can be read from.</li> <li>Outfeed that can be written to.</li> </ul> <p>Unlike channels, which are used to communicate between processes and therefore have processes at both of their ends, infeeds and outfeeds have their other end implementation-defined.</p> <p>Further formalization, e.g. how streaming communication influences execution order and what kind of synchronization is introduced by it, is TBD.</p>"},{"location":"spec/#collective-ops","title":"Collective ops","text":"<p>There are five collective ops in StableHLO: <code>all_gather</code>, <code>all_reduce</code>, <code>all_to_all</code>, <code>collective_permute</code> and <code>reduce_scatter</code>. All these ops split the processes in the StableHLO grid into StableHLO process groups and execute a joint computation within each process group, independently from other process groups.</p> <p>Within each process group, collective ops may introduce a synchronization barrier. Further formalization, e.g. elaborating on when exactly this synchronization happens, how exactly the processes arrive at this barrier, and what happens if they don't, is TBD.</p> <p>If the process group involves cross-partition communication, i.e. there are processes in the process group whose partition ids are different, then execution of the collective op needs a channel, and the collective op must provide a positive <code>channel_id</code> of type <code>si64</code>. Cross-replica communication doesn't need channels.</p> <p>The computations performed by the collective ops are specific to individual ops and are described in individual op sections above. However, the strategies by which the grid is split into process groups are shared between these ops and are described in this section. More formally, StableHLO supports the following four strategies.</p>"},{"location":"spec/#cross_replica","title":"cross_replica","text":"<p>Only cross-replica communications happen within each process group. This strategy takes <code>replica_groups</code> - a list of lists of replica ids - and computes a Cartesian product of <code>replica_groups</code> by <code>partition_ids</code>. <code>replica_groups</code> must have unique elements and cover all <code>replica_ids</code>. More formally:</p> <pre><code>def cross_replica(replica_groups: List[List[ReplicaId]]) -&gt; List[List[ProcessId]]:\n  for replica_group in replica_groups:\n    for partition_id in partition_ids:\n      process_group = []\n      for replica_id in replica_group:\n        process_group.append((replica_id, partition_id))\n      yield process_group\n</code></pre> <p>For example, for <code>replica_groups = [[0, 1], [2, 3]]</code> and <code>num_partitions = 2</code>, <code>cross_replica</code> will produce <code>[[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(2, 0), (3, 0)], [(2, 1), (3, 1)]]</code>.</p>"},{"location":"spec/#cross_partition","title":"cross_partition","text":"<p>Only cross-partition communications happen within each process group. This strategy takes <code>partition_groups</code> - a list of lists of partition ids - and computes a Cartesian product of <code>partition_groups</code> by <code>replica_ids</code>. <code>partition_groups</code> must have unique elements and cover all <code>partition_ids</code>. More formally:</p> <pre><code>def cross_partition(partition_groups: List[List[PartitionId]]) -&gt; List[List[ProcessId]]:\n  for partition_group in partition_groups:\n    for replica_id in replica_ids:\n      process_group = []\n      for partition_id in partition_group:\n        process_group.append((replica_id, partition_id))\n      yield process_group\n</code></pre> <p>For example, for <code>partition_groups = [[0, 1]]</code> and <code>num_replicas = 4</code>, <code>cross_partition</code> will produce <code>[[(0, 0), (0, 1)], [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)]]</code>.</p>"},{"location":"spec/#cross_replica_and_partition","title":"cross_replica_and_partition","text":"<p>Both cross-replica and cross-partition communications may happen within each process group. This strategy takes <code>replica_groups</code> - a list of lists of replica ids - and computes Cartesian products of each <code>replica_group</code> by <code>partition_ids</code>. <code>replica_groups</code> must have unique elements and cover all <code>replica_ids</code>. More formally:</p> <pre><code>def cross_replica_and_partition(replica_groups: List[List[ReplicaId]]) -&gt; List[List[ProcessId]]:\n  for replica_group in replica_groups:\n    process_group = []\n    for partition_id in partition_ids:\n      for replica_id in replica_group:\n        process_group.append((replica_id, partition_id))\n    yield process_group\n</code></pre> <p>For example, for <code>replica_groups = [[0, 1], [2, 3]]</code> and <code>num_partitions = 2</code>, <code>cross_replica_and_partition</code> will produce <code>[[(0, 0), (1, 0), (0, 1), (1, 1)], [(2, 0), (3, 0), (2, 1), (3, 1)]]</code>.</p>"},{"location":"spec/#flattened_ids","title":"flattened_ids","text":"<p>This strategy takes <code>flattened_id_groups</code> - a list of lists of \"flattened\" process ids in the form of <code>replica_id * num_partitions + partition_id</code> - and turns them into process ids. <code>flattened_id_groups</code> must have unique elements and cover all <code>process_ids</code>. More formally:</p> <pre><code>def flattened_ids(flattened_id_groups: List[List[ui32]]) -&gt; List[List[ProcessId]]:\n  for flattened_id_group in flattened_id_groups:\n    process_group = []\n    for flattened_id in flattened_id_group:\n      replica_id = flattened_id // num_partitions\n      partition_id = flattened_id % num_partitions\n      process_group.append((replica_id, partition_id))\n    yield process_group\n</code></pre> <p>For example, for <code>flattened_id_groups = [[0, 1, 2, 3], [4, 5, 6, 7]]</code>, <code>num_replicas = 4</code> and <code>num_partitions = 2</code>, <code>flattened_ids</code> will produce <code>[[(0, 0), (0, 1), (1, 0), (1, 1)], [(2, 0), (2, 1), (3, 0), (3, 1)]]</code>.</p>"},{"location":"spec/#errors","title":"Errors","text":"<p>StableHLO programs are validated through an extensive set of constraints for individual ops, which rules out many classes of errors prior to run time. However, error conditions are still possible, e.g. through integer overflows, out-of-bounds accesses, etc. Unless explicitly called out, all these errors result in implementation-defined behavior.</p> <p>As an exception to this rule, floating-point exceptions in StableHLO programs have well-defined behavior. Operations which result in exceptions defined by the IEEE-754 standard (invalid operation, division-by-zero, overflow, underflow, or inexact exceptions) produce default results (as defined in the standard) and continue execution without raising the corresponding status flag; similar to <code>raiseNoFlag</code> exception handling from the standard. Exceptions for nonstandard operations (e.g. complex arithmetic and certain transcendental functions) are implementation-defined.</p>"},{"location":"spec_checklist/","title":"StableHLO Specification Checklist","text":"<p>In this document, we summarize the guidelines for reviewing changes to the specification. At the moment, these changes typically involve checking multiple things in multiple sources, so this document summarizes them all to simplify reviews:</p> <ol> <li>Check that the \"Specification\" column in status.md says \"yes\".</li> <li>Check if the section title matches the op's mnemonic in      the ODS.</li> <li>Check if the \"Semantics\" section matches XLA's      Operation Semantics.</li> <li>Check whether the \"Inputs\" and \"Outputs\" sections:<ol> <li>List the same items as the ODS.</li> <li>List the same items as HloInstruction::CreateFromProto.</li> <li>Are ordered exactly like ODS.</li> <li>If there are any mismatches, check that there are corresponding      tickets.</li> </ol> </li> <li>Check whether the \"Constraints\" section:<ol> <li>Matches XLA's      shape_inference.cc.</li> <li>Matches XLA's      hlo_verifier.cc.</li> <li>Matches the ODS.</li> <li>Matches      StablehloOps.cpp.</li> <li>If there are any mismatches, check that there are corresponding      tickets. Link all those tickets in the spec, in locations which are      as specific as possible (e.g. if a ticket is about a constraint that      hasn't been implemented, link the ticket right in that constraint).</li> <li>If the corresponding parts of the ODS and StablehloOps.cpp match the      spec, check that the \"Verification\" and \"Type Inference\" columns in      status.md      say \"yes\".</li> </ol> </li> <li>Check whether the \"Examples\" section:<ol> <li>Only has one example. (In the future, we'll link to more examples from      the StableHLO interpreter test suite).</li> <li>Uses valid MLIR syntax by running <code>stablehlo-opt</code> on code examples.</li> <li>Uses generic MLIR syntax which can be obtained by running      <code>stablehlo-opt -mlir-print-op-generic</code> (we stick to generic syntax in      the spec to avoid having to change the spec on prettyprinter changes).</li> </ol> </li> <li>Check that the <code>description</code> in op's ODS:<ol> <li>Includes the first sentence of the spec.</li> <li>Then links to the corresponding section of the spec.</li> <li>Then uses the same example as the spec but via pretty syntax which can      be obtaining by running <code>stablehlo-opt</code>.</li> </ol> </li> </ol>"},{"location":"status/","title":"StableHLO status","text":"<p>When bootstrapping StableHLO from MHLO, we have inherited MHLO's implementation of many things, including prettyprinting, verification and shape inference. Thanks to that, we already have significant coverage of the opset, but there's still plenty to do to review the existing implementations for completeness and provide new implementations where none exist.</p> <p>This live document is for the developers and the users to track the progress on various aspects of the opset - specification, verification, type inference, pretty printing, interpreter, etc.</p>"},{"location":"status/#how-to-use-it","title":"How to use it","text":"<p>The progress of a StableHLO op, as mentioned in the corresponding row, on a particular aspect, as mentioned in the corresponding column, is tracked using one of the following tracking labels.</p> <ul> <li>Generic labels</li> <li>yes: there is a comprehensive implementation.</li> <li>no: there is no implementation, but working on that is part of       the roadmap.       Note that Verifier can never be labeled as \"no\" because the ODS already       implements some verification.</li> <li>Customized labels for Verifier and Type Inference</li> <li>yes: there is an implementation, and it's in sync with     StableHLO semantics.</li> <li>yes*: there is an implementation, and it's in sync with     XLA semantics.     Since XLA semantics is oftentimes underdocumented, we are using     hlo_verifier.cc     and shape_inference.cc     as the reference.</li> <li>revisit: there is an implementation, but it doesn't fall under \"yes\"       or \"yes*\" - either because we haven't audited it yet, or because we have       and found issues.</li> <li>infeasible: there is no implementation, because it's infeasible.       For example, because the result type of an op cannot be inferred from       its operands and attributes.</li> </ul>"},{"location":"status/#status","title":"Status","text":"StableHLO Op Specification Verification Type Inference Pretty Printing Interpreter     abs yes yes yes yes no   add yes yes yes yes yes   after_all yes yes yes yes no   all_gather yes revisit no no no   all_reduce yes revisit yes no no   all_to_all yes revisit yes no no   and yes yes yes yes yes   atan2 yes revisit yes yes no   batch_norm_grad yes revisit yes no no   batch_norm_inference yes revisit yes no no   batch_norm_training yes revisit yes no no   bitcast_convert yes yes infeasible yes no   broadcast no yes* yes* yes no   broadcast_in_dim yes yes infeasible yes no   case yes revisit yes no no   cbrt yes revisit yes yes no   ceil yes yes yes yes yes   cholesky yes yes yes yes no   clamp yes revisit yes yes no   collective_permute yes revisit yes no no   compare yes yes yes yes no   complex yes yes yes yes no   compute_reshape_shape no revisit no yes no   concatenate yes yes yes yes no   constant yes yes yes yes yes   convert yes yes infeasible yes no   convolution revisit yes infeasible revisit no   cosine yes yes yes yes yes   count_leading_zeros yes yes yes yes no   create_token no yes* yes* yes no   cross-replica-sum no revisit yes* no no   cstr_reshapable no revisit no yes no   custom_call yes yes infeasible yes no   divide yes yes yes yes no   dot no revisit infeasible yes no   dot_general yes revisit infeasible no no   dynamic_broadcast_in_dim no revisit infeasible no no   dynamic_conv no revisit no no no   dynamic_gather no revisit revisit no no   dynamic_iota no revisit infeasible yes no   dynamic_pad no revisit no yes no   dynamic_reshape no revisit infeasible yes no   dynamic_slice yes revisit yes yes no   dynamic_update_slice yes yes yes yes no   einsum no revisit no yes no   exponential yes yes yes yes no   exponential_minus_one yes yes yes yes no   fft yes revisit yes yes no   floor yes yes yes yes yes   gather yes yes yes no no   get_dimension_size yes revisit yes yes no   get_tuple_element yes yes yes yes no   if yes revisit yes no no   imag yes yes yes yes no   infeed yes revisit infeasible no no   iota yes yes infeasible yes yes   is_finite yes yes yes yes no   log yes yes yes yes no   log_plus_one yes yes yes yes no   logistic yes yes yes yes no   map yes revisit yes no no   maximum yes yes yes yes yes   minimum yes yes yes yes yes   multiply yes yes yes yes yes   negate yes yes yes yes yes   not yes yes yes yes yes   optimization_barrier yes yes yes yes no   or yes yes yes yes yes   outfeed yes yes yes no no   pad yes yes yes yes no   partition_id yes yes yes yes no   popcnt yes yes yes yes no   power yes revisit yes yes no   real yes yes yes yes no   real_dynamic_slice no revisit no yes no   recv yes revisit infeasible no no   reduce yes revisit yes revisit no   reduce_precision yes yes yes yes no   reduce_scatter yes revisit no no no   reduce_window yes revisit yes no no   remainder yes yes yes yes no   replica_id yes yes yes yes no   reshape yes yes infeasible yes yes   return no revisit yes yes no   reverse yes revisit yes yes no   rng yes yes yes yes no   rng_bit_generator yes revisit infeasible yes no   round_nearest_afz yes yes yes yes no   round_nearest_even yes yes yes yes no   rsqrt yes yes yes yes no   scatter yes revisit yes no no   select yes yes yes yes no   select_and_scatter yes revisit yes no no   send yes revisit yes no no   set_dimension_size no yes* yes* yes no   shift_left yes revisit yes yes no   shift_right_arithmetic yes revisit yes yes no   shift_right_logical yes revisit yes yes no   sign yes yes yes yes no   sine yes yes yes yes yes   slice yes yes yes no no   sort yes yes yes no no   sqrt yes yes yes yes no   subtract yes yes yes yes yes   tanh yes yes yes yes yes   torch_index_select no revisit no no no   trace no revisit no yes no   transpose yes yes yes yes yes   triangular_solve yes revisit yes no no   tuple yes yes yes yes no   unary_einsum no revisit no yes no   uniform_dequantize no yes* yes* yes no   uniform_quantize no yes* infeasible yes no   while yes revisit yes revisit no   xor yes yes yes yes yes"},{"location":"type_inference/","title":"Type Inference","text":"<p>StableHLO has been originally bootstrapped from the MHLO dialect, including inheriting the implementation of type inference. The implementation progress is tracked in status.md.</p> <p>To implement high-quality verifiers and shape functions for StableHLO ops, these guidelines are proposed below to follow:</p>"},{"location":"type_inference/#proposal","title":"Proposal","text":"<p>These proposals apply to both revisiting existing implementations, and achieving new ops until a comprehensive coverage.</p>"},{"location":"type_inference/#p1-use-the-stablehlo-spec-as-the-source-of-truth","title":"(P1) Use the StableHLO spec as the source of truth","text":"<p>The spec is the source of truth for all verifiers and shape functions of the StableHLO ops. The existing verifiers and shape functions of every op need revisited to be fully aligned with the specification. Note that the specification document keeps evolving, in cases that the spec for an op is not available, the XLA implementation should be used as the source of truth instead: including xla/service/shape_inference.cc and xla/service/hlo_verifier.cc. XLA implementation doesn't cover unbounded dynamism, so for unbounded dynamism we'll apply common sense until the dynamism RFC is available.</p>"},{"location":"type_inference/#p2-make-the-most-of-ods","title":"(P2) Make the most of ODS","text":"<p>ODS files (like StablehloOps.td) define ops with traits and types for each operands/attributes/results and will do the verifications. Thus NO verification code needed in the verifiers or shape functions for the properties which are already guaranteed by the ODS. Remove the verification code if duplicated with ODS, as they will never be triggered.</p> <p>Do we need adding tests for the constraints from the ODS? Please see \u201cEstablish testing guidelines\u201d below.</p>"},{"location":"type_inference/#p3-maintain-verification-code-in-verifiers-and-shape-functions","title":"(P3) Maintain verification code in verifiers and shape functions","text":"<p>Both:</p> <ul> <li>verifiers: implemented by <code>Op::verify()</code>, and</li> <li>shape functions: implemented by <code>InferTypeOpInterface</code> like     <code>Op::inferReturnTypes()</code> or <code>Op::inferReturnTypeComponents</code></li> </ul> <p>may have verification code to check operands/attributes/results. An initial split would be that: let the verifiers check the operands/attributes, then let shape functions only calculate inferred result types and check the compatibility against the real result types. However, in reality this split has a few problems:</p> <ul> <li>The shape function can be called by the autogenerated <code>build()</code> functions,   without calling the verifier first. So the related inputs must be verified in   shape function as well.</li> <li>Duplicated code: for example in verifiers we do some processing on the   operands then verify some intermediate results, then in shape functions these   intermediate results are useful to infer the final results. These   intermediate results have to be calculated twice.</li> <li>Maintenance burden: as verifications of an op are contained in two different   methods.</li> </ul> <p>The solution is as follows:</p> <ol> <li> <p>For most ops without regions (like <code>PadOp</code>): Put all the verification code into the shape functions, and discard verifiers totally.</p> </li> <li> <p>For ops with regions (like <code>ReduceOp/IfOp</code>, a full list is here): the autogenerated builders don't take regions as parameters, so if these builders involve type inference, then the shape function will be called with empty regions, for example.</p> <ol> <li> <p>If the regions are not needed for type inference (like <code>ReduceOp</code>), put the region related verification logic in verifiers instead of the shape functions. Duplicate some code if it is inevitable.</p> </li> <li> <p>If the regions are needed for type inference (<code>IfOp/CaseOp/MapOp</code>), then additionally the shape function must verify the regions are not empty explicitly, even though the ODS may already guarantee its existence in the Op definition.</p> </li> </ol> </li> </ol>"},{"location":"type_inference/#p4-establish-testing-guidelines","title":"(P4) Establish testing guidelines","text":"<p>Do we need to add/maintain tests for verifications that are covered by ODS?</p> <p>We do not. The tests should focus on the verifiers and shape functions, while changes to ODS need a revisit of this op.</p> <p>But stay careful about the missing pieces: for example, if the op contains the trait <code>SameOperandsAndResultShape</code> which checks only shapes but not element type, then the verification for element types of operands/results still need tests.</p> <p>Where do we put tests for verifiers and type inference?</p> <p>ops_stablehlo.mlir contains the positive cases of ops, and (at least) 1 negative test for every verification error. It is also able to check the inferred return type is compatible (not the same!) as the real result type.</p> <p>infer_stablehlo.mlir verifies the existence of the shape function of an op by line with <code>hlo_test_infer.get_return_type_components\"(%x):...</code> and the inferred type matches exactly as expected. One positive test per op in general.</p>"},{"location":"type_inference/#what-to-do","title":"What to do","text":"<p>When implementing or revisiting the verifier and/or shape function of an op:</p> <ol> <li> <p>Put all positive cases and negative cases in ops_stablehlo.mlir.</p> </li> <li> <p>Add a single positive test in infer_stablehlo.mlir to test the interface.</p> </li> <li> <p>(Optional) If an op is complicated and could contain a lot of tests, consider adding a separate test file named <code>verify_&lt;op_name&gt;.mlir</code> or <code>verify_&lt;your_topic&gt;.mlir</code> within the same folder.</p> </li> </ol> <p>Note: For now, the tests for new bounded dynamism / sparsity are also put in infer_stablehlo.mlir.</p>"}]}