{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"StableHLO StableHLO is an operation set that expresses ML computations. It has been originally bootstrapped from the MHLO dialect and enhances it with additional functionality, including serialization and versioning. StableHLO is a portability layer between ML frameworks and ML compilers. We are aiming for adoption by a wide variety of ML frameworks including TensorFlow, JAX and PyTorch, as well as ML compilers including XLA and IREE. Development We're using GitHub issues / pull requests to organize development and GitHub discussions to have longer discussions. We also have a #stablehlo channel on the OpenXLA Discord server . Community With StableHLO, our goal is to create a community to build an amazing portability layer between ML frameworks and ML compilers. Let's work together on figuring out the appropriate governance to make this happen. Roadmap Workstream #1: Stable version of HLO/MHLO, including the spec , the corresponding dialect with high-quality implementations of prettyprinting , verification and type inference , and the interpeter . ETA: H2 2022. Workstream #2: Evolution beyond what's currently in HLO/MHLO. Ongoing work on dynamism , sparsity, quantization and extensibility. ETA: H2 2022. Workstream #3: Support for ML frameworks (TensorFlow, JAX, PyTorch) and ML compilers (XLA and IREE). ETA: H2 2022.","title":"Home"},{"location":"#stablehlo","text":"StableHLO is an operation set that expresses ML computations. It has been originally bootstrapped from the MHLO dialect and enhances it with additional functionality, including serialization and versioning. StableHLO is a portability layer between ML frameworks and ML compilers. We are aiming for adoption by a wide variety of ML frameworks including TensorFlow, JAX and PyTorch, as well as ML compilers including XLA and IREE.","title":"StableHLO"},{"location":"#development","text":"We're using GitHub issues / pull requests to organize development and GitHub discussions to have longer discussions. We also have a #stablehlo channel on the OpenXLA Discord server .","title":"Development"},{"location":"#community","text":"With StableHLO, our goal is to create a community to build an amazing portability layer between ML frameworks and ML compilers. Let's work together on figuring out the appropriate governance to make this happen.","title":"Community"},{"location":"#roadmap","text":"Workstream #1: Stable version of HLO/MHLO, including the spec , the corresponding dialect with high-quality implementations of prettyprinting , verification and type inference , and the interpeter . ETA: H2 2022. Workstream #2: Evolution beyond what's currently in HLO/MHLO. Ongoing work on dynamism , sparsity, quantization and extensibility. ETA: H2 2022. Workstream #3: Support for ML frameworks (TensorFlow, JAX, PyTorch) and ML compilers (XLA and IREE). ETA: H2 2022.","title":"Roadmap"},{"location":"bytecode/","text":"StableHLO Bytecode Currently Encoded Attributes / Types StableHLO Attributes and Types Documentation on the structure of the encoded attributes and types can be found in the following code comments: Attributes: See stablehlo_encoding::AttributeCode in StablehloBytecode.cpp [ link ] Types: See stablehlo_encoding::TypeCode in StablehloBytecode.cpp [ link ] CHLO Attributes and Types Documentation on the structure of the encoded attributes and types can be found in the following code comments: Attributes: See chlo_encoding::AttributeCode in ChloBytecode.cpp [ link ] Types: See chlo_encoding::TypeCode in ChloBytecode.cpp [ link ] Not Included: The following attributes / types are subclasses of builtin machinery and call into the bytecode implementations in the Builtin Dialect. StableHLO_ArrayOfLayoutAttr StableHLO_BoolElementsAttr StableHLO_FlatSymbolRefArrayAttr StableHLO_LayoutAttr HLO_ComplexTensor HLO_Complex HLO_DimensionTensor HLO_DimensionValue HLO_Float32Or64 HLO_Float HLO_Fp32Or64Tensor HLO_FpOrComplexTensor HLO_FpTensor HLO_IntFpOrComplexTensor HLO_IntOrFpTensor HLO_IntTensor HLO_Int HLO_PredIntOrFpTensor HLO_PredOrIntTensor HLO_PredTensor HLO_Pred HLO_QuantizedIntTensor HLO_QuantizedInt HLO_QuantizedSignedInt HLO_QuantizedUnsignedInt HLO_SInt HLO_ScalarIntTensor HLO_StaticShapeTensor HLO_TensorOrTokenOrTuple HLO_TensorOrToken HLO_Tensor HLO_Tuple HLO_UInt Special Cases: StableHLO_ConvolutionAttributes Despite its name, is not an attribute and is not encoded. Rather, it is a dag which gets expanded into several attributes which are all encoded separately. StableHLO_CustomCallApiVersionAttr This enum is defined strictly as an attribute of I32EnumAttr and not an EnumAttr of the StablehloDialect . This differs from FftType and other enum attributes. Because of this, it is handled by the builtin encoding. Other Notes Testing Bytecode with Round Trips Testing that the round-trip of an MLIR file produces the same results is a good way to test that the bytecode is implemented properly. $ stablehlo-opt -emit-bytecode stablehlo/tests/print_stablehlo.mlir | stablehlo-opt Find out what attributes or types are not encoded: Since attributes and types that don't get encoded are instead stored as strings, the strings command can be used to see what attributes were missed: Note: Currently all types/attrs are implemented and log only shows the dialect name stablehlo and the unregistered stablehlo.frontend_attributes and stablehlo.sharding attributes. $ stablehlo-opt -emit-bytecode file.mlir | strings | grep stablehlo stablehlo stablehlo.frontend_attributes stablehlo.sharding Debugging Bytecode with Traces Each read/write function called during bytecoding is traced, and can be viewed using the flag -debug-only=stablehlo-bytecode for StableHLO and -debug-only=chlo-bytecode for CHLO. stablehlo-opt -emit-bytecode -debug-only=stablehlo-bytecode ../tmp.mlir Called: writeType(mlir::Type, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [type:auto = mlir::stablehlo::TokenType] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::TransposeAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::RngAlgorithmAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::ChannelHandleAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::ChannelHandleAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::TypeExtensionsAttr] ... stablehlo-opt -emit-bytecode -debug-only=stablehlo-bytecode bytecoded_file.mlir Called: readComparisonDirectionAttr(mlir::DialectBytecodeReader &) const Called: readTypeExtensionsAttr(mlir::DialectBytecodeReader &) const Called: readChannelHandleAttr(mlir::DialectBytecodeReader &) const Called: readChannelHandleAttr(mlir::DialectBytecodeReader &) const Called: readRngAlgorithmAttr(mlir::DialectBytecodeReader &) const Adding Bytecode for a New Type / Attribute Adding bytecode for a new type or attribute is simple. In the file StablehloBytecode.cpp or ChloBytecode.cpp search for the term TO ADD ATTRIBUTE or TO ADD TYPE depending on the change. Ensure that each location tagged with TO ADD instructions is addressed. If so, bytecode for the attr/type should be generated on next call to stablehlo-opt -emit-bytecode . This can be verified using the proper bytecode trace. Encoding enum class values Enum class values can be encoded as their underlying numeric types using varint . Currently all enums in StableHLO use uint32_t as the underlying value.","title":"Bytecode"},{"location":"bytecode/#stablehlo-bytecode","text":"","title":"StableHLO Bytecode"},{"location":"bytecode/#currently-encoded-attributes-types","text":"","title":"Currently Encoded Attributes / Types"},{"location":"bytecode/#stablehlo-attributes-and-types","text":"Documentation on the structure of the encoded attributes and types can be found in the following code comments: Attributes: See stablehlo_encoding::AttributeCode in StablehloBytecode.cpp [ link ] Types: See stablehlo_encoding::TypeCode in StablehloBytecode.cpp [ link ]","title":"StableHLO Attributes and Types"},{"location":"bytecode/#chlo-attributes-and-types","text":"Documentation on the structure of the encoded attributes and types can be found in the following code comments: Attributes: See chlo_encoding::AttributeCode in ChloBytecode.cpp [ link ] Types: See chlo_encoding::TypeCode in ChloBytecode.cpp [ link ]","title":"CHLO Attributes and Types"},{"location":"bytecode/#not-included","text":"The following attributes / types are subclasses of builtin machinery and call into the bytecode implementations in the Builtin Dialect. StableHLO_ArrayOfLayoutAttr StableHLO_BoolElementsAttr StableHLO_FlatSymbolRefArrayAttr StableHLO_LayoutAttr HLO_ComplexTensor HLO_Complex HLO_DimensionTensor HLO_DimensionValue HLO_Float32Or64 HLO_Float HLO_Fp32Or64Tensor HLO_FpOrComplexTensor HLO_FpTensor HLO_IntFpOrComplexTensor HLO_IntOrFpTensor HLO_IntTensor HLO_Int HLO_PredIntOrFpTensor HLO_PredOrIntTensor HLO_PredTensor HLO_Pred HLO_QuantizedIntTensor HLO_QuantizedInt HLO_QuantizedSignedInt HLO_QuantizedUnsignedInt HLO_SInt HLO_ScalarIntTensor HLO_StaticShapeTensor HLO_TensorOrTokenOrTuple HLO_TensorOrToken HLO_Tensor HLO_Tuple HLO_UInt Special Cases: StableHLO_ConvolutionAttributes Despite its name, is not an attribute and is not encoded. Rather, it is a dag which gets expanded into several attributes which are all encoded separately. StableHLO_CustomCallApiVersionAttr This enum is defined strictly as an attribute of I32EnumAttr and not an EnumAttr of the StablehloDialect . This differs from FftType and other enum attributes. Because of this, it is handled by the builtin encoding.","title":"Not Included:"},{"location":"bytecode/#other-notes","text":"","title":"Other Notes"},{"location":"bytecode/#testing-bytecode-with-round-trips","text":"Testing that the round-trip of an MLIR file produces the same results is a good way to test that the bytecode is implemented properly. $ stablehlo-opt -emit-bytecode stablehlo/tests/print_stablehlo.mlir | stablehlo-opt","title":"Testing Bytecode with Round Trips"},{"location":"bytecode/#find-out-what-attributes-or-types-are-not-encoded","text":"Since attributes and types that don't get encoded are instead stored as strings, the strings command can be used to see what attributes were missed: Note: Currently all types/attrs are implemented and log only shows the dialect name stablehlo and the unregistered stablehlo.frontend_attributes and stablehlo.sharding attributes. $ stablehlo-opt -emit-bytecode file.mlir | strings | grep stablehlo stablehlo stablehlo.frontend_attributes stablehlo.sharding","title":"Find out what attributes or types are not encoded:"},{"location":"bytecode/#debugging-bytecode-with-traces","text":"Each read/write function called during bytecoding is traced, and can be viewed using the flag -debug-only=stablehlo-bytecode for StableHLO and -debug-only=chlo-bytecode for CHLO. stablehlo-opt -emit-bytecode -debug-only=stablehlo-bytecode ../tmp.mlir Called: writeType(mlir::Type, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [type:auto = mlir::stablehlo::TokenType] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::TransposeAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::RngAlgorithmAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::ChannelHandleAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::ChannelHandleAttr] Called: writeAttribute(mlir::Attribute, mlir::DialectBytecodeWriter &)::(anonymous class)::operator()(auto) const [attr:auto = mlir::stablehlo::TypeExtensionsAttr] ... stablehlo-opt -emit-bytecode -debug-only=stablehlo-bytecode bytecoded_file.mlir Called: readComparisonDirectionAttr(mlir::DialectBytecodeReader &) const Called: readTypeExtensionsAttr(mlir::DialectBytecodeReader &) const Called: readChannelHandleAttr(mlir::DialectBytecodeReader &) const Called: readChannelHandleAttr(mlir::DialectBytecodeReader &) const Called: readRngAlgorithmAttr(mlir::DialectBytecodeReader &) const","title":"Debugging Bytecode with Traces"},{"location":"bytecode/#adding-bytecode-for-a-new-type-attribute","text":"Adding bytecode for a new type or attribute is simple. In the file StablehloBytecode.cpp or ChloBytecode.cpp search for the term TO ADD ATTRIBUTE or TO ADD TYPE depending on the change. Ensure that each location tagged with TO ADD instructions is addressed. If so, bytecode for the attr/type should be generated on next call to stablehlo-opt -emit-bytecode . This can be verified using the proper bytecode trace.","title":"Adding Bytecode for a New Type / Attribute"},{"location":"bytecode/#encoding-enum-class-values","text":"Enum class values can be encoded as their underlying numeric types using varint . Currently all enums in StableHLO use uint32_t as the underlying value.","title":"Encoding enum class values"},{"location":"governance/","text":"StableHLO Governance Future governance We aim to establish an open governance model drawing from standards such as LLVM, with particular emphasis on open design/roadmap discussions, public process for gaining technical steering rights, and OSS-first docs & repo governance (e.g. location, CLA, etc), repo location. Near-term governance During the bootstrapping phase of the project in 2022, Google engineers will assume responsibility for the technical leadership of the project. It is a high priority to create a path for community members to take on technical leadership roles.","title":"StableHLO Governance"},{"location":"governance/#stablehlo-governance","text":"","title":"StableHLO Governance"},{"location":"governance/#future-governance","text":"We aim to establish an open governance model drawing from standards such as LLVM, with particular emphasis on open design/roadmap discussions, public process for gaining technical steering rights, and OSS-first docs & repo governance (e.g. location, CLA, etc), repo location.","title":"Future governance"},{"location":"governance/#near-term-governance","text":"During the bootstrapping phase of the project in 2022, Google engineers will assume responsibility for the technical leadership of the project. It is a high priority to create a path for community members to take on technical leadership roles.","title":"Near-term governance"},{"location":"reference/","text":"Interpreter Design Data Model StableHLO programs are computations over tensors (n-dimensional arrays), which, in the current model, are implemented using class Tensor . The underlying storage class for a Tensor object, detail::Buffer , stores the mlir::ShapedType of the tensor along with a mlir::HeapAsmResourceBlob object representing a mutable blob of tensor data laid out as contiguous byte array in major-to-minor order . detail::Buffer objects are reference-counted to simplify memory management. Individual elements of a tensor are represented using Element class which uses discriminated union holding one of APInt , APFloat or pair<APFloat,APFloat> for storage. The last one is used for storing elements with complex types. Tensor class has the following APIs to interact with its individual elements: - Element Tensor::get(llvm::ArrayRef<int64_t> index) : To extract an individual tensor element at multi-dimensional index index as Element object. - void Tensor::set(llvm::ArrayRef<int64_t> index, Element element); : To update an Element object element into a tensor at multi-dimensional index index . Working of the interpreter The entry function to the interpreter is SmallVector < Tensor > eval ( func :: FuncOp func , ArrayRef < Tensor > args ); which does the following: Tracks the SSA arguments of func and their associated runtime Tensor values, provided in args , using a symbol table map, M. Foreach op within func in their SSACFG order: Invokes eval on op. For each SSA operand of the op, extract its runtime value from M to be provided as argument to the eval invocation. Tracks the SSA result(s) of the op and the evaluated value in M. The op-level eval as mentioned in (2) is responsible for implementing the execution semantics of the op. Following is an example for stablehlo::AddOp . In the example, individual elements of the lhs and rhs tensors are pairwise extracted as Element objects which are then added. The result of the addition, an Element object, is stored in the final result tensor. Tensor eval ( AddOp op , const Tensor & lhs , const Tensor & rhs ) { Tensor result ( op . getType ()); for ( auto it = result . index_begin (); it != result . index_end (); ++ it ) result . set ( * it , lhs . get ( * it ) + rhs . get ( * it )); return result ; } Overall, the design of the interpreter is optimized for readability of implementations of eval functions for individual ops because it's meant to serve as a reference implementation for StableHLO. For example, instead of defining eval as a template function and parameterizing it with element types, we encapsulate details about how different element types are handled in Element::operator+ etc, simplifying the implementation of eval . Using interpreter for constant folding We can use the interpreter mechanism to fold operations with constant operand values. The following code snippet demonstrates an idea of the implementation for folding stablehlo::AddOp with floating-point typed operands: OpFoldResult AddOp::fold ( ArrayRef < Attribute > attrs ) { DenseElementsAttr lhsData = attrs [ 0 ]. dyn_cast < DenseElementsAttr > (); DenseElementsAttr rhsData = attrs [ 1 ]. dyn_cast < DenseElementsAttr > (); if ( ! lhsData || ! rhsData ) return {}; auto lhs = Tensor ( lhsData ); auto rhs = Tensor ( rhsData ); auto result = eval ( * this , lhs , rhs ); SmallVector < APFloat > values ; for ( auto i = 0 ; i < result . getNumElements (); ++ i ) { Element element = result . get ( i ); values . push_back ( element . getValue (). cast < FloatAttr > (). getValue ()); } return DenseElementsAttr :: get ( result . getType (), values ); } At the moment, we aren't actively working on integrating the interpreter into constant folding because we aren't planning to implement folder for StableHLO. However, in the future, we are planning to leverage the interpreter for constant folding in MHLO, at which point we'll improve ergonomics of the code snippet above (e.g. we could have a helper function which packs constant operands into Tensor objects and unpacks Tensor results into OpFoldResult ). Testing the interpreter The interpreter takes as inputs (A) a StableHLO program, and (B) data values to be fed to the program, and generates output data values, which are matched against the user-provided expected data values. In the current implementation, we package the inputs (MLIR program + input data values) and outputs in a lit-based test as follows: // CHECK-LABEL: Evaluated results of function: add_op_test_ui4 func . func @ add_op_test_ui4 () -> tensor < 2 xui4 > { % 0 = stablehlo . constant dense < [ 0 , 2 ] > : tensor < 2 xui4 > % 1 = stablehlo . constant dense < [ 15 , 3 ] > : tensor < 2 xui4 > % 2 = stablehlo . add % 0 , % 1 : tensor < 2 xui4 > func . return % 2 : tensor < 2 xui4 > // CHECK-NEXT: tensor<2xui4> // CHECK-NEXT: 15 : ui4 // CHECK-NEXT: 5 : ui4 } A test utility stablehlo-interpreter ( code ) is responsible for parsing the program, interpreting each function, and returning the resulting tensor(s) to be matched against the output tensor provided in FileCheck directives . We have a dedicated test-suite, consisting of several tests exercising various runtime behaviors, for each StableHLO Op. The tests can be found here (e.g. interpret_*.mlir).","title":"Interpreter design"},{"location":"reference/#interpreter-design","text":"","title":"Interpreter Design"},{"location":"reference/#data-model","text":"StableHLO programs are computations over tensors (n-dimensional arrays), which, in the current model, are implemented using class Tensor . The underlying storage class for a Tensor object, detail::Buffer , stores the mlir::ShapedType of the tensor along with a mlir::HeapAsmResourceBlob object representing a mutable blob of tensor data laid out as contiguous byte array in major-to-minor order . detail::Buffer objects are reference-counted to simplify memory management. Individual elements of a tensor are represented using Element class which uses discriminated union holding one of APInt , APFloat or pair<APFloat,APFloat> for storage. The last one is used for storing elements with complex types. Tensor class has the following APIs to interact with its individual elements: - Element Tensor::get(llvm::ArrayRef<int64_t> index) : To extract an individual tensor element at multi-dimensional index index as Element object. - void Tensor::set(llvm::ArrayRef<int64_t> index, Element element); : To update an Element object element into a tensor at multi-dimensional index index .","title":"Data Model"},{"location":"reference/#working-of-the-interpreter","text":"The entry function to the interpreter is SmallVector < Tensor > eval ( func :: FuncOp func , ArrayRef < Tensor > args ); which does the following: Tracks the SSA arguments of func and their associated runtime Tensor values, provided in args , using a symbol table map, M. Foreach op within func in their SSACFG order: Invokes eval on op. For each SSA operand of the op, extract its runtime value from M to be provided as argument to the eval invocation. Tracks the SSA result(s) of the op and the evaluated value in M. The op-level eval as mentioned in (2) is responsible for implementing the execution semantics of the op. Following is an example for stablehlo::AddOp . In the example, individual elements of the lhs and rhs tensors are pairwise extracted as Element objects which are then added. The result of the addition, an Element object, is stored in the final result tensor. Tensor eval ( AddOp op , const Tensor & lhs , const Tensor & rhs ) { Tensor result ( op . getType ()); for ( auto it = result . index_begin (); it != result . index_end (); ++ it ) result . set ( * it , lhs . get ( * it ) + rhs . get ( * it )); return result ; } Overall, the design of the interpreter is optimized for readability of implementations of eval functions for individual ops because it's meant to serve as a reference implementation for StableHLO. For example, instead of defining eval as a template function and parameterizing it with element types, we encapsulate details about how different element types are handled in Element::operator+ etc, simplifying the implementation of eval .","title":"Working of the interpreter"},{"location":"reference/#using-interpreter-for-constant-folding","text":"We can use the interpreter mechanism to fold operations with constant operand values. The following code snippet demonstrates an idea of the implementation for folding stablehlo::AddOp with floating-point typed operands: OpFoldResult AddOp::fold ( ArrayRef < Attribute > attrs ) { DenseElementsAttr lhsData = attrs [ 0 ]. dyn_cast < DenseElementsAttr > (); DenseElementsAttr rhsData = attrs [ 1 ]. dyn_cast < DenseElementsAttr > (); if ( ! lhsData || ! rhsData ) return {}; auto lhs = Tensor ( lhsData ); auto rhs = Tensor ( rhsData ); auto result = eval ( * this , lhs , rhs ); SmallVector < APFloat > values ; for ( auto i = 0 ; i < result . getNumElements (); ++ i ) { Element element = result . get ( i ); values . push_back ( element . getValue (). cast < FloatAttr > (). getValue ()); } return DenseElementsAttr :: get ( result . getType (), values ); } At the moment, we aren't actively working on integrating the interpreter into constant folding because we aren't planning to implement folder for StableHLO. However, in the future, we are planning to leverage the interpreter for constant folding in MHLO, at which point we'll improve ergonomics of the code snippet above (e.g. we could have a helper function which packs constant operands into Tensor objects and unpacks Tensor results into OpFoldResult ).","title":"Using interpreter for constant folding"},{"location":"reference/#testing-the-interpreter","text":"The interpreter takes as inputs (A) a StableHLO program, and (B) data values to be fed to the program, and generates output data values, which are matched against the user-provided expected data values. In the current implementation, we package the inputs (MLIR program + input data values) and outputs in a lit-based test as follows: // CHECK-LABEL: Evaluated results of function: add_op_test_ui4 func . func @ add_op_test_ui4 () -> tensor < 2 xui4 > { % 0 = stablehlo . constant dense < [ 0 , 2 ] > : tensor < 2 xui4 > % 1 = stablehlo . constant dense < [ 15 , 3 ] > : tensor < 2 xui4 > % 2 = stablehlo . add % 0 , % 1 : tensor < 2 xui4 > func . return % 2 : tensor < 2 xui4 > // CHECK-NEXT: tensor<2xui4> // CHECK-NEXT: 15 : ui4 // CHECK-NEXT: 5 : ui4 } A test utility stablehlo-interpreter ( code ) is responsible for parsing the program, interpreting each function, and returning the resulting tensor(s) to be matched against the output tensor provided in FileCheck directives . We have a dedicated test-suite, consisting of several tests exercising various runtime behaviors, for each StableHLO Op. The tests can be found here (e.g. interpret_*.mlir).","title":"Testing the interpreter"},{"location":"spec/","text":"StableHLO Specification Types Following are the supported element types in StableHLO: Integer types Signed integer with two\u2019s complement representation. Referred to in the document as si<N> , where the bit-width N \u220a {4, 8, 16, 32, 64}. Unsigned integer referred to in the document as ui<N> , where the bit-width N \u220a {4, 8, 16, 32, 64}. Boolean type referred to in the document as i1 . Exact representation of boolean types (e.g. 1 byte per boolean vs 1 bit per boolean) is implementation-defined. Floating-point types Single precision f32 , double precision f64 and half precision f16 floating-points complying with IEEE 754-2019 format . Bfloat16 bf16 floating-point complying with BFloat16 format . Provides the same number of exponent bits as f32 , so that it matches its dynamic range, but with greatly reduced precision. This also ensures identical behavior for underflows, overflows, and NaNs. However, bf16 handles denormals differently from f32 : it flushes them to zero. FP8 f8E4M3FN and f8E5M2 types corresponding to respectively the E4M3 and E5M2 types from the whitepaper FP8 Formats for Deep Learning . Complex types represent a pair of floating-point types. Supported ones are complex<f32> (represents a par of f32 ) and complex<f64> (represents a pair of f64 ). Exact representation of complex types (e.g. whether the real part or the imaginary part comes first in memory) is implementation-defined. Tensor types are the cornerstone of the StableHLO type system. They model immutable n-dimensional arrays and are referred to in the document as tensor<SxE> where: Shape S represented as (d0)x(d1)x...x(dR-1) is a 1-dimensional array of dimension sizes di , in the increasing order of the corresponding dimensions (which are also called axes ) 0, 1, ..., R-1. The size R of this array is called rank . Dimension sizes have type si64 and are non-negative (dimension sizes equal to zero are allowed, and their meaning is described below). Ranks equal to zero are also allowed, and their meaning is also described below. Element type E is any one of the supported element types mentioned above. For example, tensor<2x3xf32> is a tensor type with shape 2x3 and element type f32 . It has two dimensions (or, in other words, two axes) whose sizes are 2 and 3. Its rank is 2. At the logical level, a tensor<SxE> maps a 1-dimensional array of indices {i0, i1, ..., iR-1} on elements of type E . If a tensor t maps an index i on an element e , we say that t[i0, i1, ..., iR-1] = e . Individual indices have type si64 and are within the range [0, di) defined by the corresponding dimension. The size of the index array is equal to R . At the moment, StableHLO only supports dense tensors, so each tensor has 1*(d0)*(d1)*...*(dR-1) elements whose indices are drawn from an index space which is a Cartesian product of its dimensions. For example: tensor<2x3xf32> has 6 elements whose indices are {0, 0} , {0, 1} , {0, 2} , {1, 0} , {1, 1} and {1, 2} . Tensors of rank zero, e.g tensor<f32> , have 1 element. Such tensors are allowed and are useful to model scalars. Tensors with dimensions of size zero, e.g. tensor<2x0xf32> , have 0 elements. Such tensors are allowed and are useful in rare cases, e.g. to model empty slices. Canonical representation of a tensor is a 1-dimensional array of elements which correspond to indices ordered lexicographically. For example, for a tensor<2x3xf32> with the following mapping from indices to elements: {0, 0} => 1 , {0, 1} => 2 , {0, 2} => 3 , {1, 0} => 4 , {1, 1} => 5 , {1, 2} => 6 - the canonical representation would be: [1, 2, 3, 4, 5, 6] . Exact representation of tensors is implementation-defined. This specification does not define in which order tensor elements are laid out in memory (e.g. whether/when they follow the canonical order) and how individual tensor elements in a particular order are packed together into a tensor (e.g. how these elements are aligned, whether they are stored contiguously, etc). Token type Values of this type are used for imposing order on execution of side-effecting operations using data dependencies. Tuple types model heterogeneous lists and are referred to in the document using: 1) the full form: tuple<T0, ... TN-1> , 2) the short form: tuple , where: N is the tuple size. Ti are types of tuple elements. Element types are one of tensor , token or tuple . Tuple types are inherited from HLO where they are used to model variadic inputs and outputs. In StableHLO, variadic inputs and outputs are supported natively, so the only use of tuple types in StableHLO is in custom_call where tuple types are used to model HLO-compatible ABI of custom calls. Function types model functions and are referred to in the document using: 1) the full form: (I1, ..., IN) -> (O1, ..., OM) , or 2) the short form: function , where: Ii are types of inputs of the corresponding function. Oj are types of outputs of the corresponding function. Input types and output types are one of tensor , token or tuple . Function types are not first class, i.e. StableHLO doesn't support values of function types. Some StableHLO ops can take functions as inputs, but they are never produced as outputs. String type represents a sequence of bytes and is referred to in the document as string . Exact representation of string type (e.g. null terminated or not, encoding etc.) is implementation-defined. Strings types are not first class, i.e. StableHLO doesn't support values of string types. Some StableHLO ops can take strings as inputs, but they are never produced as outputs. Programs StableHLO programs consist of StableHLO functions . Each function has inputs and outputs of supported types and a list of ops in static single-assignment (SSA) form which is terminated by a return op which produces the outputs of the function. Here is an example of a program that consists of a function @main which takes three inputs ( %image , %weights and %bias ) and produces one output ( %4 ). Below we describe how this program can be executed. stablehlo.func @main( %image: tensor<28x28xf32>, %weights: tensor<784x10xf32>, %bias: tensor<1x10xf32> ) -> tensor<1x10xf32> { %0 = \"stablehlo.reshape\"(%image) : (tensor<28x28xf32>) -> tensor<1x784xf32> %1 = \"stablehlo.dot\"(%0, %weights) : (tensor<1x784xf32>, tensor<784x10xf32>) -> tensor<1x10xf32> %2 = \"stablehlo.add\"(%1, %bias) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> %3 = \"stablehlo.constant\"() { value = dense<0.0> : tensor<1x10xf32> } : () -> tensor<1x10xf32> %4 = \"stablehlo.maximum\"(%2, %3) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> \"stablehlo.return\"(%4): (tensor<1x10xf32>) -> () } Execution Sequential execution A StableHLO program is executed by providing input values to the main function and computing output values. Output values of a function are computed by executing the graph of ops rooted in the corresponding return op. The execution order is implementation-defined, as long as ops are executed before their uses. Possible execution orders of the example program above are %0 \u2192 %1 \u2192 %2 \u2192 %3 \u2192 %4 \u2192 return or %3 \u2192 %0 \u2192 %1 \u2192 %2 \u2192 %4 \u2192 return . More formally, a StableHLO process is a combination of: 1) a StableHLO program, 2) operation statuses (not executed yet, already executed), and 3) intermediate values that the process is working on. The process starts with input values to the main function, progresses through the graph of ops updating operation statuses and intermediate values and finishes with output values. Further formalization is TBD. Parallel execution StableHLO programs can be executed in parallel, organized into a 2D grid of num_replicas by num_partitions which both have type ui32 . In the StableHLO grid , num_replicas * num_partitions of StableHLO processes are executing at the same time. Each process has a unique process_id = (replica_id, partition_id) , where replica_id \u220a replica_ids = [0, ..., num_replicas-1] and partition_id \u220a partition_ids = [0, ..., num_partitions-1] which both have type ui32 . The size of the grid is known statically for every program, and the position within the grid is known statically for every process. Each process has access to its position within the grid via the replica_id and partition_id ops. Within the grid, the programs can all be the same (in the \"Single Program, Multiple Data\" style), can all be different (in the \"Multiple Program, Multiple Data\" style) or something in between. Within the grid, the processes are mostly independent from each other - they have separate operation statuses, separate input/intermediate/output values and most of the ops are executed separately between processes, with the exception of a small number of collective ops described below. Given that execution of most of the ops is only using values from the same process, it is usually unambiguous to refer to these values by their names. However, when describing semantics of collective ops, that is insufficient, and we use the notation name@process_id to refer to the value name within a particular process. (From that perspective, unqualified name can be viewed as a shorthand for name@(replica_id(), partition_id()) ). The execution order across processes is implementation-defined, except for the synchronization introduced by point-to-point communication and collective ops as described below. Point-to-point communication StableHLO processes can communicate with each other through StableHLO channels . A channel is represented by a positive id of type si64 . Through various ops, it is possible to send values to channels and receive them from channels. Further formalization, e.g. where these channel ids are coming from, how processes programs become aware of them and what kind of synchronization is introduced by them, is TBD. Streaming communication Every StableHLO process has access to two streaming interfaces: Infeed that can be read from. Outfeed that can be written to. Unlike channels, which are used to communicate between processes and therefore have processes at both of their ends, infeeds and outfeeds have their other end implementation-defined. Further formalization, e.g. how streaming communication influences execution order and what kind of synchronization is introduced by it, is TBD. Collective ops There are five collective ops in StableHLO: all_gather , all_reduce , all_to_all , collective_permute and reduce_scatter . All these ops split the processes in the StableHLO grid into StableHLO process groups and execute a joint computation within each process group, independently from other process groups. Within each process group, collective ops may introduce a synchronization barrier. Further formalization, e.g. elaborating on when exactly this synchronization happens, how exactly the processes arrive at this barrier, and what happens if they don't, is TBD. If the process group involves cross-partition communication, i.e. there are processes in the process group whose partition ids are different, then execution of the collective op needs a channel, and the collective op must provide a positive channel_id of type si64 . Cross-replica communication doesn't need channels. The computations performed by the collective ops are specific to individual ops and are described in individual op sections below. However, the strategies by which the grid is split into process groups are shared between these ops and are described in this section. More formally, StableHLO supports the following four strategies. cross_replica Only cross-replica communications happen within each process group. This strategy takes replica_groups - a list of lists of replica ids - and computes a Cartesian product of replica_groups by partition_ids . replica_groups must have unique elements and cover all replica_ids . More formally: def cross_replica ( replica_groups : List [ List [ ReplicaId ]]) -> List [ List [ ProcessId ]]: for replica_group in replica_groups : for partition_id in partition_ids : process_group = [] for replica_id in replica_group : process_group . append (( replica_id , partition_id )) yield process_group For example, for replica_groups = [[0, 1], [2, 3]] and num_partitions = 2 , cross_replica will produce [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(2, 0), (3, 0)], [(2, 1), (3, 1)]] . cross_partition Only cross-partition communications happen within each process group. This strategy takes partition_groups - a list of lists of partition ids - and computes a Cartesian product of partition_groups by replica_ids . partition_groups must have unique elements and cover all partition_ids . More formally: def cross_partition ( partition_groups : List [ List [ PartitionId ]]) -> List [ List [ ProcessId ]]: for partition_group in partition_groups : for replica_id in replica_ids : process_group = [] for partition_id in partition_group : process_group . append (( replica_id , partition_id )) yield process_group For example, for partition_groups = [[0, 1]] and num_replicas = 4 , cross_partition will produce [[(0, 0), (0, 1)], [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)]] . cross_replica_and_partition Both cross-replica and cross-partition communications may happen within each process group. This strategy takes replica_groups - a list of lists of replica ids - and computes Cartesian products of each replica_group by partition_ids . replica_groups must have unique elements and cover all replica_ids . More formally: def cross_replica_and_partition ( replica_groups : List [ List [ ReplicaId ]]) -> List [ List [ ProcessId ]]: for replica_group in replica_groups : process_group = [] for partition_id in partition_ids : for replica_id in replica_group : process_group . append (( replica_id , partition_id )) yield process_group For example, for replica_groups = [[0, 1], [2, 3]] and num_partitions = 2 , cross_replica_and_partition will produce [[(0, 0), (1, 0), (0, 1), (1, 1)], [(2, 0), (3, 0), (2, 1), (3, 1)]] . flattened_ids This strategy takes flattened_id_groups - a list of lists of \"flattened\" process ids in the form of replica_id * num_partitions + partition_id - and turns them into process ids. flattened_id_groups must have unique elements and cover all process_ids . More formally: def flattened_ids ( flattened_id_groups : List [ List [ ui32 ]]) -> List [ List [ ProcessId ]]: for flattened_id_group in flattened_id_groups : process_group = [] for flattened_id in flattened_id_group : replica_id = flattened_id // num_partitions partition_id = flattened_id % num_partitions process_group . append (( replica_id , partition_id )) yield process_group For example, for flattened_id_groups = [[0, 1, 2, 3], [4, 5, 6, 7]] , num_replicas = 4 and num_partitions = 2 , flattened_ids will produce [[(0, 0), (0, 1), (1, 0), (1, 1)], [(2, 0), (2, 1), (3, 0), (3, 1)]] . Errors StableHLO programs are validated through an extensive set of constraints for individual ops, which rules out many classes of errors prior to run time. However, error conditions are still possible, e.g. through integer overflows, out-of-bounds accesses, etc. Unless explicitly called out, all these errors result in implementation-defined behavior. As an exception to this rule, floating-point exceptions in StableHLO programs have well-defined behavior. Operations which result in exceptions defined by the IEEE-754 standard (invalid operation, division-by-zero, overflow, underflow, or inexact exceptions) produce default results (as defined in the standard) and continue execution without raising the corresponding status flag; similar to raiseNoFlag exception handling from the standard. Exceptions for nonstandard operations (e.g. complex arithmetic and certain transcendental functions) are implementation-defined. Constants The section describes the constants supported in StableHLO along with their syntax. Integer constants use decimal notation, e.g. 123 , or hexadecimal notation, e.g. ff . Negative numbers can be used with signed integer types, but not with unsigned integer types. Boolean constants true and false are both valid constants of the pred type. Floating-point constants use decimal notation, e.g. 123.421 , exponential notation, e.g. 1.23421e+2 , or a more precise hexadecimal notation, e.g. 0x42f6d78d . Complex constants Complex constants are represented as a pair of floating-point constants of f32 or f64 types, e.g. (12.34, 56.78) , where the first constant is the real part, and the second constant is the imaginary part. Tensor constants use NumPy notation. For example, [[1, 2, 3], [4, 5, 6]] is a constant of type tensor<2x3xf32> with the following mapping from indices to elements: {0, 0} => 1 , {0, 1} => 2 , {0, 2} => 3 , {1, 0} => 4 , {1, 1} => 5 , {1, 2} => 6 . String constants String constants are represented as a sequence of bytes enclosed in double quotation mark symbols, e.g. \"foo123?\" (in ASCII encoding) or \"\\18\\A3\" (in hex encoding). Index of Ops abs add after_all all_gather all_reduce all_to_all and atan2 batch_norm_grad batch_norm_inference batch_norm_training bitcast_convert broadcast_in_dim case cbrt ceil cholesky clamp collective_permute compare complex concatenate constant convert convolution cosine count_leading_zeros custom_call divide dot_general dynamic_slice dynamic_update_slice exponential exponential_minus_one fft floor gather get_tuple_element if imag infeed iota is_finite log log_plus_one logistic map maximum minimum multiply negate not optimization_barrier or outfeed pad partition_id popcnt power real recv reduce reduce_precision reduce_scatter reduce_window remainder replica_id reshape reverse rng rng_bit_generator round_nearest_afz round_nearest_even rsqrt scatter select select_and_scatter send shift_left shift_right_arithmetic shift_right_logical sign sine slice sort sqrt subtract tanh transpose triangular_solve tuple while xor stablehlo.abs Semantics Performs element-wise absolute value of operand tensor and produces a result tensor. For floating-point element types, it implements the abs operation from the IEEE-754 specification. For n-bit signed integer, the absolute value of \\(-2^{n-1}\\) is implementation- defined and one of the following: Saturation to \\(2^{n-1}-1\\) \\(-2^{n-1}\\) Inputs Name Type operand tensor of signed integer, floating-point, or complex type Outputs Name Type result tensor of signed integer, floating-point, or complex type Constraints (C1) operand and result have the same shape. (C2) operand and result have the same element type, except when the element type of the operand is complex type, in which case the element type of the result is the element type of the complex type (e.g. the element type of the result is f64 for operand type complex<f64> ). Examples // %operand: [-2, 0, 2] %result = \"stablehlo.abs\"(%operand) : (tensor<3xi32>) -> tensor<3xi32> // %result: [2, 0, 2] Back to Ops stablehlo.add Semantics Performs element-wise addition of two tensors lhs and rhs and produces a result tensor. For integer element types, if the element-wise sum has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For floating-point element types, it implements the addition operation from the IEEE-754 specification. For boolean element type, the behavior is same as stablehlo.or . Inputs Name Type lhs tensor of any supported type rhs tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.add\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[6, 8], [10, 12]] More Examples Back to Ops stablehlo.after_all Semantics Ensures that the operations producing the inputs are executed before any operations that depend on result . Execution of this operation does nothing, it only exists to establish data dependencies from result to inputs . Inputs Name Type inputs variadic number of token Outputs Name Type result token Examples %result = \"stablehlo.after_all\"(%input0, %input1) : (!stablehlo.token, !stablehlo.token) -> !stablehlo.token Back to Ops stablehlo.all_gather Semantics Within each process group in the StableHLO grid, concatenates the values of the operand tensor from each process along all_gather_dim and produces a result tensor. The operation splits the StableHLO grid into process_groups as follows: channel_id <= 0 and use_global_device_ids = false , cross_replica(replica_groups) . channel_id > 0 and use_global_device_ids = false , cross_replica_and_partition(replica_groups) . channel_id > 0 and use_global_device_ids = true , flattened_ids(replica_groups) . Afterwards, within each process_group : operands@receiver = [operand@sender for sender in process_group] for all receiver in process_group . result@process = concatenate(operands@process, all_gather_dim) for all process in process_group . Inputs Name Type operand tensor of any supported type all_gather_dim constant of type si64 replica_groups 2-dimensional tensor constant of type si64 channel_id constant of type si64 use_global_device_ids constant of type boolean Outputs Name Type result tensor of any supported type Constraints (C1) all_gather_dim \\(\\in\\) [0, rank( operand )). (C2) All values in replica_groups are unique. (C3) size(replica_groups) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_replica_and_partition , num_replicas . If flattened_ids , num_processes . (C4) \\(0 \\le\\) replica_groups [i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C5) If use_global_device_ids = true , then channel_id > 0 . todo (C6) type(result) = type(operand) except: dim(result, all_gather_dim) = dim(operand, all_gather_dim) * dim(process_groups, 1) . Examples // num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [[1.0, 2.0], [3.0, 4.0]] // %operand@(1, 0): [[5.0, 6.0], [7.0, 8.0]] %result = \"stablehlo.all_gather\"(%operand) { all_gather_dim = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> // use_global_device_ids = false } : (tensor<2x2xf32>) -> tensor<2x4xf32> // %result@(0, 0): [[1.0, 2.0, 5.0, 6.0], [3.0, 4.0, 7.0, 8.0]] // %result@(1, 0): [[1.0, 2.0, 5.0, 6.0], [3.0, 4.0, 7.0, 8.0]] Back to Ops stablehlo.all_reduce Semantics Within each process group in the StableHLO grid, applies a reduction function computation to the values of the operand tensor from each process and produces a result tensor. The operation splits the StableHLO grid into process groups as follows: channel_id <= 0 and use_global_device_ids = false , cross_replica(replica_groups) . channel_id > 0 and use_global_device_ids = false , cross_replica_and_partition(replica_groups) . channel_id > 0 and use_global_device_ids = true , flattened_ids(replica_groups) . Afterwards, within each process_group : operands@receiver = [operand@sender for sender in process_group] for all receiver in process_group . result@process[i0, i1, ..., iR-1] = reduce_without_init( inputs=operands@process[:][i0, i1, ..., iR-1], dimensions=[0], body=computation ) where reduce_without_init works exactly like reduce , except that its schedule doesn't include init values. Inputs Name Type operand tensor of any supported type replica_groups variadic number of 1-dimensional tensor constants of type si64 channel_id constant of type si64 use_global_device_ids constant of type boolean computation function Outputs Name Type result tensor of any supported type Constraints (C1) All values in replica_groups are unique. (C2) size(replica_groups) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_replica_and_partition , num_replicas . If flattened_ids , num_processes . (C3) \\(0 \\le\\) replica_groups [i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C4) If use_global_device_ids = true , then channel_id > 0 . todo (C5) computation has type (tensor<E>, tensor<E>) -> (tensor<E>) where E = element_type(operand) . (C6) type( result ) \\(=\\) type( operand ). Examples // num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [1.0, 2.0, 3.0, 4.0] // %operand@(1, 0): [5.0, 6.0, 7.0, 8.0] %result = \"stablehlo.all_reduce\"(%operand) ({ ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32> \"stablehlo.return\"(%0) : (tensor<f32>) -> () }) { replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> // use_global_device_ids = false } : (tensor<4xf32>) -> tensor<4xf32> // %result@(0, 0): [6.0, 8.0, 10.0, 12.0] // %result@(1, 0): [6.0, 8.0, 10.0, 12.0] Back to Ops stablehlo.all_to_all Semantics Within each process group in the StableHLO grid, splits the values of the operand tensor along split_dimension into parts, scatters the split parts between the processes, concatenates the scattered parts along concat_dimension and produces a result tensor. The operation splits the StableHLO grid into process groups using the cross_replica(replica_groups) strategy. Afterwards, within each process_group : split_parts@sender = [ slice( operand=operand@sender, start_indices=[s0, s1, ..., sR-1], # where # - sj = 0 if j != split_dimension # - sj = i * dim(operand, j) / split_count, if j == split_dimension # - R = rank(operand) limit_indices=[l0, l1, ..., lR-1], # where # - lj = dim(operand, j) if j != split_dimension # - lj = (i + 1) * dim(operand, j) / split_count, if j == split_dimension strides=[1, ..., 1] ) for i in range(split_count) ] for all sender in process_group . scattered_parts@receiver = [split_parts@sender[receiver_index] for sender in process_group] where receiver_index = index_of(receiver, process_group) . result@process = concatenate(scattered_parts@process, concat_dimension) . Inputs Name Type operand tensor of any supported type split_dimension constant of type si64 concat_dimension constant of type si64 split_count constant of type si64 replica_groups 2-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) split_dimension \\(\\in\\) [0, rank( operand )). (C2) dim( operand , split_dimension ) % split_count \\(=\\) 0. (C3) concat_dimension \\(\\in\\) [0, rank( operand )). (C4) split_count \\(\\gt\\) 0. (C5) All values in replica_groups are unique. (C6) size(replica_groups) = num_replicas . (C7) \\(0 \\le\\) replica_groups [i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C8) type(result) = type(operand) except: dim(result, split_dimension) = dim(operand, split_dimension) / split_count . dim(result, concat_dimension) = dim(operand, concat_dimension) * split_count . Examples // num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [ // [1.0, 2.0, 3.0, 4.0], // [5.0, 6.0, 7.0, 8.0] // ] // %operand@(1, 0): [ // [9.0, 10.0, 11.0, 12.0], // [13.0, 14.0, 15.0, 16.0] // ] %result = \"stablehlo.all_to_all\"(%operand) { split_dimension = 1 : i64, concat_dimension = 0 : i64, split_count = 2 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64> } : (tensor<2x4xf32>) -> tensor<4x2xf32> // %result@(0, 0): [ // [1.0, 2.0], // [5.0, 6.0], // [9.0, 10.0], // [13.0, 14.0] // ] // %result@(1, 0): [ // [3.0, 4.0], // [7.0, 8.0], // [11.0, 12.0], // [15.0, 16.0] // ] Back to Ops stablehlo.and Semantics Performs element-wise bitwise or logical AND of two tensors lhs and rhs and produces a result tensor. For integer tensors, computes the bitwise operation. For boolean tensors, computes the logical operation. Inputs Name Type lhs tensor of integer or boolean type rhs tensor of integer or boolean type Outputs Name Type result tensor of integer or boolean type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.and\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[1, 2], [3, 0]] Back to Ops stablehlo.atan2 Semantics Performs element-wise atan2 operation on lhs and rhs tensor and produces a result tensor, implementing the atan2 operation from the IEEE-754 specification. For complex element types, it computes a complex atan2 function with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type lhs tensor of floating-point or complex type rhs tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) lhs , rhs , and result have the same type. Examples // %lhs: [0.0, 1.0, -1.0] // %rhs: [0.0, 0.0, 0.0] %result = \"stablehlo.atan2\"(%lhs, %rhs) : (tensor<3xf32>, tensor<3xf32>) -> tensor<3xf32> // %result: [0.0, 1.57079637, -1.57079637] // [0.0, pi/2, -pi/2] Back to Ops stablehlo.batch_norm_grad Semantics Computes gradients of several inputs of batch_norm_training backpropagating from grad_output , and produces grad_operand , grad_scale and grad_offset tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows: def compute_sum ( operand , feature_index ): ( sum ,) = reduce ( inputs = [ operand ], init_values = [ 0.0 ], dimensions = [ i for i in range ( rank ( operand )) if i != feature_index ], body = lambda x , y : add ( x , y )) return sum def compute_mean ( operand , feature_index ): sum = compute_sum ( operand , feature_index ) divisor = constant ( num_elements ( operand ) / dim ( operand , feature_index )) divisor_bcast = broadcast_in_dim ( divisor , [], shape ( sum )) return divide ( sum , divisor_bcast ) def batch_norm_grad ( operand , scale , mean , variance , grad_output , epsilon , feature_index ): # Broadcast inputs to shape(operand) scale_bcast = broadcast_in_dim ( scale , [ feature_index ], shape ( operand )) mean_bcast = broadcast_in_dim ( mean , [ feature_index ], shape ( operand )) variance_bcast = broadcast_in_dim ( variance , [ feature_index ], shape ( operand )) epsilon_bcast = broadcast_in_dim ( constant ( epsilon ), [], shape ( operand )) # Perform normalization using the provided `mean` and `variance` # Intermediate values will be useful for computing gradients centered_operand = subtract ( operand , mean_bcast ) stddev = sqrt ( add ( variance_bcast , epsilon_bcast )) normalized_operand = divide ( centered_operand , stddev ) # Use the implementation from batchnorm_expander.cc in XLA # Temporary variables have exactly the same names as in the C++ code elements_per_feature = constant ( divide ( size ( operand ), dim ( operand , feature_index ))) i1 = multiply ( grad_output , broadcast_in_dim ( elements_per_feature , [], shape ( operand ))) i2 = broadcast_in_dim ( compute_sum ( grad_output , feature_index ), [ feature_index ], shape ( operand )) i3 = broadcast_in_dim ( compute_sum ( multiply ( grad_output , centered_operand )), [ feature_index ], shape ( operand )) i4 = multiply ( i3 , centered_operand ) i5 = divide ( i4 , add ( variance_bcast , epsilon_bcast )) grad_operand = multiply ( divide ( divide ( scale_bcast , stddev ), elements_per_feature ), subtract ( subtract ( i1 , i2 ), i5 )) grad_scale = compute_sum ( multiply ( grad_output , normalized_operand ), feature_index ) grad_offset = compute_sum ( grad_output , feature_index ) return grad_operand , grad_scale , grad_offset Inputs Name Type operand tensor of floating-point type scale 1-dimensional tensor of floating-point type mean 1-dimensional tensor of floating-point type variance 1-dimensional tensor of floating-point type grad_output tensor of floating-point type epsilon constant of type f32 feature_index constant of type si64 Outputs Name Type grad_operand tensor of floating-point type grad_scale 1-dimensional tensor of floating-point type grad_offset 1-dimensional tensor of floating-point type Constraints (C1) 0 \\(\\le\\) feature_index \\(\\lt\\) rank( operand ). (C2) operand , scale , mean , variance , grad_output , grad_operand grad_scale and grad_offset have the same element type. (C3) operand , grad_output and grad_operand have the same shape. (C4) scale , mean , variance , grad_scale and grad_offset have the same shape. (C5) size( scale ) \\(=\\) dim(operand, feature_index) . Examples // %operand: [ // [[1.0, 2.0], [3.0, 4.0]], // [[3.0, 4.0], [1.0, 2.0]] // ] // %scale: [1.0, 1.0] // %mean: [2.0, 3.0] // %variance: [1.0, 1.0] // %grad_output: [ // [[0.1, 0.1], [0.1, 0.1]], // [[0.1, 0.1], [0.1, 0.1]] // ] %grad_operand, %grad_scale, %grad_offset = \"stablehlo.batch_norm_grad\"(%operand, %scale, %mean, %variance, %grad_output) { epsilon = 0.0 : f32, feature_index = 2 : i64 } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2x2x2xf32>) -> (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) // %grad_operand: [ // [[0.0, 0.0], [0.0, 0.0]], // [[0.0, 0.0], [0.0, 0.0]] // ] // %grad_scale: [0.0, 0.0] // %grad_offset: [0.4, 0.4] Back to Ops stablehlo.batch_norm_inference Semantics Normalizes the operand tensor across all dimensions except for the feature_index dimension and produces a result tensor. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows: def batch_norm_inference ( operand , scale , offset , mean , variance , epsilon , feature_index ): # Broadcast inputs to shape(operand) scale_bcast = broadcast_in_dim ( scale , [ feature_index ], shape ( operand )) offset_bcast = broadcast_in_dim ( offset , [ feature_index ], shape ( operand )) mean_bcast = broadcast_in_dim ( mean , [ feature_index ], shape ( operand )) variance_bcast = broadcast_in_dim ( variance , [ feature_index ], shape ( operand )) epsilon_bcast = broadcast_in_dim ( constant ( epsilon ), [], shape ( operand )) # Perform normalization using the provided `mean` and `variance` instead of # computing them like `batch_norm_training` does. centered_operand = subtract ( operand , mean_bcast ) stddev = sqrt ( add ( variance_bcast , epsilon_bcast )) normalized_operand = divide ( centered_operand , stddev ) return add ( multiply ( scale_bcast , normalized_operand ), offset_bcast ) Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point type scale 1-dimensional tensor of floating-point type offset 1-dimensional tensor of floating-point type mean 1-dimensional tensor of floating-point type variance 1-dimensional tensor of floating-point type epsilon constant of type f32 feature_index constant of type si64 Outputs Name Type result tensor of floating-point type Constraints (C1) 0 \\(\\le\\) feature_index \\(\\lt\\) rank( operand ). (C2) operand , scale , offset , mean , variance and result have the same element type. (C3) size( scale ) \\(=\\) dim(operand, feature_index) . (C4) size( offset ) \\(=\\) dim(operand, feature_index) . (C5) size( mean ) \\(=\\) dim(operand, feature_index) . (C6) size( variance ) \\(=\\) dim(operand, feature_index) . (C7) operand and result have the same type. Examples // %operand: [ // [[1.0, 2.0], [3.0, 4.0]], // [[3.0, 4.0], [1.0, 2.0]] // ] // %scale: [1.0, 1.0] // %offset: [1.0, 1.0] // %mean: [2.0, 3.0] // %variance: [1.0, 1.0] %result = \"stablehlo.batch_norm_inference\"(%operand, %scale, %offset, %mean, %variance) { epsilon = 0.0 : f32, feature_index = 2 : i64 } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>) -> tensor<2x2x2xf32> // %result: [ // [[0.0, 0.0], [2.0, 2.0]], // [[2.0, 2.0], [0.0, 0.0]] // ] Back to Ops stablehlo.batch_norm_training Semantics Computes mean and variance across all dimensions except for the feature_index dimension and normalizes the operand tensor producing output , batch_mean and batch_var tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows: def compute_mean ( operand , feature_index ): ( sum ,) = reduce ( inputs = [ operand ], init_values = [ 0.0 ], dimensions = [ i for i in range ( rank ( operand )) if i != feature_index ], body = lambda x , y : add ( x , y )) divisor = constant ( num_elements ( operand ) / dim ( operand , feature_index )) divisor_bcast = broadcast_in_dim ( divisor , [], shape ( sum )) return divide ( sum , divisor_bcast ) def compute_variance ( operand , feature_index ): mean = compute_mean ( operand , feature_index ) mean_bcast = broadcast_in_dim ( mean , [ feature_index ], shape ( operand )) centered_operand = subtract ( operand , mean_bcast ) return compute_mean ( mul ( centered_operand , centered_operand ), feature_index ) def batch_norm_training ( operand , scale , offset , epsilon , feature_index ): mean = compute_mean ( operand , feature_index ) variance = compute_variance ( operand , feature_index ) return batch_norm_inference ( operand , scale , offset , mean , variance , epsilon , feature_index ) Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point type scale 1-dimensional tensor of floating-point type offset 1-dimensional tensor of floating-point type epsilon constant of type f32 feature_index constant of type si64 Outputs Name Type output tensor of floating-point type batch_mean 1-dimensional tensor of floating-point type batch_var 1-dimensional tensor of floating-point type Constraints (C1) 0 \\(\\le\\) feature_index \\(\\lt\\) rank( operand ). (C2) operand , scale , offset , result , batch_mean and batch_var have the same element type. (C3) size( scale ) \\(=\\) dim(operand, feature_index) . (C4) size( offset ) \\(=\\) dim(operand, feature_index) . (C5) size( batch_mean ) \\(=\\) dim(operand, feature_index) . (C6) size( batch_var ) \\(=\\) dim(operand, feature_index) . (C7) operand and output have the same type. Examples // %operand: [ // [[1.0, 2.0], [3.0, 4.0]], // [[3.0, 4.0], [1.0, 2.0]] // ] // %scale: [1.0, 1.0] // %offset: [1.0, 1.0] %output, %batch_mean, %batch_var = \"stablehlo.batch_norm_training\"(%operand, %scale, %offset) { epsilon = 0.0 : f32, feature_index = 2 : i64 } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) -> (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) // %output: [ // [[0.0, 0.0], [2.0, 2.0]], // [[2.0, 2.0], [0.0, 0.0]] // ] // %batch_mean: [2.0, 3.0] // %batch_var: [1.0, 1.0] Back to Ops stablehlo.bitcast_convert Semantics Performs a bitcast operation on operand tensor and produces a result tensor where the bits of the entire operand tensor are reinterpreted using the type of the result tensor. Let E and E' be the operand and result element type respectively, and R = rank(operand) : If num_bits(E') \\(=\\) num_bits(E) , bits(result[i0, ..., iR-1]) = bits(operand[i0, ..., iR-1]) . If num_bits(E') \\(\\lt\\) num_bits(E) , bits(result[i0, ..., iR-1, :]) = bits(operand[i0, ..., iR-1]) . If num_bits(E') \\(\\gt\\) num_bits(E) , bits(result[i0, ..., iR-2]) = bits(operand[i0, ..., iR-2, :]) . The behavior of bits is implementation-defined because the exact representation of tensors is implementation-defined, and the exact representation of element types is implementation-defined as well. Inputs Name Type operand tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) Let E and E' be the operand and result element type, respectively and R = rank(operand) : If num_bits(E') \\(=\\) num_bits(E) , shape( result ) \\(=\\) shape( operand ). If num_bits(E') \\(\\lt\\) num_bits(E) : rank(result) = R+1 . dim( result , i ) \\(=\\) dim( operand , i ) for all i \\(\\in\\) [0, R -1]. dim(result, R) = num_bits(E)/num_bits(E') . If num_bits(E') \\(\\gt\\) num_bits(E) : rank(result) = R-1 . dim( result , i ) \\(=\\) dim( operand , i ) for all i \\(\\in\\) [0, R -1). dim(operand, R-1) = num_bits(E')/num_bits(E) . (C2) Conversion between complex and non-complex types is not permitted. Examples // %operand: [0.0, 1.0] %result = \"stablehlo.bitcast_convert\"(%operand) : (tensor<2xf32>) -> tensor<2x4xi8> // %result: [ // [0, 0, 0, 0], // [0, 0, -128, 63] // little-endian representation of 1.0 // ] Back to Ops stablehlo.broadcast_in_dim Semantics Expands the dimensions and/or rank of an input tensor by duplicating the data in the operand tensor and produces a result tensor. Formally, result[i0, i1, ..., iR-1] \\(=\\) operand[j0, j1, ..., jR'-1] such that jk \\(=\\) dim(operand, k) == 1 ? 0 : i[broadcast_dimensions[k]] for all dimensions k in operand . Inputs Name Type operand tensor of any supported type broadcast_dimensions 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same element type. (C2) size( broadcast_dimensions ) \\(=\\) rank( operand ). (C3) \\(0 \\le\\) broadcast_dimensions[i] \\(\\lt\\) rank( result ) for all dimensions i in operand . (C4) All dimensions in broadcast_dimensions are unique. (C5) For all dimensions j in operand : dim(operand, j) = 1 or dim(operand, j) = dim(result, i) where i = broadcast_dimensions[j] . Examples // %operand: [ // [1, 2, 3] // ] %result = \"stablehlo.broadcast_in_dim\"(%operand) { broadcast_dimensions = dense<[2, 1]>: tensor<2xi64> } : (tensor<1x3xi32>) -> tensor<2x3x2xi32> // %result: [ // [ // [1, 1], // [2, 2], // [3, 3] // ], // [ // [1, 1], // [2, 2], // [3, 3] // ] // ] Back to Ops stablehlo.case Semantics Produces the output from executing exactly one function from branches depending on the value of index . Formally, if \\(0 \\le\\) index \\(\\lt\\) N-1 , output of branches[index] is returned, else, output of branches[N-1] is returned. Inputs Name Type index 1-dimensional tensor of type si32 branches variadic number of function Outputs Name Type results variadic number of tensors of any supported type or tokens Constraints (C1) branches have at least one function. (C2) All functions in branches have 0 inputs. (C3) All functions in branches have the same output types. (C4) For all i , type(results[i]) = type(branches[0]).outputs[i] . Examples // %result_branch0: 10 // %result_branch1: 11 // %index: 1 %result = \"stablehlo.case\"(%index) ({ \"stablehlo.return\"(%result_branch0) : (tensor<i32>) -> () }, { \"stablehlo.return\"(%result_branch1) : (tensor<i32>) -> () }) : (tensor<i32>) -> tensor<i32> // %result: 11 Back to Ops stablehlo.cbrt Semantics Performs element-wise cubic root operation on operand tensor and produces a result tensor, implementing the rootn(x, 3) operation from the IEEE-754 specification. For complex element types, it computes a complex cubic root, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [0.0, 1.0, 8.0, 27.0] %result = \"stablehlo.cbrt\"(%operand) : (tensor<4xf32>) -> tensor<4xf32> // %result: [0.0, 1.0, 2.0, 3.0] Back to Ops stablehlo.ceil Semantics Performs element-wise ceil of operand tensor and produces a result tensor. Implements the roundToIntegralTowardPositive operation from the IEEE-754 specification. Inputs Name Type operand tensor of floating-point type Outputs Name Type result tensor of floating-point type Constraints (C1) operand and result have the same type. Examples // %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0] %result = \"stablehlo.ceil\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-0.0, -0.0, 1.0, 1.0, 2.0] More Examples Back to Ops stablehlo.cholesky Semantics Computes the Cholesky decomposition of a batch of matrices. More formally, for all i , result[i0, ..., iR-3, :, :] is a Cholesky decomposition of a[i0, ..., iR-3, :, :] , in the form of either of a lower-triangular (if lower is true ) or upper-triangular (if lower is false ) matrix. The output values in the opposite triangle, i.e. the strict upper triangle or strict lower triangle correspondingly, are implementation-defined. If there exists i where the input matrix is not an Hermitian positive-definite matrix, then the behavior is undefined. Inputs Name Type a tensor of floating-point or complex type lower 0-dimensional tensor constant of type i1 Outputs Name Type result tensor of floating-point or complex type Constraints (C1) a and result have the same type. (C2) rank( a ) >= 2. (C3) dim( a , -2) = dim( a , -1). Examples // %a: [ // [1.0, 2.0, 3.0], // [2.0, 20.0, 26.0], // [3.0, 26.0, 70.0] // ] %result = \"stablehlo.cholesky\"(%a) { lower = true } : (tensor<3x3xf32>) -> tensor<3x3xf32> // %result: [ // [1.0, 0.0, 0.0], // [2.0, 4.0, 0.0], // [3.0, 5.0, 6.0] // ] Back to Ops stablehlo.clamp Semantics Clamps every element of the operand tensor between a minimum and maximum value and produces a result tensor. More formally, result[i0, ..., iR-1] = minimum(maximum(operand[i0, ..., iR-1], min_val), max_val) , where min_val = rank(min) == 0 ? min : min[i0, ..., iR-1] , max_val = rank(max) == 0 ? max : max[i0, ..., iR-1] , minimum and maximum operations correspond to stablehlo.minimum and stablehlo.maximum . Inputs Name Type min tensor of any supported type operand tensor of any supported type max tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) Either rank(min) \\(=\\) 0 or shape(min) \\(=\\) shape(operand) . (C2) Either rank(max) \\(=\\) 0 or shape(max) \\(=\\) shape(operand) . (C3) min , operand , and max have the same element type. (C4) operand and result have the same type. Examples // %min: [5, 10, 15] // %operand: [3, 13, 23] // %max: [10, 15, 20] %result = \"stablehlo.clamp\"(%min, %operand, %max) : (tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> tensor<3xi32> // %result: [5, 13, 20] Back to Ops stablehlo.collective_permute Semantics Within each process group in the StableHLO grid, sends the value of the operand tensor from the source process to the target process and produces a result tensor. The operation splits the StableHLO grid into process_groups as follows: channel_id <= 0 , cross_replica(replica_groups) . channel_id > 0 , cross_partition(replica_groups) . Afterwards, result@process is given by: operand@process_groups[i, 0] , if there exists an i such that process_groups[i, 1] = process . broadcast_in_dim(0, [], shape(result)) , otherwise. Inputs Name Type operand tensor of any supported type source_target_pairs 2-dimensional tensor constant of type si64 channel_id constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) dim( source_target_pairs , 1) \\(=\\) 2. (C2) All values in source_target_pairs[:, 0] are unique. (C3) All values in source_target_pairs[:, 1] are unique. (C4) \\(0 \\le\\) source_target_pairs[i][0], source_target_pairs[i][1] \\(\\lt N\\) , where \\(N\\) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_partition , num_partitions . (C5) type( result ) \\(=\\) type( operand ). Examples // num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [[1, 2], [3, 4]] // %operand@(1, 0): [[5, 6], [7, 8]] %result = \"stablehlo.collective_permute\"(%operand) { source_target_pairs = dense<[[0, 1]]> : tensor<2x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> } : (tensor<2x2xf32>) -> tensor<2x2xf32> // // %result@(0, 0): [[0, 0], [0, 0]] // %result@(1, 0): [[1, 2], [3, 4]] Back to Ops stablehlo.compare Semantics Performs element-wise comparison of lhs and rhs tensors according to comparison_direction and compare_type , and produces a result tensor. The values of comparison_direction and compare_type have the following semantics: For integer and boolean element types: EQ : lhs \\(=\\) rhs . NE : lhs \\(\\ne\\) rhs . GE : lhs \\(\\ge\\) rhs . GT : lhs \\(\\gt\\) rhs . LE : lhs \\(\\le\\) rhs . LT : lhs \\(\\lt\\) rhs . For floating-point element types and compare_type = FLOAT , the op implements the following IEEE-754 operations: EQ : compareQuietEqual . NE : compareQuietNotEqual . GE : compareQuietGreaterEqual . GT : compareQuietGreater . LE : compareQuietLessEqual . LT : compareQuietLess . For floating-point element types and compare_type = TOTALORDER , the op uses the combination of totalOrder and compareQuietEqual operations from IEEE-754. For complex element types, lexicographic comparison of (real, imag) pairs is performed using the provided comparison_direction and compare_type . Inputs Name Type lhs tensor of any supported type rhs tensor of any supported type comparison_direction enum of EQ , NE , GE , GT , LE , and LT compare_type enum of FLOAT , TOTALORDER , SIGNED , and UNSIGNED Outputs Name Type result tensor of boolean type Constraints (C1) lhs and rhs have the same element type. (C2) lhs , rhs , and result have the same shape. (C3) Given E is the lhs element type, the following are legal values of compare_type : If E is signed integer type, compare_type = SIGNED . If E is unsigned integer or boolean type, compare_type = UNSIGNED . If E is floating-point type, compare_type \\(\\in\\) { FLOAT , TOTALORDER }. If E is complex type, compare_type = FLOAT . Examples // %lhs: [1.0, 3.0] // %rhs: [1.1, 2.9] %result = \"stablehlo.compare\"(%lhs, %rhs) { comparison_direction = #stablehlo<comparison_direction LT>, compare_type = #stablehlo<comparison_type FLOAT> } : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xi1> // %result: [true, false] Back to Ops stablehlo.complex Semantics Performs element-wise conversion to a complex value from a pair of real and imaginary values, lhs and rhs , and produces a result tensor. Inputs Name Type lhs tensor of type f32 or f64 rhs tensor of type f32 or f64 Outputs Name Type result tensor of complex type Constraints (C1) lhs and rhs have the same type. (C2) shape( result ) \\(=\\) shape( lhs ). (C3) element_type( result ) = complex_type(element_type( lhs )). Examples // %lhs: [1.0, 3.0] // %rhs: [2.0, 4.0] %result = \"stablehlo.complex\"(%lhs, %rhs) : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xcomplex<f32>> // %result: [(1.0, 2.0), (3.0, 4.0)] Back to Ops stablehlo.concatenate Semantics Concatenates a variadic number of tensors in inputs along dimension dimension in the same order as the given arguments and produces a result tensor. More formally, result[i0, ..., id, ..., iR-1] = inputs[k][i0, ..., kd, ..., iR-1] , where: id = d0 + ... + dk-1 + kd . d is equal to dimension , and d0 , ... are d th dimension sizes of inputs . Inputs Name Type inputs variadic number of tensors of any supported type dimension constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) All tensors in inputs have the same element type. (C2) All tensors in inputs have the same shape except for the size of the dimension th dimension. (C3) inputs have N tensors where N >= 1. (C4) 0 \\(\\le\\) dimension \\(\\lt\\) rank of inputs[0] . (C5) result has the same element type as the tensors in inputs . (C6) result has the same shape as the tensors in inputs except for the size of the dimension th dimension, which is calculated as a sum of the size of inputs[k][dimension] for all k in inputs . Examples // %input0: [[1, 2], [3, 4], [5, 6]] // %input1: [[7, 8]] %result = \"stablehlo.concatenate\"(%input0, %input1) { dimension = 0 : i64 } : (tensor<3x2xi32>, tensor<1x2xi32>) -> tensor<4x2xi32> // %result: [[1, 2], [3, 4], [5, 6], [7, 8]] Back to Ops stablehlo.constant Semantics Produces an output tensor from a constant value . Inputs Name Type value constant of any supported type Outputs Name Type output tensor of any supported type Constraints (C1) value and output have the same type. Examples %output = \"stablehlo.constant\"() { value = dense<[[0.0, 1.0], [2.0, 3.0]]> : tensor<2x2xf32> } : () -> tensor<2x2xf32> // %output: [[0.0, 1.0], [2.0, 3.0]] More Examples Back to Ops stablehlo.convert Semantics Performs an element-wise conversion from one element type to another on operand tensor and produces a result tensor. For conversions involving integer-to-integer , if there is an unsigned/signed overflow, the result is implementation-defined and one of the following: * mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . * saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For conversions involving floating-point-to-floating-point or integer-to-floating-point , if the source value can be exactly represented in the destination type, the result value is that exact representation. Otherwise, the behavior is TBD. Conversion involving complex-to-complex follows the same behavior of floating-point-to-floating-point conversions for converting real and imaginary parts. For conversions involving floating-point-to-complex or complex-to-floating-point , the destination imaginary value is zeroed or the source imaginary value is ignored, respectively. The conversion of the real part follows the floating-point-to-floating-point conversion. Conversions involving integer-to-complex follows the same behavior as integer-to-floating-point conversion while converting the source integer to destination real part. The destination imaginary part is zeroed. For conversions involving floating-point-to-integer , the fractional part is truncated. If the truncated value cannot be represented in the destination type, the behavior is TBD. Conversions involving complex-to-integer follows the same behavior while converting the source real part to destination integer. The source imaginary part is ignored. For boolean-to-any-supported-type conversions, the value false is converted to zero, and the value true is converted to one. For any-supported-type-to-boolean conversions, a zero value is converted to false and any non-zero value is converted to true . Inputs Name Type operand tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same shape. Examples // %operand: [1, 2, 3] %result = \"stablehlo.convert\"(%operand) : (tensor<3xi32>) -> tensor<3xcomplex<f32>> // %result: [(1.0, 0.0), (2.0, 0.0), (3.0, 0.0)] Back to Ops stablehlo.convolution Semantics Computes dot products between windows of lhs and slices of rhs and produces result . The following diagram shows how elements in result are computed from lhs and rhs using a concrete example. More formally, we start with reframing the inputs to the operation in terms of lhs in order to be able to express windows of lhs : lhs_window_dimensions = lhs_shape(dim(lhs, input_batch_dimension), dim(rhs, kernel_spatial_dimensions), dim(lhs, input_feature_dimension)) . lhs_window_strides = lhs_shape(1, window_strides, 1) . lhs_padding = lhs_shape([0, 0], padding, [0, 0]) . lhs_base_dilations = lhs_shape(1, lhs_dilation, 1) . lhs_window_dilations = lhs_shape(1, rhs_dilation, 1) . This reframing uses the following helper functions: lhs_shape(n, hw, c) = permute([n] + hw + [c], [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension]) . result_shape(n1, hw, c1) = permute([n1] + hw + [c1], [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension]) . If feature_group_count = 1 and batch_group_count = 1 , then for all output_spatial_index in the index space of dim(result, output_spatial_dimensions) , result[result_shape(:, output_spatial_index, :)] = dot_product where: padded_lhs = pad(lhs, 0, lhs_padding[:, 0], lhs_padding[:, 1], lhs_base_dilations) . lhs_window_start = lhs_shape(0, output_spatial_index, 0) * lhs_window_strides . lhs_window = slice(padded_lhs, lhs_window_start, lhs_window_start + lhs_window_dimensions, lhs_window_dilations) . reversed_lhs_window = reverse(lhs_window, [input_spatial_dimensions[dim] for dim in [0, size(window_reversal) and window_reversal[dim] = true]) . dot_product = dot_general(reversed_lhs_window, rhs, lhs_batching_dimensions=[], lhs_contracting_dimensions=input_spatial_dimensions + [input_feature_dimension], rhs_batching_dimensions=[], rhs_contracting_dimensions=kernel_spatial_dimensions + [kernel_input_feature_dimension]) . If feature_group_count > 1 : lhses = split(lhs, feature_group_count, input_feature_dimension) . rhses = split(rhs, feature_group_count, kernel_output_feature_dimension) . results[:] = convolution(lhses[:], rhses[:], ..., feature_group_count=1, ...) . result = concatenate(results, output_feature_dimension) . If batch_group_count > 1 : lhses = split(lhs, batch_group_count, input_batch_dimension) . rhses = split(rhs, batch_group_count, kernel_output_feature_dimension) . results[:] = convolution(lhses[:], rhses[:], ..., batch_group_count=1, ...) . result = concatenate(results, output_feature_dimension) . Inputs Name Type Constraints lhs tensor of any supported type (C1), (C2), (C11), (C12), (C26), (C27) rhs tensor of any supported type (C1), (C2), (C15), (C16), (C17), (C26) window_strides 1-dimensional tensor constant of type si64 (C3), (C4), (C26) padding 2-dimensional tensor constant of type si64 (C5), (C26) lhs_dilation 1-dimensional tensor constant of type si64 (C6), (C7), (C26) rhs_dilation 1-dimensional tensor constant of type si64 (C8), (C9), (C26) window_reversal 1-dimensional tensor constant of type boolean (C10) input_batch_dimension constant of type si64 (C11), (C14), (C26) input_feature_dimension constant of type si64 (C12), (C14) input_spatial_dimensions 1-dimensional tensor constant of type si64 (C13), (C14), (C26) kernel_input_feature_dimension constant of type si64 (C15), (C19) kernel_output_feature_dimension constant of type si64 (C16), (C17), (C19), (C26) kernel_spatial_dimensions 1-dimensional tensor constant of type si64 (C18), (C19), (C26) output_batch_dimension constant of type si64 (C21), (C26) output_feature_dimension constant of type si64 (C21), (C26) output_spatial_dimensions 1-dimensional tensor constant of type si64 (C20), (C21), (C26) feature_group_count constant of type si64 (C12), (C15), (C17), (C22), (C24) batch_group_count constant of type si64 (C11), (C16), (C23), (C24), (C26) precision_config variadic number of enum of DEFAULT , HIGH , and HIGHEST (C25) Outputs Name Type Constraints result tensor of any supported type (C26), (C27), (C28) Constraints (C1) \\(N =\\) rank( lhs ) \\(=\\) rank( rhs ). (C2) element_type( lhs ) \\(=\\) element_type( rhs ). (C3) size( window_strides ) \\(= N - 2\\) . (C4) window_strides[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_strides )). (C5) dim( padding , 0) \\(= N - 2\\) and dim( padding , 1) = 2. (C6) size( lhs_dilation ) \\(= N - 2\\) . (C7) lhs_dilation[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( lhs_dilation )). (C8) size( rhs_dilation ) \\(= N - 2\\) . (C9) rhs_dilation[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( rhs_dilation )). (C10) size( window_reversal ) \\(= N - 2\\) . (C11) dim(lhs, input_batch_dimension) % batch_group_count = 0 . (C12) `dim(lhs, input_feature_dimension) % feature_group_count = 0. (C13) size( input_spatial_dimensions ) \\(= N - 2\\) . (C14) Given input_dimensions = [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension] . All dimensions in input_dimensions are unique. For any i \\(\\in\\) input_dimensions , 0 \\(\\le\\) i \\(\\lt\\) N. (C15) dim(rhs, kernel_input_feature_dimension = dim(lhs, input_feature_dimension) / feature_group_count . (C16) dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0 . (C17) dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0 . (C18) size( kernel_spatial_dimensions ) \\(= N - 2\\) . (C19) Given kernel_dimensions = kernel_spatial_dimensions + [kernel_input_feature_dimension] + [kernel_output_feature_dimension] . All dimensions in kernel_dimensions are unique. For any i \\(\\in\\) kernel_dimensions , 0 \\(\\le\\) i \\(\\lt\\) N. (C20) size( output_spatial_dimensions ) \\(= N - 2\\) . (C21) Given output_dimensions = [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension] . All dimensions in output_dimensions are unique. For any i \\(\\in\\) output_dimensions , 0 \\(\\le\\) i \\(\\lt\\) N. (C22) feature_group_count > 0 . (C23) batch_group_count > 0 . (C24) feature_group_count \\(= 1\\) OR batch_group_count \\(= 1\\) . (C25) size( precision_config ) \\(=\\) 2. (C26) For result_dim \\(\\in\\) [0, N), dim(result, result_dim) is given by dim(lhs, input_batch_dimension) / batch_group_count , if result_dim = output_batch_dimension . dim(rhs, kernel_output_feature_dimension) , if result_dim = output_feature_dimension . num_windows otherwise, where: output_spatial_dimensions[spatial_dim] = result_dim . lhs_dim = input_spatial_dimensions[spatial_dim] . rhs_dim = kernel_spatial_dimensions[spatial_dim] . dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) == 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1 . padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1] . dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) == 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1 . num_windows = (padded_input_shape[lhs_dim] == 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]) ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1 . (C27) element_type( result ) \\(=\\) element_type( lhs ). (C28) rank( result ) \\(= N\\) . Examples // %lhs: [[ // [ // [1], [2], [5], [6] // ], // [ // [3], [4], [7], [8] // ], // [ // [10], [11], [14], [15] // ], // [ // [12], [13], [16], [17] // ] // ]] // // %rhs : [ // [[[1]], [[1]], [[1]]], // [[[1]], [[1]], [[1]]], // [[[1]], [[1]], [[1]]] // ] %result = \"stablehlo.convolution\"(%lhs, %rhs) { window_strides = dense<4> : tensor<2xi64>, padding = dense<0> : tensor<2x2xi64>, lhs_dilation = dense<2> : tensor<2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_reversal = dense<false> : tensor<2xi1>, // In the StableHLO dialect, dimension numbers are encoded via: // `[<input dimensions>]x[<kernel dimensions>]->[output dimensions]`. // \"b\" is batch dimenion, \"f\" is feature dimension, // \"i\" is input feature dimension, \"o\" is output feature dimension, // \"0/1/etc\" are spatial dimensions. dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>, feature_group_count = 1 : i64, batch_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>] } : (tensor<1x4x4x1xi32>, tensor<3x3x1x1xi32>) -> tensor<1x2x2x1xi32> // %result: [[ // [[10], [26]], // [[46], [62]] // ]] Back to Ops stablehlo.cosine Semantics Performs element-wise cosine operation on operand tensor and produces a result tensor, implementing the cos operation from the IEEE-754 specification. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [ // [0.0, 1.57079632], // [0, pi/2] // [3.14159265, 4.71238898] // [pi, 3pi/2] // ] %result = \"stablehlo.cosine\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 0.0], [-1.0, 0.0]] More Examples Back to Ops stablehlo.count_leading_zeros Semantics Performs element-wise count of the number of leading zero bits in the operand tensor and produces a result tensor. Inputs Name Type operand tensor of integer type Outputs Name Type result tensor of integer type Constraints (C1) operand and result have the same type. Examples // %operand: [[0, 1], [127, -1]] %result = \"stablehlo.count_leading_zeros\"(%operand) : (tensor<2x2xi8>) -> tensor<2x2xi8> // %result: [[8, 7], [1, 0]] Back to Ops stablehlo.custom_call Semantics Encapsulates an implementation-defined operation call_target_name that takes inputs and called_computations and produces results . has_side_effect , backend_config and api_version may be used to provide additional implementation-defined metadata. Inputs Name Type inputs variadic number of values of any supported type call_target_name constant of type string has_side_effect constant of type i1 backend_config constant of type string api_version constant of type si32 called_computations variadic number of function Outputs Name Type results variadic number of values of any supported type Examples %results = \"stablehlo.custom_call\"(%input0) { call_target_name = \"foo\", has_side_effect = false, backend_config = \"bar\", api_version = 1 : i32, called_computations = [@foo] } : (tensor<f32>) -> tensor<f32> Back to Ops stablehlo.divide Semantics Performs element-wise division of dividend lhs and divisor rhs tensors and produces a result tensor. For floating-point element types, it implements the division operation from IEEE-754 specification. For integer element types, it implements integer division truncating any fractional part. For n-bit integer types, division overflow (division by zero or division of \\(-2^{n-1}\\) with \\(-1\\) ) produces an implementation-defined value. Inputs Name Type lhs tensor of integer, floating-point or complex type rhs tensor of integer, floating-point or complex type Outputs Name Type result tensor of integer, floating-point or complex type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [17.1, -17.1, 17.1, -17.1] // %rhs: [3.0, 3.0, -3.0, -3.0] %result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32> // %result: [5.66666651, -5.66666651, -5.66666651, 5.66666651] // %lhs: [17, -17, 17, -17] // %rhs: [3, 3, -3, -3] %result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32> // %result: [5, -5, -5, 5] Back to Ops stablehlo.dot_general Semantics Computes dot products between slices of lhs and slices of rhs and produces a result tensor. More formally, result[result_index] = dot_product , where: lhs_result_dimensions = [d for d in axes(lhs) and d not in lhs_batching_dimensions and d not in lhs_contracting_dimensions] . rhs_result_dimensions = [d for d in axes(rhs) and d not in rhs_batching_dimensions and d not in rhs_contracting_dimensions] . result_batching_index + result_lhs_index + result_rhs_index = result_index where size(result_batching_index) = size(lhs_batching_dimensions) , size(result_lhs_index) = size(lhs_result_dimensions) and size(result_rhs_index) = size(rhs_result_dimensions) . transposed_lhs = transpose(lhs, lhs_batching_dimensions + lhs_result_dimensions + lhs_contracting_dimensions) . transposed_lhs_slice = slice(result_batching_index + result_lhs_index + [:, ..., :]) . reshaped_lhs_slice = reshape(transposed_lhs_slice, dims(lhs, lhs_contracting_dimensions)) . transposed_rhs = transpose(rhs, rhs_batching_dimensions + rhs_result_dimensions + rhs_contracting_dimensions) . transposed_rhs_slice = slice(result_batching_index + result_rhs_index + [:, ..., :]) . reshaped_rhs_slice = reshape(transposed_rhs_slice, dims(rhs, rhs_contracting_dimensions)) . dot_product = reduce( inputs=[multiply(reshaped_lhs_slice, reshaped_rhs_slice)], init_values=[0], dimensions=[0, ..., size(lhs_contracting_dimensions) - 1], body=lambda x, y: add(x, y)) . precision_config controls the tradeoff between speed and accuracy for computations on accelerator backends. This can be one of the following: DEFAULT : Fastest calculation, but least accurate approximation to the original number. HIGH : Slower calculation, but more accurate approximation to the original number. HIGHEST : Slowest calculation, but most accurate approximation to the original number. Inputs Name Type lhs tensor of any supported type rhs tensor of any supported type lhs_batching_dimensions 1-dimensional tensor constant of type si64 rhs_batching_dimensions 1-dimensional tensor constant of type si64 lhs_contracting_dimensions 1-dimensional tensor constant of type si64 rhs_contracting_dimensions 1-dimensional tensor constant of type si64 precision_config variadic number of enum of DEFAULT , HIGH , and HIGHEST Outputs Name Type result tensor of any supported type Constraints (C1) lhs and rhs have the same element type. (C2) size( lhs_batching_dimensions ) \\(=\\) size( rhs_batching_dimensions ). (C3) size( lhs_contracting_dimensions ) \\(=\\) size( rhs_contracting_dimensions ). (C4) lhs_batching_dimensions and lhs_contracting_dimensions combined are unique. (C5) rhs_batching_dimensions and rhs_contracting_dimensions combined are unique. (C6) 0 \\(\\le\\) lhs_batching_dimensions[i] \\(\\lt\\) rank( lhs ) for all i \\(\\in\\) [0, size( lhs_batching_dimensions )). (C7) 0 \\(\\le\\) lhs_contracting_dimensions[i] \\(\\lt\\) rank( lhs ) for all i \\(\\in\\) [0, size( lhs_contracting_dimensions )). (C8) 0 \\(\\le\\) rhs_batching_dimensions[d] \\(\\lt\\) rank( rhs ) for all i \\(\\in\\) [0, size( rhs_batching_dimensions )). (C9) 0 \\(\\le\\) rhs_contracting_dimensions[d] \\(\\lt\\) rank( rhs ) for all i \\(\\in\\) [0, size( rhs_contracting_dimensions )). (C10) dim( lhs , lhs_batching_dimensions[i] ) \\(=\\) dim( rhs , rhs_batching_dimensions[i] ) for all i \\(\\in\\) [0, size( lhs_batching_dimensions )). (C11) dim( lhs , lhs_contracting_dimensions[i] ) \\(=\\) dim( rhs , rhs_contracting_dimensions[i] ) for all i \\(\\in\\) [0, size( lhs_contracting_dimensions )). (C12) size( precision_config ) \\(=\\) 2. (C13) shape( result ) \\(=\\) dim( lhs , lhs_batching_dimensions ) + dim( lhs , lhs_result_dimensions ) + dim( rhs , rhs_result_dimensions ). Examples // %lhs: [ // [[1, 2], // [3, 4]], // [[5, 6], // [7, 8]] // ] // %rhs: [ // [[1, 0], // [0, 1]], // [[1, 0], // [0, 1]] // ] %result = \"stablehlo.dot_general\"(%lhs, %rhs) { dot_dimension_numbers = #stablehlo.dot< lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1] >, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>] } : (tensor<2x2x2xi32>, tensor<2x2x2xi32>) -> tensor<2x2x2xi32> // %result: [ // [[1, 2], // [3, 4]], // [[5, 6], // [7, 8]] // ] More Examples Back to Ops stablehlo.dynamic_slice Semantics Extracts a slice from the operand using dynamically-computed starting indices and produces a result tensor. start_indices contain the starting indices of the slice for each dimension subject to potential adjustment, and slice_sizes contain the sizes of the slice for each dimension. More formally, result[i0, ..., iR-1] = operand[j0, ..., jR-1] where: jd = adjusted_start_indices[d][] + id . adjusted_start_indices = clamp(0, start_indices, shape(operand) - slice_sizes) . Inputs Name Type operand tensor of any supported type start_indices variadic number of 0-dimensional tensors of integer type slice_sizes 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same element type. (C2) size( start_indices ) \\(=\\) size( slice_sizes ) \\(=\\) rank( operand ). (C3) All start_indices have the same type. (C4) slice_sizes[k] \\(\\in\\) [0, dim( operand , k )) for all k \\(\\in\\) [0, rank( operand )). (C5) shape( result ) \\(=\\) slice_sizes . Examples // %operand: [ // [0, 0, 1, 1], // [0, 0, 1, 1], // [0, 0, 0, 0], // [0, 0, 0, 0] // ] // %start_indices0: -1 // %start_indices1: 3 %result = \"stablehlo.dynamic_slice\"(%operand, %start_indices0, %start_indices1) { slice_sizes = dense<[2, 2]> : tensor<2xi64> } : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32> // %result: [ // [1, 1], // [1, 1] // ] Back to Ops stablehlo.dynamic_update_slice Semantics Produces a result tensor which is equal to the operand tensor except that the slice starting at start_indices is updated with the values in update . More formally, result[i0, ..., iR-1] is defined as: update[j0, ..., jR-1] if jd = adjusted_start_indices[d][] + id where adjusted_start_indices = clamp(0, start_indices, shape(operand) - update) . operand[i0, ..., iR-1] otherwise. Inputs Name Type operand tensor of any supported type update tensor of any supported type start_indices variadic number of 0-dimensional tensors of integer type Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same type. (C2) element_type( update ) \\(=\\) element_type( operand ). (C3) rank( update ) \\(=\\) rank( operand ). (C4) size( start_indices ) \\(=\\) rank( operand ). (C5) All start_indices have the same type. (C6) dim( update , k ) \\(\\in\\) [0, dim( operand , k )] for all k \\(\\in\\) [0, rank( operand )). Examples // %operand: [ // [1, 1, 0, 0], // [1, 1, 0, 0], // [1, 1, 1, 1], // [1, 1, 1, 1] // ] // %update: [ // [1, 1], // [1, 1] // ] // %start_indices0: -1 // %start_indices1: 3 %result = \"stablehlo.dynamic_update_slice\"(%operand, %update, %start_indices0, %start_indices1) : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32> // %result: [ // [1, 1, 1, 1], // [1, 1, 1, 1], // [1, 1, 1, 1], // [1, 1, 1, 1] // ] Back to Ops stablehlo.exponential Semantics Performs element-wise exponential operation on operand tensor and produces a result tensor. For floating-point element types, it implements the exp operation from the IEEE-754 specification. For complex element types, it computes a complex exponential, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [[0.0, 1.0], [2.0, 3.0]] %result = \"stablehlo.exponential\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 2.71828183], [7.38905610, 20.08553692]] // %operand: (1.0, 2.0) %result = \"stablehlo.exponential\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: (-1.13120438, 2.47172667) Back to Ops stablehlo.exponential_minus_one Semantics Performs element-wise exponential minus one operation on operand tensor and produces a result tensor. For floating-point element types, it implements the expm1 operation from the IEEE-754 specification. For complex element types, it computes a complex exponential minus one, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [0.0, 1.0] %result = \"stablehlo.exponential_minus_one\"(%operand) : (tensor<2xf32>) -> tensor<2xf32> // %result: [0.0, 1.71828187] Back to Ops stablehlo.fft Semantics Performs the forward and inverse Fourier transforms for real and complex inputs/outputs. fft_type is one of the following: FFT : Forward complex-to-complex FFT. IFFT : Inverse complex-to-complex FFT. RFFT : Forward real-to-complex FFT. IRFFT : Inverse real-to-complex FFT (i.e. takes complex, returns real). More formally, given the function fft which takes 1-dimensional tensors of complex types as input, produces 1-dimensional tensors of same types as output and computes the discrete Fourier transform: For fft_type = FFT , result is defined as the final result of a series of L computations where L = size(fft_length) . For example, for L = 3 : result1[i0, ..., :] = fft(operand[i0, ..., :]) for all i . result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1]) for all i . Furthermore, given the function ifft which has the same type signature and computes the inverse of fft : For fft_type = IFFT , result is defined as the inverse of the computations for fft_type = FFT . For example, for L = 3 : result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1]) for all i . result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :] = ifft(result2[i0, ..., :]) for all i . Furthermore, given the function rfft which takes 1-dimensional tensors of floating-point types, produces 1-dimensional tensors of complex types of the same floating-point semantics and works as follows: rfft(real_operand) = truncated_result where complex_operand[i] = (real_operand, 0) for all i . complex_result = fft(complex_operand) . truncated_result = complex_result[:(rank(complex_result) / 2 + 1)] . (When the discrete Fourier transform is computed for real operands, the first N/2 + 1 elements of the result unambiguously define the rest of the result, so the result of rfft is truncated to avoid computing redundant elements). For fft_type = RFFT , result is defined as the final result of a series of L computations where L = size(fft_length) . For example, for L = 3 : result1[i0, ..., :] = rfft(operand[i0, ..., :]) for all i . result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1]) for all i . Finally, given the function irfft which has the same type signature and computes the inverse of rfft : For fft_type = IRFFT , result is defined as the inverse of the computations for fft_type = RFFT . For example, for L = 3 : result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1]) for all i . result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :] = irfft(result2[i0, ..., :]) for all i . Inputs Name Type operand tensor of floating-point or complex type fft_type enum of FFT , IFFT , RFFT , and IRFFT fft_length 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of floating-point or complex type Constraints (C1) rank(operand) \\(\\ge\\) size(fft_length) . (C2) The relationship between operand and result element types varies: If fft_type = FFT , element_type(operand) and element_type(result) have the same complex type. If fft_type = IFFT , element_type(operand) and element_type(result) have the same complex type. If fft_type = RFFT , element_type(operand) is a floating-point type and element_type(result) is a complex type of the same floating-point semantics. If fft_type = IRFFT , element_type(operand) is a complex type and element_type(result) is a floating-point type of the same floating-point semantics. (C3) 1 \\(\\le\\) size(fft_length) \\(\\le\\) 3. (C4) If among operand and result , there is a tensor real of a floating-type type, then dims(real)[-size(fft_length):] = fft_length . (C5) dim(result, d) = dim(operand, d) for all d , except for: If fft_type = RFFT , dim(result, -1) = dim(operand, -1) == 0 ? 0 : dim(operand, -1) / 2 + 1 . If fft_type = IRFFT , dim(operand, -1) = dim(result, -1) == 0 ? 0 : dim(result, -1) / 2 + 1 . Examples // %operand: [(1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)] %result = \"stablehlo.fft\"(%operand) { fft_type = #stablehlo<fft_type FFT>, fft_length = dense<4> : tensor<1xi64> } : (tensor<4xcomplex<f32>>) -> tensor<4xcomplex<f32>> // %result: [(1.0, 0.0), (1.0, 0.0), (1.0, 0.0), (1.0, 0.0)] Back to Ops stablehlo.floor Semantics Performs element-wise floor of operand tensor and produces a result tensor. Implements the roundToIntegralTowardNegative operation from the IEEE-754 specification. Inputs Name Type operand tensor of floating-point type Outputs Name Type result tensor of floating-point type Constraints (C1) operand and result have the same type. Examples // %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0] %result = \"stablehlo.floor\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-1.0, -1.0, 0.0, 0.0, 2.0] More Examples Back to Ops stablehlo.gather Semantics Gathers slices from operand tensor from offsets specified in start_indices and produces a result tensor. The following diagram shows how elements in result map on elements in operand using a concrete example. The diagram picks a few example result indices and explains in detail which operand indices they correspond to. More formally, result[result_index] = operand[operand_index] where: batch_dims = [ d for d in axes(result) and d not in offset_dims ]. batch_index = [ result_index[d] for d in batch_dims ]. start_index = start_indices[bi0, ..., :, ..., biN] where bi are individual elements in batch_index and : is inserted at the index_vector_dim index, if index_vector_dim < rank(start_indices) . [start_indices[batch_index]] otherwise. For do in axes(operand) , full_start_index[do] = start_index[ds] if do = start_index_map[ds] . full_start_index[do] = 0 otherwise. offset_index = [ result_index[d] for d in offset_dims ]. full_offset_index = [oi0, ..., 0, ..., oiN] where oi are individual elements in offset_index , and 0 is inserted at indices from collapsed_slice_dims . operand_index = add(full_start_index, full_offset_index) . If operand_index is out of bounds for operand , then the behavior is implementation-defined. If indices_are_sorted is true then the implementation can assume that start_indices are sorted with respect to start_index_map , otherwise the behavior is undefined. More formally, for all id < jd from indices(result) , full_start_index(id) <= full_start_index(jd) . Inputs Name Type Constraints operand tensor of any supported type (C1), (C10), (C11), (C12), (C15) start_indices tensor of any supported integer type (C2), (C3), (C13) offset_dims 1-dimensional tensor constant of type si64 (C1), (C4), (C5), collapsed_slice_dims 1-dimensional tensor constant of type si64 (C1), (C6), (C7), (C8), (C13) start_index_map 1-dimensional tensor constant of type si64 (C3), (C9), (C10) index_vector_dim constant of type si64 (C2), (C3), (C13) slice_sizes 1-dimensional tensor constant of type si64 (C7), (C8), (C11), (C12), (C13) indices_are_sorted constant of type i1 Outputs Name Type result tensor of any supported type Constraints (C1) rank( operand ) \\(=\\) size( offset_dims ) \\(+\\) size( collapsed_slice_dims ). (C2) \\(0 \\le\\) index_vector_dim \\(\\le\\) rank( start_indices ). (C3) size( start_index_map ) \\(=\\) index_vector_dim \\(\\lt\\) rank( start_indices ) ? dim( start_indices , index_vector_dim ) : 1. (C4) All dimensions in offset_dims are unique and sorted in ascending order. (C5) \\(0 \\le\\) offset_dims [i] \\(\\lt\\) rank( result ) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( offset_dims ). (C6) All dimensions in collapsed_slice_dims are unique and sorted in ascending order. (C7) \\(0 \\le\\) collapsed_slice_dims [i] \\(\\lt\\) size( slice_sizes ) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( collapsed_slice_dims ). (C8) slice_sizes [i] \\(\\le\\) 1 \\(\\forall i \\in\\) collapsed_slice_dims . (C9) All dimensions in start_index_map are unique. (C10) \\(0 \\le\\) start_index_map [i] \\(\\lt\\) rank( operand ) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( start_index_map ). (C11) size( slice_sizes ) \\(=\\) rank( operand ). (C12) \\(0 \\le\\) slice_sizes [i] \\(\\le\\) dim( operand , i) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( slice_sizes ). (C13) shape(result) \\(=\\) combine(batch_dim_sizes, offset_dim_sizes) where: batch_dim_sizes = shape(start_indices) except that the dimension size of start_indices corresponding to index_vector_dim is not included. offset_dim_sizes = shape(slice_sizes) except that the dimension sizes in slice_sizes corresponding to collapsed_slice_dims are not included. combine puts batch_dim_sizes at axes corresponding to batch_dims and offset_dim_sizes at axes corresponding to offset_dims . (C15) operand and result have the same element type. Examples // %operand: [ // [[1, 2], [3, 4], [5, 6], [7, 8]], // [[9, 10],[11, 12], [13, 14], [15, 16]], // [[17, 18], [19, 20], [21, 22], [23, 24]] // ] // %start_indices: [ // [[0, 0], [1, 0], [2, 1]], // [[0, 1], [1, 1], [0, 2]] // ] %result = \"stablehlo.gather\"(%operand, %start_indices) { dimension_numbers = #stablehlo.gather< offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [1, 0], index_vector_dim = 2>, slice_sizes = dense<[1, 2, 2]> : tensor<3xi64>, indices_are_sorted = false } : (tensor<3x4x2xi32>, tensor<2x3x2xi64>) -> tensor<2x3x2x2xi32> // %result: [ // [ // [[1, 2], [3, 4]], // [[3, 4], [5, 6]], // [[13, 14], [15, 16]] // ], // [ // [[9, 10], [11, 12]], // [[11, 12], [13, 14]], // [[17, 18], [19, 20]] // ] // ] Back to Ops stablehlo.get_tuple_element Semantics Extracts element at index position of the operand tuple and produces a result . Inputs Name Type operand tuple index constant of type si32 Outputs Name Type result any supported type Constraints (C1) 0 \\(\\le\\) index \\(\\lt\\) size( operand ). (C2) type( operand[index] ) \\(=\\) type( result ). Examples // %operand: ([1.0, 2.0], (3)) %result = \"stablehlo.get_tuple_element\"(%operand) { index = 0 : i32 } : (tuple<tensor<2xf32>, tuple<tensor<i32>>>) -> tensor<2xf32> // %result: [1.0, 2.0] Back to Ops stablehlo.if Semantics Produces the output from executing exactly one function from true_branch or false_branch depending on the value of pred . Formally, if pred is true , output of true_branch is returned, else if pred is false , output of false_branch is returned. Inputs Name Type pred 1-dimensional tensor constant of type i1 true_branch function false_branch function Outputs Name Type results variadic number of tensors of any supported type or tokens Constraints (C1) true_branch and false_branch have 0 inputs. (C2) true_branch and false_branch have the same output types. (C3) For all i , type(results[i]) = type(true_branch).outputs[i] . Examples // %result_true_branch: 10 // %result_false_branch: 11 // %pred: true %result = \"stablehlo.if\"(%pred) ({ \"stablehlo.return\"(%result_true_branch) : (tensor<i32>) -> () }, { \"stablehlo.return\"(%result_false_branch) : (tensor<i32>) -> () }) : (tensor<i1>) -> tensor<i32> // %result: 10 Back to Ops stablehlo.imag Semantics Extracts the imaginary part, element-wise, from the operand and produces a result tensor. More formally, for each element x : imag(x) = is_complex(x) ? x.imag : 0.0 . Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point type Constraints (C1) shape( result ) = shape( operand ). (C2) element_type( result ) \\(=\\) element_type( operand ) if it's a floating-point type. real_type(element_type( operand )) otherwise. Examples // %operand: [(1.0, 2.0), (3.0, 4.0)] %result = \"stablehlo.imag\"(%operand) : (tensor<2xcomplex<f32>>) -> tensor<2xf32> // %result: [2.0, 4.0] Back to Ops stablehlo.infeed Semantics Reads data from the infeed and produces results . Semantics of infeed_config is implementation-defined. results consist of payload values which come first and a token which comes last. The operation produces a token to reify the side effect of this operation as a value that other operations can take a data dependency on. Inputs Name Type token token infeed_config constant of type string Outputs Name Type results variadic number of tensors of any supported type or tokens Constraints (C1) size( results ) \\(\\ge\\) 1. (C2) type( results [-1]) \\(=\\) token . -- Verify layout in InfeedOp -- Examples %results:2 = \"stablehlo.infeed\"(%token) { infeed_config = \"\" } : (!stablehlo.token) -> (tensor<3x3x3xi32>, !stablehlo.token) Back to Ops stablehlo.iota Semantics Fills an output tensor with values in increasing order starting from zero along the iota_dimension dimension. More formally, output[i0, ..., id, ..., iR-1] = id , where d is equal to iota_dimension . For integers, if the dimension size is larger than what the element type's maximum value can hold, an overflow occurs and the behavior is implementation- defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) for signed overflow and saturation to \\(2^n - 1\\) for unsigned overflow. Inputs Name Type iota_dimension si64 Outputs Name Type output tensor of integer, floating-point or complex type Constraints (C1) 0 \\(\\le\\) iota_dimension \\(\\lt\\) R , where R is the rank of the output . Examples %output = \"stablehlo.iota\"() { iota_dimension = 0 : i64 } : () -> tensor<4x5xi32> // %output: [ // [0, 0, 0, 0, 0], // [1, 1, 1, 1, 1], // [2, 2, 2, 2, 2], // [3, 3, 3, 3, 3] // ] %output = \"stablehlo.iota\"() { iota_dimension = 1 : i64 } : () -> tensor<4x5xi32> // %output: [ // [0, 1, 2, 3, 4], // [0, 1, 2, 3, 4], // [0, 1, 2, 3, 4], // [0, 1, 2, 3, 4] // ] More Examples Back to Ops stablehlo.is_finite Semantics Performs element-wise check whether the value in x is finite (i.e. is neither +Inf, -Inf, nor NaN) and produces a y tensor. Implements the isFinite operation from the IEEE-754 specification. Inputs Name Type x tensor of floating-point type Outputs Name Type y tensor of boolean type Constraints (C1) x and y have the same shape. Examples // Logical values: -Inf, +Inf, NaN, ... // %x: [0xFF800000, 0x7F800000, 0x7FFFFFFF, -10.0, -0.0, 0.0, 10.0] %y = \"stablehlo.is_finite\"(%x) : (tensor<7xf32>) -> tensor<7xi1> // %y: [false, false, false, true, true, true, true] Back to Ops stablehlo.log Semantics Performs element-wise logarithm operation on operand tensor and produces a result tensor. For floating-point element types, it implements the log operation from the IEEE-754 specification. For complex element types, it computes a complex logarithm, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [[1.0, 2.0], [3.0, 4.0]] %result = \"stablehlo.log\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 0.69314718], [1.09861229, 1.38629436]] // %operand: (1.0, 2.0) %result = \"stablehlo.log\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: (0.80471896, 1.10714871) Back to Ops stablehlo.log_plus_one Semantics Performs element-wise log plus one operation on operand tensor and produces a result tensor. For floating-point element types, it implements the logp1 operation from the IEEE-754 specification. For complex element types, it computes a complex log plus one, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [-2.0, -0.0, -0.999, 7.0, 6.38905621, 15.0] %result = \"stablehlo.log_plus_one\"(%operand) : (tensor<6xf32>) -> tensor<6xf32> // %result: [-nan, 0.0, -6.90776825, 2.07944155, 2.0, 2.77258873] Back to Ops stablehlo.logistic Semantics Performs element-wise logistic (sigmoid) function on operand tensor and produces a result tensor. For floating-point element types, it implements: \\( \\(logistic(x) = division(1, addition(1, exp(-x)))\\) \\) where addition , division , and exp are operations from IEEE-754 specification. For complex element types, it computes a complex logistic function, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [[0.0, 1.0], [2.0, 3.0]] %result = \"stablehlo.logistic\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.5, 0.73105858], [0.88079708, 0.95257413]] // %operand: (1.0, 2.0) %result = \"stablehlo.logistic\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: (1.02141536, 0.40343871) Back to Ops stablehlo.map Semantics Applies a map function computation to inputs along the dimensions and produces a result tensor. More formally, result[i0, ..., iR-1] = computation(inputs[0][i0, ..., iR-1], ..., inputs[N-1][i0, ..., iR-1]) . Inputs Name Type inputs variadic number of tensors of any supported type dimensions 1-dimensional tensor constant of type si64 computation function Outputs Name Type result tensor of any supported type Constraints (C1) All inputs and result have the same shape. (C2) size( inputs ) \\(=\\) N \\(\\ge\\) 1. (C3) dimensions = [0, ..., R-1] , where R \\(=\\) rank( inputs[0] ). (C4) computation has type (tensor<E0>, ..., tensor<EN-1>) -> tensor<E'> where Ek \\(=\\) element_type( inputs[k] ) and E' \\(=\\) element_type( result ). Examples // %input0: [[0, 1], [2, 3]] // %input1: [[4, 5], [6, 7]] %result = \"stablehlo.map\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = stablehlo.multiply %arg0, %arg1 : tensor<i32> stablehlo.return %0 : tensor<i32> }) { dimensions = dense<[0, 1]> : tensor<2xi64> } : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[0, 5], [12, 21]] Back to Ops stablehlo.maximum Semantics Performs element-wise max operation on tensors lhs and rhs and produces a result tensor. For floating-point element types, it implements the maximum operation from the IEEE-754 specification. For complex element types, it performs lexicographic comparison on the (real, imaginary) pairs with corner cases TBD. For boolean element type, the behavior is same as stablehlo.or . Inputs Name Type lhs tensor of any supported type rhs tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [[1, 2], [7, 8]] // %rhs: [[5, 6], [3, 4]] %result = \"stablehlo.maximum\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 6], [7, 8]] More Examples Back to Ops stablehlo.minimum Semantics Performs element-wise min operation on tensors lhs and rhs and produces a result tensor. For floating-point element types, it implements the minimum operation from the IEEE-754 specification. For complex element types, it performs lexicographic comparison on the (real, imaginary) pairs with corner cases TBD. For boolean element type, the behavior is same as stablehlo.and . Inputs Name Type lhs tensor of any supported type rhs tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [[1, 2], [7, 8]] // %rhs: [[5, 6], [3, 4]] %result = \"stablehlo.minimum\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[1, 2], [3, 4]] More Examples Back to Ops stablehlo.multiply Semantics Performs element-wise product of two tensors lhs and rhs and produces a result tensor. For integer element types, if the element-wise product has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ \\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For floating-point element types, it implements the multiplication operation from the IEEE-754 specification. For complex element types, it computes a complex multiplication, with corner cases TBD. For boolean element type, the behavior is same as stablehlo.and . Inputs Name Type lhs tensor of any supported type rhs tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.multiply\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 12], [21, 32]] More Examples Back to Ops stablehlo.negate Semantics Performs element-wise negation of operand tensor and produces a result tensor. For floating-point element types, it implements the negate operation from the IEEE-754 specification. For signed integer types, it performs the regular negation operation where the negation of \\(-2^{n-1}\\) is implementation- defined and one of the following: Saturation to \\(2^{n-1}-1\\) \\(-2^{n-1}\\) For unsigned integer types, it bitcasts to the corresponding signed integer type, performs the regular negation operation and bitcasts back to the original unsigned integer type. Inputs Name Type operand tensor of integer, floating-point, or complex type Outputs Name Type result tensor of integer, floating-point, or complex type Constraints (C1) operand and result have the same type. Examples // Negation operation with integer Tensors // %operand: [0, -2] %result = \"stablehlo.negate\"(%operand) : (tensor<2xi32>) -> tensor<2xi32> // %result: [0, 2] // Negation operation with with complex tensors // %operand: (2.5, 0.0) %result = \"stablehlo.negate\"(%operand) : (tensor<1xcomplex<f32>>) -> tensor<1xcomplex<f32>> // %result: [-2.5, -0.0] More Examples Back to Ops stablehlo.not Semantics Performs element-wise bitwise NOT of tensor operand of type integer and produces a result tensor. For boolean tensors, it computes the logical NOT. Arguments Name Type operand tensor of integer or boolean type Outputs Name Type result tensor of integer or boolean type Constraints (C1) operand and result have the same type. Examples // Bitwise operation with with integer tensors // %operand: [[1, 2], [3, 4]] %result = \"stablehlo.not\"(%operand) : (tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[-2, -3], [-4, -5]] // Bitwise operation with with boolean tensors // %operand: [true, false] %result = \"stablehlo.not\"(%operand) : (tensor<2xi1>) -> tensor<2xi1> // %result: [false, true] Back to Ops stablehlo.optimization_barrier Semantics Ensures that the operations that produce the operand are executed before any operations that depend on the result and prevents compiler transformations from moving operations across the barrier. Other than that, the operation is an identity, i.e. result = operand . Arguments Name Type operand variadic number of tensors of any supported type or tokens Outputs Name Type result variadic number of tensors of any supported type or tokens Constraints (C1) size( operand ) \\(=\\) size( result ). (C2) type( operand[i] ) \\(=\\) type( result[i] ) for all i. Examples // %operand0: 0.0 // %operand1: 1.0 %result0, %result1 = \"stablehlo.optimization_barrier\"(%operand0, %operand1) : (tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>) // %result0: 0.0 // %result1: 1.0 Back to Ops stablehlo.or Semantics Performs element-wise bitwise OR of two tensors lhs and rhs of integer types and produces a result tensor. For boolean tensors, it computes the logical operation. Inputs Name Type lhs tensor of integer or boolean type rhs tensor of integer or boolean type Outputs Name Type result tensor of integer or boolean type Constraints (C1) operand and result have the same type. Examples // Bitwise operation with with integer tensors // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.or\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 6], [7, 12]] // Logical operation with with boolean tensors // %lhs: [[false, false], [true, true]] // %rhs: [[false, true], [false, true]] %result = \"stablehlo.or\"(%lhs, %rhs) : (tensor<2x2xi1>, tensor<2x2xi1>) -> tensor<2x2xi1> // %result: [[false, true], [true, true]] Back to Ops stablehlo.outfeed Semantics Writes inputs to the outfeed and produces a result token. Semantics of outfeed_config is implementation-defined. The operation takes a token and produces a token to reify its side effects as a value that other operations can take a data dependency on. Inputs Name Type inputs variadic number of tensors of any supported type token token outfeed_config constant of type string Outputs Name Type result token Examples %result = \"stablehlo.outfeed\"(%input0, %token) { outfeed_config = \"\" } : (tensor<3x3x3xi32>, !stablehlo.token) -> !stablehlo.token Back to Ops stablehlo.pad Semantics Expands operand by padding around the tensor as well as between the elements of the tensor with the given padding_value . edge_padding_low and edge_padding_high specify the amount of padding added at the low-end (next to index 0) and the high-end (next to the highest index) of each dimension respectively. The amount of padding can be negative, where the absolute value of negative padding indicates the number of elements to remove from the specified dimension. interior_padding specifies the amount of padding added between any two elements in each dimension which may not be negative. Interior padding occurs before edge padding such that negative edge padding will remove elements from the interior-padded operand. More formally, result[i0, ..., iR-1] is equal to: operand[j0, ..., jR-1] if id = edge_padding_low[d] + jd * (interior_padding[d] + 1) . padding_value[] otherwise. Inputs Name Type operand tensor of any supported type padding_value 0-dimensional tensor of any supported type edge_padding_low 1-dimensional tensor constant of type si64 edge_padding_high 1-dimensional tensor constant of type si64 interior_padding 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) operand , padding_value , result have the same element type. (C2) edge_padding_low , edge_padding_high , interior_padding have the size equal to operand 's rank. (C3) 0 \\(\\le\\) interior_padding[i] for all i values in interior_padding . (C4) 0 \\(\\le\\) dim(result, i) for all i th dimension of operand , where dim(result, i) = di + max(di - 1, 0) * interior_padding[i] + edge_padding_low[i] + edge_padding_high[i] and di = dim(operand, i) . Examples // %operand: [ // [1, 2, 3], // [4, 5, 6] // ] // %padding_value: 0 %result = \"stablehlo.pad\"(%operand, %padding_value) { edge_padding_low = dense<[0, 1]> : tensor<2xi64>, edge_padding_high = dense<[2, 1]> : tensor<2xi64>, interior_padding = dense<[1, 2]> : tensor<2xi64> } : (tensor<2x3xi32>, tensor<i32>) -> tensor<5x9xi32> // %result: [ // [0, 1, 0, 0, 2, 0, 0, 3, 0], // [0, 0, 0, 0, 0, 0, 0, 0, 0], // [0, 4, 0, 0, 5, 0, 0, 6, 0], // [0, 0, 0, 0, 0, 0, 0, 0, 0], // [0, 0, 0, 0, 0, 0, 0, 0, 0] // ] Back to Ops stablehlo.partition_id Semantics Produces partition_id of the current process. Outputs Name Type result 0-dimensional tensor of type ui32 Examples %result = \"stablehlo.partition_id\"() : () -> tensor<ui32> Back to Ops stablehlo.popcnt Semantics Performs element-wise count of the number of bits set in the operand tensor and produces a result tensor. Inputs Name Type operand tensor of integer type Outputs Name Type result tensor of integer type Constraints (C1) operand and result have the same type. Examples // %operand: [0, 1, 2, 127] %result = \"stablehlo.popcnt\"(%operand) : (tensor<4xi8>) -> tensor<4xi8> // %result: [0, 1, 1, 7] Back to Ops stablehlo.power Semantics Performs element-wise exponentiation of lhs tensor by rhs tensor and produces a result tensor. For integer element types, if the exponentiation has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For an integer, x , raised to a negative power, y , the behaviour is as follows: If abs(x) \\(\\gt\\) 1, then result is 0. If abs(x) \\(=\\) 1, then result is equivalent to x^abs(y) . If abs(x) \\(=\\) 0, then behaviour is implementation-defined. For floating-point element types, it implements the pow operation from the IEEE-754 specification. For complex element types, it computes complex exponentiation, with corner cases TBD. Numeric precision is implementation-defined. Inputs Name Type lhs tensor of integer, floating-point, or complex type rhs tensor of integer, floating-point, or complex type Outputs Name Type result tensor of integer, floating-point, or complex type Constraints (C1) lhs , rhs , and result have the same type. Examples // %lhs: [-2.0, -0.0, -36.0, 5.0, 3.0, 10000.0] // %rhs: [2.0, 2.0, 1.1, 2.0, -1.0, 10.0] %result = \"stablehlo.power\"(%lhs, %rhs) : (tensor<6xf32>, tensor<6xf32>) -> tensor<6xf32> // %result: [4.0, 0.0, -nan, 25.0, 0.333333343, inf] Back to Ops stablehlo.real Semantics Extracts the real part, element-wise, from the operand and produces a result tensor. More formally, for each element x : real(x) = is_complex(x) ? x.real : x . Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point type Constraints (C1) shape( result ) = shape( operand ). (C2) element_type( result ) \\(=\\) element_type( operand ) if it's a floating-point type. real_type(element_type( operand )) otherwise. Examples // %operand: [(1.0, 2.0), (3.0, 4.0)] %result = \"stablehlo.real\"(%operand) : (tensor<2xcomplex<f32>>) -> tensor<2xf32> // %result: [1.0, 3.0] Back to Ops stablehlo.recv Semantics Receives data from a channel with channel_id and produces results . If is_host_transfer is true , then the operation transfers data from the host. Otherwise, it transfers data from another device. What this means is implementation-defined. results consist of payload values which come first and a token which comes last. The operation produces a token to reify its side effects as a value that other operations can take a data dependency on. Inputs Name Type token token channel_id constant of type si64 channel_type enum of DEVICE_TO_DEVICE and HOST_TO_DEVICE is_host_transfer constant of type i1 Outputs Name Type results variadic number of tensors of any supported type or token Constraints (C1) todo channel_type must be HOST_TO_DEVICE , if is_host_transfer \\(=\\) true , DEVICE_TO_DEVICE , otherwise. (C2) size( results ) \\(\\ge\\) 1. (C3) type( results [-1]) \\(=\\) token . Examples %results:2 = \"stablehlo.recv\"(%token) { // channel_id = 5 : i64, // channel_type = #stablehlo<channel_type HOST_TO_DEVICE>, channel_handle = #stablehlo.channel_handle<handle = 5, type = 3>, is_host_transfer = true } : (!stablehlo.token) -> (tensor<3x4xi32>, !stablehlo.token) Back to Ops stablehlo.reduce Semantics Applies a reduction function body to inputs and init_values along the dimensions and produces a result tensor. The order of reductions is implementation-defined, which means that body and init_values must form a monoid to guarantee that the operation produces the same results for all inputs on all implementations. However, this condition doesn't hold for many popular reductions. E.g. floating-point addition for body and zero for init_values don't actually form a monoid because floating-point addition is not associative. What this means for numeric precision is implementation-defined. More formally, results[:][j0, ..., jR-1] = reduce(input_slices) where: input_slices = inputs[:][j0, ..., :, ..., jR-1] , where : are inserted at dimensions . reduce(input_slices) = exec(schedule) for some binary tree schedule where: exec(node) = body(exec(node.left), exec(node.right)) . exec(leaf) = leaf.value . schedule is an implementation-defined full binary tree whose in-order traversal consists of: input_slices[:][index] values, for all index in the index space of input_slices , in the ascending lexicographic order of index . Interspersed with an implementation-defined amount of init_values at implementation-defined positions. Inputs Name Type inputs variadic number of tensors of any supported type init_values variadic number of 0-dimensional tensors of any supported type dimensions 1-dimensional tensor constant of type si64 body function Outputs Name Type results variadic number of tensors of any supported type Constraints (C1) All inputs have the same shape. (C2) element_type( inputs[k] ) \\(=\\) element_type( init_values[k] ) \\(=\\) element_type( results[k] ) for all k \\(\\in\\) [0, N). (C3) size( inputs ) \\(=\\) size( init_values ) \\(=\\) size( results ) \\(=\\) N where N >= 1. (C4) 0 \\(\\le\\) dimensions[d] \\(\\lt\\) rank( inputs[0][d] ) for all dimension d . (C5) All dimensions in dimensions are unique. (C6) body has type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) where Ek = element_type(inputs[k]) . (C7) shape( results[k] ) \\(=\\) shape( inputs[k] ) except that the dimension sizes of inputs[k] corresponding to dimensions are not included. Examples // %input = [[0, 1, 2, 3, 4, 5]] // %init_value = 0 %result = \"stablehlo.reduce\"(%input, %init_value) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) { dimensions = dense<1> : tensor<1xi64> } : (tensor<1x6xi32>, tensor<i32>) -> tensor<1xi32> // %result = [15] Back to Ops stablehlo.reduce_precision Semantics Performs element-wise conversion of operand to another floating-point type that uses exponent_bits and mantissa_bits and back to the original floating-point type and produces a result tensor. More formally: * The mantissa bits of the original value are updated to round the original value to the nearest value representable with mantissa_bits using roundToIntegralTiesToEven semantics. * Then, if mantissa_bits are smaller than the number of mantissa bits of the original value, the mantissa bits are truncated to mantissa_bits . * Then, if the exponent bits of the intermediate result don't fit into the range provided by exponent_bits , the intermediate result overflows to infinity using the original sign or underflows to zero using the original sign. Inputs Name Type operand tensor of floating-point type exponent_bits constant of type si32 mantissa_bits constant of type si32 Outputs Name Type result tensor of floating-point type Constraints (C1) operand and result have the same type. (C2) exponent_bits \\(\\ge\\) 1. (C3) mantissa_bits \\(\\ge\\) 0. Examples // Logical values: -Inf, +Inf, NaN, ... // %operand: [0xFF800000, 0x7F800000, 0x7FFFFFFF, 0.0, 1000.0, 1000000.0] %result = \"stablehlo.reduce_precision\"(%operand) { exponent_bits = 5 : i32, mantissa_bits = 2 : i32 } : (tensor<6xf32>) -> tensor<6xf32> // Logical values: -Inf, +Inf, NaN, NaN, 0.0, 1024.0, +Inf // %result: [0xFF800000, 0x7F800000, 0x7FFFFFFF, 0.0, 1024.0, 0x7F800000] Back to Ops stablehlo.reduce_scatter Semantics Within each process group in the StableHLO grid, performs reduction, using computations , over the values of the operand tensor from each process, splits the reduction result along scatter_dimension into parts, and scatters the split parts between the processes to produce the result . The operation splits the StableHLO grid into process_groups as follows: channel_id <= 0 and use_global_device_ids = false , cross_replica(replica_groups) . channel_id > 0 and use_global_device_ids = false , cross_replica_and_partition(replica_groups) . channel_id > 0 and use_global_device_ids = true , flattened_ids(replica_groups) . Afterwards, within each process_group : reduced_value = all_reduce(operand, replica_groups, channel_id, use_global_device_ids, computation) . parts@sender = split(reduced_value@sender, dim(process_groups, 1), split_dimension) . result@receiver = parts@sender[receiver_index] for any sender in process_group, where receiver_index = index_of(receiver, process_group) . Inputs Name Type operand tensor of any supported type scatter_dimension constant of type si64 replica_groups 2-dimensional tensor constant of type si64 channel_id constant of type si64 use_global_device_ids constant of type boolean computation function Outputs Name Type result tensor of any supported type Constraints (C1) dim( operand , scatter_dimension ) % dim( process_groups , 1) \\(=\\) 0. (C2) scatter_dimension \\(\\in\\) [0, rank( operand )). (C3) All values in replica_groups are unique. (C4) size(replica_groups) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_replica_and_partition , num_replicas . If flattened_ids , num_processes . (C5) \\(0 \\le\\) replica_groups[i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C6) If use_global_device_ids = true , then channel_id > 0 . todo (C7) computation has type (tensor<E>, tensor<E>) -> (tensor<E>) where E = element_type(operand) . (C8) type(result) = type(operand) except: dim(result, scatter_dimension) = dim(operand, scatter_dimension) / dim(process_groups, 1) . Examples // num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [ // [1.0, 2.0, 3.0, 4.0], // [5.0, 6.0, 7.0, 8.0] // ] // %operand@(1, 0): [ // [9.0, 10.0, 11.0, 12.0], // [13.0, 14.0, 15.0, 16.0] // ] %result = \"stablehlo.reduce_scatter\"(%operand) ({ ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32> \"stablehlo.return\"(%0) : (tensor<f32>) -> () }) { scatter_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> // use_global_device_ids = false } : (tensor<2x4xf32>) -> tensor<2x2xf32> // // %result@(0, 0): [ // [10.0, 12.0], // [18.0, 20.0] // ] // %result@(1, 0): [ // [14.0, 16.0], // [22.0, 24.0] // ] Back to Ops stablehlo.reduce_window Semantics Applies a reduction function body to windows of inputs and init_values and produces results . The following diagram shows how elements in results[k] are computed from inputs[k] using a concrete example. More formally, results[:][result_index] = reduce(windows, init_values, axes(inputs[:]), body) where: padded_inputs = pad(inputs[:], init_values[:], padding[:, 0], padding[:, 1], base_dilations) . window_start = result_index * window_strides . windows = slice(padded_inputs[:], window_start, window_start + window_dimensions, window_dilations) . Inputs Name Type Constraints inputs variadic number of tensors of any supported type (C1-C4), (C6), (C8), (C10), (C12), (C13), (C15) init_values variadic number of 0-dimensional tensors of any supported type (C1), (C13), (C16) window_dimensions 1-dimensional tensor constant of type si64 (C4), (C5), (C15) window_strides 1-dimensional tensor constant of type si64 (C6), (C7), (C15) base_dilations 1-dimensional tensor constant of type si64 (C8), (C9), (C15) window_dilations 1-dimensional tensor constant of type si64 (C10), (C11), (C15) padding 2-dimensional tensor constant of type si64 (C12), (C15) body function (C13) Outputs Name Type Constraints results variadic number of tensors of any supported type (C1), (C14-C16) Constraints (C1) size( inputs ) \\(=\\) size( init_values ) \\(=\\) size( results ) \\(=\\) N and N \\(\\ge\\) 1. (C2) All inputs have the same shape. (C3) element_type(inputs[k]) = element_type(init_values[k]) for any k \\(\\in\\) [0, N). (C4) size( window_dimensions ) \\(=\\) rank( inputs[0] ). (C5) window_dimensions[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_dimensions )). (C6) size( window_strides ) \\(=\\) rank( inputs[0] ). (C7) window_strides[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_strides )). (C8) size( base_dilations ) \\(=\\) rank( inputs[0] ). (C9) base_dilations[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( base_dilations )). (C10) size( window_dilations ) \\(=\\) rank( inputs[0] ). (C11) window_dilations[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_dilations )). (C12) dim( padding , 0) \\(=\\) rank( inputs[0] ) and dim( padding , 1) = 2. (C13) body has type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) where Ek = element_type(inputs[0]) . (C14) All results have the same shape. (C15) shape(results[0]) = num_windows dilated_input_shape = shape(inputs[0]) == 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1 . padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1] . dilated_window_shape = window_dimensions == 0 ? 0 : (window_dimensions - 1) * window_dilations + 1 . num_windows = (padded_input_shape == 0 || dilated_window_shape > padded_input_shape) ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1 . (C16) element_type(results[k]) = element_type(init_values[k]) for any k \\(\\in\\) [0, N). Examples // %input = [[1, 2], [3, 4], [5, 6]] // %init_value = 0 %result = \"stablehlo.reduce_window\"(%input, %init_value) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) { window_dimensions = dense<[2, 1]> : tensor<2xi64>, window_strides = dense<[4, 1]> : tensor<2xi64>, base_dilations = dense<[2, 1]> : tensor<2xi64>, window_dilations = dense<[3, 1]> : tensor<2xi64>, padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<3x2xi32>, tensor<i32>) -> tensor<2x2xi32> // %result = [[0, 0], [3, 4]] Back to Ops stablehlo.remainder Semantics Performs element-wise remainder of dividend lhs and divisor rhs tensors and produces a result tensor. The sign of the result is taken from the dividend, and the absolute value of the result is always less than the divisor's absolute value. The remainder is calculated as lhs - d * rhs , where d = stablehlo.divide . For floating-point element types, this is in contrast with the remainder operation from IEEE-754 specification where d is an integral value nearest to the exact value of lhs/rhs with ties to even. For floating-point types, the corner cases are TBD. For n-bit integer, division overflow (remainder by zero or remainder of \\(-2^{n-1}\\) with \\(-1\\) ) produces an implementation-defined value. Inputs Name Type lhs tensor of integer, floating-point or complex type rhs tensor of integer, floating-point or complex type Outputs Name Type result tensor of integer, floating-point or complex type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [17.1, -17.1, 17.1, -17.1] // %rhs: [3.0, 3.0, -3.0, -3.0] %result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32> // %result: [2.1, -2.1, 2.1, -2.1] // %lhs: [17, -17, 17, -17] // %rhs: [3, 3, -3, -3] %result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32> // %result: [2, -2, 2, -2] Back to Ops stablehlo.replica_id Semantics Produces replica_id of the current process. Outputs Name Type result 0-dimensional tensor of type ui32 Examples %result = \"stablehlo.replica_id\"() : () -> tensor<ui32> Back to Ops stablehlo.reshape Semantics Performs reshape of operand tensor to a result tensor. Conceptually, it amounts to keeping the same canonical representation but potentially changing the shape, e.g. from tensor<2x3xf32> to tensor<3x2xf32> or tensor<6xf32> . More formally, result[i0, ..., iR-1] = operand[j0, ..., jR'-1] where i and j have the same position in the lexicographic ordering of the index spaces of result and operand . Inputs Name Type operand tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same element type. (C2) operand and result have the same number of elements. Examples // %operand: [[1, 2, 3], [4, 5, 6]]] %result = \"stablehlo.reshape\"(%operand) : (tensor<2x3xi32>) -> tensor<3x2xi32> // %result: [[1, 2], [3, 4], [5, 6]] More Examples Back to Ops stablehlo.reverse Semantics Reverses the order of elements in the operand along the specified dimensions and produces a result tensor. More formally, result[i0, ..., ik,..., iR-1] = operand[i0, ..., ik',..., iR-1] where ik + ik' = dk - 1 for all dimensions k in dimensions . Inputs Name Type operand tensor of any supported type dimensions 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same type. (C2) All dimensions in dimensions are unique. (C3) For all dimensions k in dimensions , 0 \\(\\le\\) dimensions[k] \\(\\lt\\) R , where R is the rank of the result . Examples // Reverse along dimension 0 // %operand = [[1, 2], [3, 4], [5, 6]] %result = \"stablehlo.reverse\"(%operand) { dimensions = dense<0> : tensor<i64> } : (tensor<3x2xi32>) -> tensor<3x2xi32> // %result: [[5, 6], [3, 4], [1, 2]] // Reverse along dimension 1 // %operand = [[1, 2], [3, 4], [5, 6]] %result = \"stablehlo.reverse\"(%operand) { dimensions = dense<1> : tensor<i64> } : (tensor<3x2xi32>) -> tensor<3x2xi32> // %result: [[2, 1], [4, 3], [6, 5]] Back to Ops stablehlo.rng Semantics Generates random numbers using the rng_distribution algorithm and produces a result tensor of a given shape shape . If rng_distribution \\(=\\) UNIFORM , then the random numbers are generated following the uniform distribution over the interval [ a , b ). If a \\(\\ge\\) b , the behavior is undefined. If rng_distribution \\(=\\) NORMAL , then the random numbers are generated following the normal distribution with mean = a and standard deviation = b . If b \\(\\lt\\) 0, the behavior is undefined. The exact way how random numbers are generated is implementation-defined. For example, they may or may not be deterministic, and they may or may not use hidden state. Inputs Name Type a 0-dimensional tensor of integer, boolean, or floating-point type b 0-dimensional tensor of integer, boolean, or floating-point type shape 1-dimensional tensor constant of type si64 rng_distribution enum of UNIFORM and NORMAL Outputs Name Type result tensor of integer, boolean, or floating-point type Constraints (C1) a , b , and result have the same element type. (C2) If rng_distribution = NORMAL , a , b , and result have the same floating-point element type. (C3) shape( result ) = shape . Examples // %a = 0 // %b = 2 // %shape = [3, 3] %result = \"stablehlo.rng\"(%a, %b, %shape) { rng_distribution = #stablehlo<rng_distribution UNIFORM> } : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32> // %result: [ // [1, 0, 1], // [1, 1, 1], // [0, 0, 0] // ] Back to Ops stablehlo.rng_bit_generator Semantics Returns an output filled with uniform random bits and an updated output state output_state given an initial state initial_state using the pseudorandom number generator algorithm rng_algorithm . The output is guaranteed to be deterministic function of initial_state , but it is not guaranteed to be deterministic between implementations. rng_algorithm is one of the following: DEFAULT : Implementation-defined algorithm. THREE_FRY : Implementation-defined variant of the Threefry algorithm.* PHILOX : Implementation-defined variant of the Philox algorithm.* * See: Salmon et al. SC 2011. Parallel random numbers: as easy as 1, 2, 3. Inputs Name Type initial_state 1-dimensional tensor of type ui64 rng_algorithm enum of DEFAULT , THREE_FRY , and PHILOX Outputs Name Type output_state 1-dimensional tensor of type ui64 output tensor of integer or floating-point type Constraints (C1) type( initial_state ) \\(=\\) type( output_state ). (C2) size( initial_state ) depends on rng_algorithm : DEFAULT : implementation-defined. THREE_FRY : 2 . PHILOX : 2 or 3 . Examples // %initial_state: [1, 2] %output_state, %output = \"stablehlo.rng_bit_generator\"(%initial_state) { rng_algorithm = #stablehlo<rng_algorithm THREE_FRY> } : (tensor<2xui64>) -> (tensor<2xui64>, tensor<2x2xui64>) // %output_state: [1, 6] // %output: [ // [9236835810183407956, 16087790271692313299], // [18212823393184779219, 2658481902456610144] // ] Back to Ops stablehlo.round_nearest_afz Semantics Performs element-wise rounding towards the nearest integer, breaking ties away from zero, on the operand tensor and produces a result tensor. Implements the roundToIntegralTiesToAway operation from the IEEE-754 specification. Inputs Name Type operand tensor of floating-point type Outputs Name Type result tensor of floating-point type Constraints (C1) operand and result have the same type. Examples // %operand = [-2.5, 0.4, 0.5, 0.6, 2.5] %result = \"stablehlo.round_nearest_afz\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-3.0, 0.0, 1.0, 1.0, 3.0] Back to Ops stablehlo.round_nearest_even Semantics Performs element-wise rounding towards the nearest integer, breaking ties towards the even integer, on the operand tensor and produces a result tensor. Implements the roundToIntegralTiesToEven operation from the IEEE-754 specification. Inputs Name Type operand tensor of floating-point type Outputs Name Type result tensor of floating-point type Constraints (C1) operand and result have the same type. Examples // %operand = [-2.5, 0.4, 0.5, 0.6, 2.5] %result = \"stablehlo.round_nearest_even\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-2.0, 0.0, 0.0, 1.0, 2.0] Back to Ops stablehlo.rsqrt Semantics Performs element-wise reciprocal square root operation on operand tensor and produces a result tensor, implementing the rSqrt operation from the IEEE-754 specification. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [[1.0, 4.0], [9.0, 25.0]] %result = \"stablehlo.rsqrt\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 0.5], [0.33333343, 0.2]] // %operand: [(1.0, 2.0)] %result = \"stablehlo.rsqrt\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: [(0.56886448, -0.35157758)] Back to Ops stablehlo.scatter Semantics Produces results tensors which are equal to inputs tensors except that several slices specified by scatter_indices are updated with the values updates using update_computation . The following diagram shows how elements in updates[k] map on elements in results[k] using a concrete example. The diagram picks a few example updates[k] indices and explains in detail which results[k] indices they correspond to. More formally, for all update_index from the index space of updates[0] : update_scatter_dims = [ d for d in axes(updates[0]) and d not in update_window_dims ]. update_scatter_index = [ update_index[d] for d in update_scatter_dims ]. start_index = scatter_indices[si0, ..., :, ..., siN] where si are individual elements in update_scatter_index and : is inserted at the index_vector_dim index, if index_vector_dim < rank(scatter_indices) . [scatter_indices[update_scatter_index]] otherwise. For do in axes(inputs[0]) , full_start_index[do] = start_index[ds] if do = scatter_dims_to_operand_dims[ds] . full_start_index[do] = 0 otherwise. update_window_index = [ update_index[d] for d in update_window_dims ]. full_window_index = [oi0, ..., 0, ..., oiN] where oi are individual elements in update_window_index , and 0 is inserted at indices from inserted_window_dims . result_index = add(full_start_index, full_window_index) . Using this mapping between update_index and result_index , we define results = exec(schedule, inputs) , where: schedule is an implementation-defined permutation of the index space of updates[0] . exec([update_index, ...], results) = exec([...], updated_results) where: updated_values = update_computation(results[:][result_index], updates[:][update_index]) . updated_results is a copy of results with results[:][result_index] set to updated_values[:] . If result_index is out of bounds for shape(results[:]) , the behavior is implementation-defined. exec([], results) = results . If indices_are_sorted is true then the implementation can assume that scatter_indices are sorted with respect to scatter_dims_to_operand_dims , otherwise the behavior is undefined. More formally, for all id < jd from indices(result) , full_start_index(id) <= full_start_index(jd) . If unique_indices is true then the implementation can assume that all result_index indices being scattered to are unique. If unique_indices is true but the indices being scattered to are not unique then the behavior is undefined. Inputs Name Type Constraints inputs variadic number of tensors of any supported types (C1), (C2), (C4), (C5), (C6), (C10), (C13), (C15), (C16) scatter_indices tensor of any supported integer type (C4), (C11), (C14) updates variadic number of tensors of any supported types (C3), (C4), (C5), (C6), (C8) update_window_dims 1-dimensional tensor constant of type si64 (C2), (C4), (C7), (C8) inserted_window_dims 1-dimensional tensor constant of type si64 (C2), (C4), (C9), (C10) scatter_dims_to_operand_dims 1-dimensional tensor constant of type si64 (C11),(C12), (C13) index_vector_dim constant of type si64 (C4), (C11), (C14) indices_are_sorted constant of type i1 unique_indices constant of type i1 update_computation function (C15) Outputs Name Type results variadic number of tensors of any supported types Constraints (C1) All inputs have the same shape. (C2) rank( inputs [0]) = size( update_window_dims ) + size( inserted_window_dims ). (C3) All updates have the same shape. (C4) shape(updates[0]) \\(=\\) combine(update_scatter_dim_sizes, update_window_dim_sizes) where: update_scatter_dim_sizes = shape(scatter_indices) except that the dimension size of scatter_indices corresponding to index_vector_dim is not included. update_window_dim_sizes \\(\\le\\) shape(inputs[0]) except that the dimension sizes in inputs[0] corresponding to inserted_window_dims are not included. combine puts update_scatter_dim_sizes at axes corresponding to update_scatter_dims and update_window_dim_sizes at axes corresponding to update_window_dims . (C5) N \\(=\\) size( inputs ) = size( updates ) and N \\(\\ge\\) 1. (C6) element_type(updates[k]) = element_type(inputs[k]) for any k \\(\\in\\) [0, N). (C7) All dimensions in update_window_dims are unique and sorted. (C8) For all i \\(\\in\\) [0, size( update_window_dims )), \\(0 \\le\\) update_window_dims [i] \\(\\lt\\) rank( updates [0]). (C9) All dimensions in inserted_window_dims are unique and sorted. (C10) For all i \\(\\in\\) [0, size( inserted_window_dims )), \\(0 \\le\\) inserted_window_dims [i] \\(\\lt\\) rank( inputs [0]). (C11) size( scatter_dims_to_operand_dims ) \\(=\\) index_vector_dim \\(\\lt\\) rank( scatter_indices ) ? dim( scatter_indices , index_vector_dim ) : 1. (C12) All dimensions in scatter_dims_to_operand_dims are unique. (C13) For all i \\(\\in\\) [0, size( scatter_dims_to_operand_dims )), \\(0 \\le\\) scatter_dims_to_operand_dims [i] \\(\\lt\\) rank( inputs [0]). (C14) \\(0 \\le\\) index_vector_dim \\(\\le\\) rank( scatter_indices ). (C15) update_computation has type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) where Ek = element_type(inputs[k]) for any k \\(\\in\\) [0, N). (C16) inputs[k] and result[k] have the same type for any k \\(\\in\\) [0, N). Examples // %input: [ // [[1, 2], [3, 4], [5, 6], [7, 8]], // [[9, 10], [11, 12], [13, 14], [15, 16]], // [[17, 18], [19, 20], [21, 22], [23, 24]] // ] // %scatter_indices: [[[0, 2], [1, 0], [2, 1]], [[0, 1], [1, 0], [2, 0]]] // %update: [ // [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 1], [1, 1]]], // [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 1], [1, 1]]] // ] %result = \"stablehlo.scatter\"(%input, %scatter_indices, %update) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) { scatter_dimension_numbers = #stablehlo.scatter< update_window_dims = [2,3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [1, 0], index_vector_dim = 2>, indices_are_sorted = false, unique_indices = false } : (tensor<3x4x2xi32>, tensor<2x3x2xi64>, tensor<2x3x2x2xi32>) -> tensor<3x4x2xi32> // %result: [ // [[1, 2], [5, 6], [8, 9], [8, 9]], // [[10, 11], [12, 13], [14, 15], [16, 17]], // [[18, 19], [20, 21], [21, 22], [23, 24]] // ] Back to Ops stablehlo.select Semantics Produces a result tensor where each element is selected from on_true or on_false tensor based on the value of the corresponding element of pred . More formally, result[i0, ..., iR-1] = pred_val ? on_true[i0, ..., iR-1] : on_false[i0, ..., iR-1] , where pred_val = rank(pred) == 0 ? pred : pred[i0, ..., iR-1] . Inputs Name Type pred tensor of type i1 on_true tensor of any supported type on_false tensor of any supported type Outputs Name Type result tensor of any supported type Constraints (C1) Either rank(pred) \\(=\\) 0 or shape(pred) \\(=\\) shape(on_true) . (C2) on_true , on_false and result have same type. Examples // %pred: [[false, true], [true, false]] // %on_true: [[1, 2], [3, 4]] // %on_false: [[5, 6], [7, 8]] %result = \"stablehlo.select\"(%pred, %on_true, %on_false) : (tensor<2x2xi1>, tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 2], [3, 8]] Back to Ops stablehlo.select_and_scatter Semantics Scatters the values from the source tensor using scatter based on the outcome of reduce_window of the input tensor using select and produces a result tensor. The following diagram shows how elements in result are computed from operand and source using a concrete example. More formally: selected_values = reduce_window_without_init(...) with the following inputs: inputs \\(=\\) [ operand ]. window_dimensions , window_strides , and padding which are used as is. base_dilations \\(=\\) windows_dilations \\(=\\) [1, ..., 1] . body defined as: ( tensor < E > arg0 , tensor < E > arg1 ) -> tensor < E > { return select ( arg0 , arg1 ) ? arg0 : arg1 ; } where E = element_type(operand) . where reduce_window_without_init works exactly like reduce_window , except that the schedule of the underlying reduce doesn't include init values. result[result_index] = reduce([source_values], [init_value], [0], scatter) where: source_values \\(=\\) [ source[source_index] for source_index in source_indices ]. source_indices \\(=\\) [ source_index for source_index in indices(source) if selected_index(source_index) = result_index ]. selected_index(source_index) = operand_index if selected_values[source_index] has the operand element from operand_index . Inputs Name Type Constraints operand tensor of any supported type (C1-C5), (C7), (C9), (C10-C12) source tensor of any supported type (C2), (C3) init_value 0-dimensional tensor of any supported type (C4) window_dimensions 1-dimensional tensor constant type si64 (C1), (C3), (C5), (C6) window_strides 1-dimensional tensor constant type si64 (C3), (C7), (C8) padding 2-dimensional tensor constant type si64 (C3), (C9) select function (C10) scatter function (C11) Outputs Name Type Constraints result tensor of any supported type (C12) Constraints (C1) rank( operand ) \\(=\\) size( window_dimensions ). (C2) operand and source have the same element type. (C3) shape(source) = (padded_operand_shape == 0 || window_dimensions > padded_operand_shape) ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1: padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1] . (C4) element_type( init_value ) \\(=\\) element_type( operand ). (C5) size( window_dimensions ) \\(=\\) rank( operand ). (C6) window_dimensions[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size(window_dimensions)). (C7) size( window_strides ) \\(=\\) rank( operand ). (C8) window_strides[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size(window_strides)). (C9) dim( padding , 0) \\(=\\) rank( operand ) and dim( padding , 1) = 2. (C10) select has type (tensor<E>, tensor<E>) -> tensor<i1> where E = element_type(operand) . (C11) scatter has type (tensor<E>, tensor<E>) -> tensor<E> where E = element_type(operand) . (C12) type( operand ) \\(=\\) type( result ). Examples // %operand: [[1, 5], [2, 5], [3, 6], [4, 4]] // %source: [[5, 6], [7, 8]] // %init_value: 0 %result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = stablehlo.compare GE, %arg0, %arg1 : (tensor<i32>, tensor<i32>) -> tensor<i1> stablehlo.return %0 : tensor<i1> }, { ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = stablehlo.add %arg0, %arg1 : tensor<i32> stablehlo.return %0 : tensor<i32> }) { window_dimensions = dense<[3, 1]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>, padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<4x2xi32>, tensor<2x2xi32>, tensor<i32>) -> tensor<4x2xi32> // %result: [[0, 0], [0, 0], [5, 14], [7, 0]] Back to Ops stablehlo.send Semantics Sends inputs to a channel channel_id and produces a result token. The operation takes a token and produces a token to reify its side effects as a value that other operations can take a data dependency on. If is_host_transfer is true , then the operation transfers data to the host. Otherwise, it transfers data to another device. What this means is implementation-defined. Inputs Name Type inputs variadic number of tensors of any supported type token token channel_id constant of type si64 channel_type enum of DEVICE_TO_DEVICE and DEVICE_TO_HOST is_host_transfer constant of type i1 Outputs Name Type result token Constraints (C1) todo channel_type must be DEVICE_TO_HOST , if is_host_transfer \\(=\\) true , DEVICE_TO_DEVICE , otherwise. Examples %result = \"stablehlo.send\"(%operand, %token) { // channel_id = 5 : i64, // channel_type = #stablehlo<channel_type DEVICE_TO_HOST>, channel_handle = #stablehlo.channel_handle<handle = 5, type = 2>, is_host_transfer = true } : (tensor<3x4xi32>, !stablehlo.token) -> !stablehlo.token Back to Ops stablehlo.shift_left Semantics Performs element-wise left-shift operation on the lhs tensor by rhs number of bits and produces a result tensor. Inputs Name Type lhs tensor of integer type rhs tensor of integer type Outputs Name Type result tensor of integer type Constraints (C1) lhs , rhs , and result have the same type. Examples // %lhs: [-1, -2, 3, 4, 7, 7] // %rhs: [1, 2, 3, 6, 7, 8] %result = \"stablehlo.shift_left\"(%lhs, %rhs): (tensor<6xi8>, tensor<6xi8>) -> tensor<6xi8> // %result: [-2, -8, 24, 0, -128, 0] Back to Ops stablehlo.shift_right_arithmetic Semantics Performs element-wise arithmetic right-shift operation on the lhs tensor by rhs number of bits and produces a result tensor. Inputs Name Type lhs tensor of integer type rhs tensor of integer type Outputs Name Type result tensor of integer type Constraints (C1) lhs , rhs , and result have the same type. Examples // %lhs: [-1, -128, -36, 5, 3, 7] // %rhs: [1, 2, 3, 2, 1, 3] %result = \"stablehlo.shift_right_arithmetic\"(%lhs, %rhs): (tensor<6xi8>, tensor<6xi8>) -> tensor<6xi8> // %result: [-1, -32, -5, 1, 1, 0] Back to Ops stablehlo.shift_right_logical Semantics Performs element-wise logical right-shift operation on the lhs tensor by rhs number of bits and produces a result tensor. Inputs Name Type lhs tensor of integer type rhs tensor of integer type Outputs Name Type result tensor of integer type Constraints (C1) lhs , rhs , and result have the same type. Examples // %lhs: [-1, -128, -36, 5, 3, 7] // %rhs: [1, 2, 3, 2, 1, 3] %result = \"stablehlo.shift_right_logical\"(%lhs, %rhs): (tensor<6xi8>, tensor<6xi8>) -> tensor<6xi8> // %result: [127, 32, 27, 1, 1, 0] Back to Ops stablehlo.sign Semantics Returns the sign of the operand element-wise and produces a result tensor. More formally, for each element x , the semantics can be expressed using Python-like syntax as follows: def sign ( x ): if is_integer ( x ): if compare ( x , 0 , LT , SIGNED ): return - 1 if compare ( x , 0 , EQ , SIGNED ): return 0 if compare ( x , 0 , GT , SIGNED ): return 1 elif is_float ( x ): if x is NaN : return NaN else : if compare ( x , 0.0 , LT , FLOAT ): return - 1.0 if compare ( x , - 0.0 , EQ , FLOAT ): return - 0.0 if compare ( x , + 0.0 , EQ , FLOAT ): return + 0.0 if compare ( x , 0.0 , GT , FLOAT ): return 1.0 elif is_complex ( x ): if x . real is NaN or x . imag is NaN : return NaN else : return divide ( x , abs ( x )) Inputs Name Type operand tensor of signed integer, floating-point, or complex type Outputs Name Type result tensor of signed integer, floating-point, or complex type Constraints (C1) operand and result have the same type. Examples // Logical values: -Inf, +Inf, NaN, ... // %operand: [0xFF800000, 0x7F800000, 0x7FFFFFFF, -10.0, -0.0, 0.0, 10.0] %result = \"stablehlo.sign\"(%operand) : (tensor<7xf32>) -> tensor<7xf32> // %result: [-1.0, 1.0, 0x7FFFFFFF, -1.0, -0.0, 0.0, 1.0] Back to Ops stablehlo.sine Semantics Performs element-wise sine operation on operand tensor and produces a result tensor, implementing the sin operation from the IEEE-754 specification. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [ // [0.0, 1.57079632], // [0, pi/2] // [3.14159265, 4.71238898] // [pi, 3pi/2] // ] %result = \"stablehlo.sine\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 1.0], [0.0, -1.0]] More Examples Back to Ops stablehlo.slice Semantics Extracts a slice from the operand using statically-computed starting indices and produces a result tensor. start_indices contain the starting indices of the slice for each dimension, limit_indices contain the ending indices (exclusive) for the slice for each dimension, and strides contain the strides for each dimension. More formally, result[i0, ..., iR-1] = operand[j0, ..., jR-1] where jd = start_indices[d] + id * strides[d] . Inputs Name Type operand tensor of any supported type start_indices 1-dimensional tensor constant of type si64 limit_indices 1-dimensional tensor constant of type si64 strides 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same element type. (C2) size( start_indices ) = size( limit_indices ) = size( strides ) = rank( operand ). (C3) 0 \\(\\le\\) start_indices[d] \\(\\le\\) limit_indices[d] \\(\\le\\) dim(operand, d) for all dimension d . (C4) 0 \\(\\lt\\) strides[d] for all dimension d . (C5) dim(result, d) = \\(\\lceil\\) (limit_indices[d]-start_indices[d])/stride[d] \\(\\rceil\\) for all dimension d in operand . Examples // 1-dimensional slice // %operand: [0, 1, 2, 3, 4] %result = \"stablehlo.slice\"(%operand) { start_indices = dense<2> : tensor<1xi64>, limit_indices = dense<4> : tensor<1xi64>, strides = dense<1> : tensor<1xi64> } : (tensor<5xi64>) -> tensor<2xi64> // %result: [2, 3] // 2-dimensional slice // %operand: [ // [0, 0, 0, 0], // [0, 0, 1, 1], // [0, 0, 1, 1] // ] %result = \"stablehlo.slice\"(%operand) { start_indices = dense<[1, 2]> : tensor<2xi64>, limit_indices = dense<[3, 4]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> } : (tensor<3x4xi64>) -> tensor<2x2xi64> // % result: [ // [1, 1], // [1, 1] // ] Back to Ops stablehlo.sort Semantics Sorts a variadic number of tensors in inputs together, according to a custom comparator , along the given dimension and produces a variadic number of tensors as results . If is_stable is true, then the sorting is stable, that is, relative order of elements considered to be equal by the comparator is preserved. Two elements e1 and e2 are considered to be equal by the comparator if and only if comparator(e1, e2) = comparator(e2, e1) = false . More formally, for all 0 <= id < jd < dim(inputs[0], d) , either compare_i_j = compare_j_i = false or compare_i_j = true , where: compare_i_j \\(=\\) comparator(inputs[0][i], inputs[0][j], inputs[1][i], inputs[1][j], ...) . For all indices i = [i0, ..., iR-1] and j = [j0, ..., jR-1] . Where i \\(=\\) j everywhere except for the d th dimension. Where d \\(=\\) dimension >= 0 ? dimension : rank(inputs[0]) + dimension . Inputs Name Type inputs variadic number of tensors of any supported type dimension constant of type si64 is_stable constant of type i1 comparator function Outputs Name Type results variadic number of tensors of any supported type Constraints (C1) inputs have at least 1 tensor. (C2) For all i , type(inputs[i]) = type(results[i]) . (C3) All tensors in inputs and results have the same shape. (C4) -R \\(\\le\\) dimension \\(\\lt\\) R , where R is rank of inputs[0] . (C5) comparator has type (tensor<E1>, tensor<E1>, ..., tensor<EN-1>, tensor<EN-1>) -> tensor<i1> , where Ei is element type of inputs[i] . Examples // Sort along dimension 0 // %input0 = [[1, 2, 3], [3, 2, 1]] // %input1 = [[3, 2, 1], [1, 2, 3]] %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<i32>): %predicate = \"stablehlo.compare\"(%arg0, %arg1) { comparison_direction = #stablehlo<comparison_direction GT> } : (tensor<i32>, tensor<i32>) -> tensor<i1> \"stablehlo.return\"(%predicate) : (tensor<i1>) -> () }) { dimension = 0 : i64, is_stable = true } : (tensor<2x3xi32>, tensor<2x3xi32>) -> (tensor<2x3xi32>, tensor<2x3xi32>) // %result0 = [[3, 2, 3], [1, 2, 1]] // %result1 = [[1, 2, 1], [3, 2, 3]] // Sort along dimension 1 // %input0 = [[1, 2, 3], [3, 2, 1]] // %input1 = [[3, 2, 1], [1, 2, 3]] %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<i32>): %predicate = \"stablehlo.compare\"(%arg0, %arg1) { comparison_direction = #stablehlo<comparison_direction GT> } : (tensor<i32>, tensor<i32>) -> tensor<i1> \"stablehlo.return\"(%predicate) : (tensor<i1>) -> () }) { dimension = 1 : i64, is_stable = true } : (tensor<2x3xi32>, tensor<2x3xi32>) -> (tensor<2x3xi32>, tensor<2x3xi32>) // %result0 = [[3, 2, 1], [3, 2, 1]] // %result1 = [[1, 2, 3], [1, 2, 3]] Back to Ops stablehlo.sqrt Semantics Performs element-wise square root operation on operand tensor and produces a result tensor, implementing the squareRoot operation from the IEEE-754 specification. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [[0.0, 1.0], [4.0, 9.0]] %result = \"stablehlo.sqrt\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 1.0], [2.0, 3.0]] // %operand: [(1.0, 2.0)] %result = \"stablehlo.sqrt\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: [(1.27201965, 0.78615138)] Back to Ops stablehlo.subtract Semantics Performs element-wise subtraction of two tensors lhs and rhs and produces a result tensor. For integer element types, if the element-wise difference has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For floating-point element types, it implements the subtraction operation from the IEEE-754 specification. Inputs Name Type lhs tensor of integer, floating-point, or complex type rhs tensor of integer, floating-point, or complex type Outputs Name Type result tensor of integer, floating-point, or complex type Constraints (C1) lhs , rhs and result have the same type. Examples // %lhs: [[6, 8], [10, 12]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.subtract\"(%lhs, %rhs) : (tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2xf32>) // %result: [[1, 2], [3, 4]] More Examples Back to Ops stablehlo.tanh Semantics Performs element-wise tanh operation on operand tensor and produces a result tensor, implementing the tanh operation from the IEEE-754 specification. Numeric precision is implementation-defined. Inputs Name Type operand tensor of floating-point or complex type Outputs Name Type result tensor of floating-point or complex type Constraints (C1) operand and result have the same type. Examples // %operand: [-1.0, 0.0, 1.0] %result = \"stablehlo.tanh\"(%operand) : (tensor<3xf32>) -> tensor<3xf32> // %result: [-0.76159416, 0.0, 0.76159416] More Examples Back to Ops stablehlo.transpose Semantics Permutes the dimensions of operand tensor using permutation and produces a result tensor. More formally, result[i0, ..., iR-1] = operand[j0, ..., jR-1] where i[d] = j[permutation[d]] . Inputs Name Type operand tensor of any supported type permutation 1-dimensional tensor constant of type si64 Outputs Name Type result tensor of any supported type Constraints (C1) operand and result have the same element type. (C2) permutation is a permutation of [0, 1, ..., R-1] where R is the rank of operand . (C3) For all dimensions i in operand , dim(operand, i) = dim(result, j) where j = permutation[i] . Examples // %operand: [ // [[1,2], [3,4], [5,6]], // [[7,8], [9,10], [11,12]] // ] %result = \"stablehlo.transpose\"(%operand) { permutation = dense<[2, 1, 0]> : tensor<3xi64> } : (tensor<2x3x2xi32>) -> tensor<2x3x2xi32> // %result: [ // [[1,7], [3,9], [5,11]], // [[2,8], [4,10], [6,12]] // ] More Examples Back to Ops stablehlo.triangular_solve Semantics Solves batches of systems of linear equations with lower or upper triangular coefficient matrices. More formally, given a and b , result[i0, ..., iR-3, :, :] is the solution to op(a[i0, ..., iR-3, :, :]) * x = b[i0, ..., iR-3, :, :] when left_side is true or x * op(a[i0, ..., iR-3, :, :]) = b[i0, ..., iR-3, :, :] when left_side is false , solving for the variable x where op(a) is determined by transpose_a , which can be one of the following: NO_TRANSPOSE : Perform operation using a as-is. TRANSPOSE : Perform operation on transpose of a . ADJOINT : Perform operation on conjugate transpose of a . Input data is read only from the lower triangle of a , if lower is true or upper triangle of a , otherwise. Output data is returned in the same triangle; the values in the other triangle are implementation-defined. If unit_diagonal is true, then the implementation can assume that the diagonal elements of a are equal to 1, otherwise the behavior is undefined. Inputs Name Type a tensor of floating-point or complex type b tensor of floating-point or complex type left_side constant of type i1 lower constant of type i1 unit_diagonal constant of type i1 transpose_a enum of NO_TRANSPOSE , TRANSPOSE , and ADJOINT Outputs Name Type result tensor of floating-point or complex type Constraints (C1) a and b have the same element type (C2) rank( a ) \\(=\\) rank( b ) \\(\\ge\\) 2. (C3) The relationship between shape( a ) and shape( b ) is as follows: For all i \\(\\in\\) [0, R-3], dim( a , i ) \\(=\\) dim( b , i ). dim(a, R-2) \\(=\\) dim(a, R-1) \\(=\\) dim(b, left_side ? R-2 : R-1) . (C4) b and result have the same type. Examples // %a = [ // [1.0, 0.0, 0.0], // [2.0, 4.0, 0.0], // [3.0, 5.0, 6.0] // ] // %b = [ // [2.0, 0.0, 0.0], // [4.0, 8.0, 0.0], // [6.0, 10.0, 12.0] // ] %result = \"stablehlo.triangular_solve\"(%a, %b) { left_side = true, lower = true, unit_diagonal = false, transpose_a = #stablehlo<transpose NO_TRANSPOSE> } : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<3x3xf32> // %result: [ // [2.0, 0.0, 0.0], // [0.0, 2.0, 0.0], // [0.0, 0.0, 2.0] // ] Back to Ops stablehlo.tuple Semantics Produces a result tuple from values val . Inputs Name Type val variadic number of values of any supported type Outputs Name Type result tuple Constraints (C1) size( val ) \\(=\\) size( result ) \\(=\\) N. (C2) type(val[i]) \\(=\\) type(result[i]) , for all i \\(\\in\\) range [0, N). Examples // %val0: [1.0, 2.0] // %val1: (3) %result = \"stablehlo.tuple\"(%val0, %val1) : (tensor<2xf32>, tuple<tensor<i32>>) -> tuple<tensor<2xf32>, tuple<tensor<i32>>> // %result: ([1.0, 2.0], (3)) Back to Ops stablehlo.while Semantics Produces the output from executing body function 0 or more times while the cond function outputs true . More formally, the semantics can be expressed using Python-like syntax as follows: internal_state = operands while cond ( internal_state ) == True : internal_state = body ( internal_state ) results = internal_state The behavior of an infinite loop is TBD. Inputs Name Type operands variadic number of tensors of any supported type or tokens cond function body function Outputs Name Type results variadic number of tensors of any supported type or tokens Constraints (C1) cond has type (T0, ..., TN-1) -> tensor<i1> , where Ti = type(operands[i]) . (C2) body has type (T0, ..., TN-1) -> (T0, ..., TN-1) , where Ti = type(operands[i]) . (C3) For all i , type(results[i]) = type(operands[i]) . Examples // %constant0: 1 // %input0: 0 // %input1: 10 %results:2 = \"stablehlo.while\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.compare\"(%arg0, %arg1) { comparison_direction = #stablehlo<comparison_direction LT> } : (tensor<i32>, tensor<i32>) -> tensor<i1> \"stablehlo.return\"(%0) : (tensor<i1>) -> () }, { ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %constant0) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0, %arg1) : (tensor<i32>, tensor<i32>) -> () }) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>) // %results#0: 10 // %results#1: 10 Back to Ops stablehlo.xor Semantics Performs element-wise bitwise XOR of two tensors lhs and rhs of integer types and produces a result tensor. For boolean tensors, it computes the logical operation. Inputs Name Type lhs tensor of integer or boolean type rhs tensor of integer or boolean type Outputs Name Type result tensor of integer or boolean type Constraints (C1) lhs , rhs and result have the same type. Examples // Bitwise operation with with integer tensors // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[4, 4], [4, 12]] // Logical operation with with boolean tensors // %lhs: [[false, false], [true, true]] // %rhs: [[false, true], [false, true]] %result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor<2x2xi1>, tensor<2x2xi1>) -> tensor<2x2xi1> // %result: [[false, true], [true, false]] Back to Ops","title":"StableHLO Specification"},{"location":"spec/#stablehlo-specification","text":"","title":"StableHLO Specification"},{"location":"spec/#types","text":"Following are the supported element types in StableHLO: Integer types Signed integer with two\u2019s complement representation. Referred to in the document as si<N> , where the bit-width N \u220a {4, 8, 16, 32, 64}. Unsigned integer referred to in the document as ui<N> , where the bit-width N \u220a {4, 8, 16, 32, 64}. Boolean type referred to in the document as i1 . Exact representation of boolean types (e.g. 1 byte per boolean vs 1 bit per boolean) is implementation-defined. Floating-point types Single precision f32 , double precision f64 and half precision f16 floating-points complying with IEEE 754-2019 format . Bfloat16 bf16 floating-point complying with BFloat16 format . Provides the same number of exponent bits as f32 , so that it matches its dynamic range, but with greatly reduced precision. This also ensures identical behavior for underflows, overflows, and NaNs. However, bf16 handles denormals differently from f32 : it flushes them to zero. FP8 f8E4M3FN and f8E5M2 types corresponding to respectively the E4M3 and E5M2 types from the whitepaper FP8 Formats for Deep Learning . Complex types represent a pair of floating-point types. Supported ones are complex<f32> (represents a par of f32 ) and complex<f64> (represents a pair of f64 ). Exact representation of complex types (e.g. whether the real part or the imaginary part comes first in memory) is implementation-defined. Tensor types are the cornerstone of the StableHLO type system. They model immutable n-dimensional arrays and are referred to in the document as tensor<SxE> where: Shape S represented as (d0)x(d1)x...x(dR-1) is a 1-dimensional array of dimension sizes di , in the increasing order of the corresponding dimensions (which are also called axes ) 0, 1, ..., R-1. The size R of this array is called rank . Dimension sizes have type si64 and are non-negative (dimension sizes equal to zero are allowed, and their meaning is described below). Ranks equal to zero are also allowed, and their meaning is also described below. Element type E is any one of the supported element types mentioned above. For example, tensor<2x3xf32> is a tensor type with shape 2x3 and element type f32 . It has two dimensions (or, in other words, two axes) whose sizes are 2 and 3. Its rank is 2. At the logical level, a tensor<SxE> maps a 1-dimensional array of indices {i0, i1, ..., iR-1} on elements of type E . If a tensor t maps an index i on an element e , we say that t[i0, i1, ..., iR-1] = e . Individual indices have type si64 and are within the range [0, di) defined by the corresponding dimension. The size of the index array is equal to R . At the moment, StableHLO only supports dense tensors, so each tensor has 1*(d0)*(d1)*...*(dR-1) elements whose indices are drawn from an index space which is a Cartesian product of its dimensions. For example: tensor<2x3xf32> has 6 elements whose indices are {0, 0} , {0, 1} , {0, 2} , {1, 0} , {1, 1} and {1, 2} . Tensors of rank zero, e.g tensor<f32> , have 1 element. Such tensors are allowed and are useful to model scalars. Tensors with dimensions of size zero, e.g. tensor<2x0xf32> , have 0 elements. Such tensors are allowed and are useful in rare cases, e.g. to model empty slices. Canonical representation of a tensor is a 1-dimensional array of elements which correspond to indices ordered lexicographically. For example, for a tensor<2x3xf32> with the following mapping from indices to elements: {0, 0} => 1 , {0, 1} => 2 , {0, 2} => 3 , {1, 0} => 4 , {1, 1} => 5 , {1, 2} => 6 - the canonical representation would be: [1, 2, 3, 4, 5, 6] . Exact representation of tensors is implementation-defined. This specification does not define in which order tensor elements are laid out in memory (e.g. whether/when they follow the canonical order) and how individual tensor elements in a particular order are packed together into a tensor (e.g. how these elements are aligned, whether they are stored contiguously, etc). Token type Values of this type are used for imposing order on execution of side-effecting operations using data dependencies. Tuple types model heterogeneous lists and are referred to in the document using: 1) the full form: tuple<T0, ... TN-1> , 2) the short form: tuple , where: N is the tuple size. Ti are types of tuple elements. Element types are one of tensor , token or tuple . Tuple types are inherited from HLO where they are used to model variadic inputs and outputs. In StableHLO, variadic inputs and outputs are supported natively, so the only use of tuple types in StableHLO is in custom_call where tuple types are used to model HLO-compatible ABI of custom calls. Function types model functions and are referred to in the document using: 1) the full form: (I1, ..., IN) -> (O1, ..., OM) , or 2) the short form: function , where: Ii are types of inputs of the corresponding function. Oj are types of outputs of the corresponding function. Input types and output types are one of tensor , token or tuple . Function types are not first class, i.e. StableHLO doesn't support values of function types. Some StableHLO ops can take functions as inputs, but they are never produced as outputs. String type represents a sequence of bytes and is referred to in the document as string . Exact representation of string type (e.g. null terminated or not, encoding etc.) is implementation-defined. Strings types are not first class, i.e. StableHLO doesn't support values of string types. Some StableHLO ops can take strings as inputs, but they are never produced as outputs.","title":"Types"},{"location":"spec/#programs","text":"StableHLO programs consist of StableHLO functions . Each function has inputs and outputs of supported types and a list of ops in static single-assignment (SSA) form which is terminated by a return op which produces the outputs of the function. Here is an example of a program that consists of a function @main which takes three inputs ( %image , %weights and %bias ) and produces one output ( %4 ). Below we describe how this program can be executed. stablehlo.func @main( %image: tensor<28x28xf32>, %weights: tensor<784x10xf32>, %bias: tensor<1x10xf32> ) -> tensor<1x10xf32> { %0 = \"stablehlo.reshape\"(%image) : (tensor<28x28xf32>) -> tensor<1x784xf32> %1 = \"stablehlo.dot\"(%0, %weights) : (tensor<1x784xf32>, tensor<784x10xf32>) -> tensor<1x10xf32> %2 = \"stablehlo.add\"(%1, %bias) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> %3 = \"stablehlo.constant\"() { value = dense<0.0> : tensor<1x10xf32> } : () -> tensor<1x10xf32> %4 = \"stablehlo.maximum\"(%2, %3) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32> \"stablehlo.return\"(%4): (tensor<1x10xf32>) -> () }","title":"Programs"},{"location":"spec/#execution","text":"","title":"Execution"},{"location":"spec/#sequential-execution","text":"A StableHLO program is executed by providing input values to the main function and computing output values. Output values of a function are computed by executing the graph of ops rooted in the corresponding return op. The execution order is implementation-defined, as long as ops are executed before their uses. Possible execution orders of the example program above are %0 \u2192 %1 \u2192 %2 \u2192 %3 \u2192 %4 \u2192 return or %3 \u2192 %0 \u2192 %1 \u2192 %2 \u2192 %4 \u2192 return . More formally, a StableHLO process is a combination of: 1) a StableHLO program, 2) operation statuses (not executed yet, already executed), and 3) intermediate values that the process is working on. The process starts with input values to the main function, progresses through the graph of ops updating operation statuses and intermediate values and finishes with output values. Further formalization is TBD.","title":"Sequential execution"},{"location":"spec/#parallel-execution","text":"StableHLO programs can be executed in parallel, organized into a 2D grid of num_replicas by num_partitions which both have type ui32 . In the StableHLO grid , num_replicas * num_partitions of StableHLO processes are executing at the same time. Each process has a unique process_id = (replica_id, partition_id) , where replica_id \u220a replica_ids = [0, ..., num_replicas-1] and partition_id \u220a partition_ids = [0, ..., num_partitions-1] which both have type ui32 . The size of the grid is known statically for every program, and the position within the grid is known statically for every process. Each process has access to its position within the grid via the replica_id and partition_id ops. Within the grid, the programs can all be the same (in the \"Single Program, Multiple Data\" style), can all be different (in the \"Multiple Program, Multiple Data\" style) or something in between. Within the grid, the processes are mostly independent from each other - they have separate operation statuses, separate input/intermediate/output values and most of the ops are executed separately between processes, with the exception of a small number of collective ops described below. Given that execution of most of the ops is only using values from the same process, it is usually unambiguous to refer to these values by their names. However, when describing semantics of collective ops, that is insufficient, and we use the notation name@process_id to refer to the value name within a particular process. (From that perspective, unqualified name can be viewed as a shorthand for name@(replica_id(), partition_id()) ). The execution order across processes is implementation-defined, except for the synchronization introduced by point-to-point communication and collective ops as described below.","title":"Parallel execution"},{"location":"spec/#point-to-point-communication","text":"StableHLO processes can communicate with each other through StableHLO channels . A channel is represented by a positive id of type si64 . Through various ops, it is possible to send values to channels and receive them from channels. Further formalization, e.g. where these channel ids are coming from, how processes programs become aware of them and what kind of synchronization is introduced by them, is TBD.","title":"Point-to-point communication"},{"location":"spec/#streaming-communication","text":"Every StableHLO process has access to two streaming interfaces: Infeed that can be read from. Outfeed that can be written to. Unlike channels, which are used to communicate between processes and therefore have processes at both of their ends, infeeds and outfeeds have their other end implementation-defined. Further formalization, e.g. how streaming communication influences execution order and what kind of synchronization is introduced by it, is TBD.","title":"Streaming communication"},{"location":"spec/#collective-ops","text":"There are five collective ops in StableHLO: all_gather , all_reduce , all_to_all , collective_permute and reduce_scatter . All these ops split the processes in the StableHLO grid into StableHLO process groups and execute a joint computation within each process group, independently from other process groups. Within each process group, collective ops may introduce a synchronization barrier. Further formalization, e.g. elaborating on when exactly this synchronization happens, how exactly the processes arrive at this barrier, and what happens if they don't, is TBD. If the process group involves cross-partition communication, i.e. there are processes in the process group whose partition ids are different, then execution of the collective op needs a channel, and the collective op must provide a positive channel_id of type si64 . Cross-replica communication doesn't need channels. The computations performed by the collective ops are specific to individual ops and are described in individual op sections below. However, the strategies by which the grid is split into process groups are shared between these ops and are described in this section. More formally, StableHLO supports the following four strategies.","title":"Collective ops"},{"location":"spec/#cross_replica","text":"Only cross-replica communications happen within each process group. This strategy takes replica_groups - a list of lists of replica ids - and computes a Cartesian product of replica_groups by partition_ids . replica_groups must have unique elements and cover all replica_ids . More formally: def cross_replica ( replica_groups : List [ List [ ReplicaId ]]) -> List [ List [ ProcessId ]]: for replica_group in replica_groups : for partition_id in partition_ids : process_group = [] for replica_id in replica_group : process_group . append (( replica_id , partition_id )) yield process_group For example, for replica_groups = [[0, 1], [2, 3]] and num_partitions = 2 , cross_replica will produce [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(2, 0), (3, 0)], [(2, 1), (3, 1)]] .","title":"cross_replica"},{"location":"spec/#cross_partition","text":"Only cross-partition communications happen within each process group. This strategy takes partition_groups - a list of lists of partition ids - and computes a Cartesian product of partition_groups by replica_ids . partition_groups must have unique elements and cover all partition_ids . More formally: def cross_partition ( partition_groups : List [ List [ PartitionId ]]) -> List [ List [ ProcessId ]]: for partition_group in partition_groups : for replica_id in replica_ids : process_group = [] for partition_id in partition_group : process_group . append (( replica_id , partition_id )) yield process_group For example, for partition_groups = [[0, 1]] and num_replicas = 4 , cross_partition will produce [[(0, 0), (0, 1)], [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)]] .","title":"cross_partition"},{"location":"spec/#cross_replica_and_partition","text":"Both cross-replica and cross-partition communications may happen within each process group. This strategy takes replica_groups - a list of lists of replica ids - and computes Cartesian products of each replica_group by partition_ids . replica_groups must have unique elements and cover all replica_ids . More formally: def cross_replica_and_partition ( replica_groups : List [ List [ ReplicaId ]]) -> List [ List [ ProcessId ]]: for replica_group in replica_groups : process_group = [] for partition_id in partition_ids : for replica_id in replica_group : process_group . append (( replica_id , partition_id )) yield process_group For example, for replica_groups = [[0, 1], [2, 3]] and num_partitions = 2 , cross_replica_and_partition will produce [[(0, 0), (1, 0), (0, 1), (1, 1)], [(2, 0), (3, 0), (2, 1), (3, 1)]] .","title":"cross_replica_and_partition"},{"location":"spec/#flattened_ids","text":"This strategy takes flattened_id_groups - a list of lists of \"flattened\" process ids in the form of replica_id * num_partitions + partition_id - and turns them into process ids. flattened_id_groups must have unique elements and cover all process_ids . More formally: def flattened_ids ( flattened_id_groups : List [ List [ ui32 ]]) -> List [ List [ ProcessId ]]: for flattened_id_group in flattened_id_groups : process_group = [] for flattened_id in flattened_id_group : replica_id = flattened_id // num_partitions partition_id = flattened_id % num_partitions process_group . append (( replica_id , partition_id )) yield process_group For example, for flattened_id_groups = [[0, 1, 2, 3], [4, 5, 6, 7]] , num_replicas = 4 and num_partitions = 2 , flattened_ids will produce [[(0, 0), (0, 1), (1, 0), (1, 1)], [(2, 0), (2, 1), (3, 0), (3, 1)]] .","title":"flattened_ids"},{"location":"spec/#errors","text":"StableHLO programs are validated through an extensive set of constraints for individual ops, which rules out many classes of errors prior to run time. However, error conditions are still possible, e.g. through integer overflows, out-of-bounds accesses, etc. Unless explicitly called out, all these errors result in implementation-defined behavior. As an exception to this rule, floating-point exceptions in StableHLO programs have well-defined behavior. Operations which result in exceptions defined by the IEEE-754 standard (invalid operation, division-by-zero, overflow, underflow, or inexact exceptions) produce default results (as defined in the standard) and continue execution without raising the corresponding status flag; similar to raiseNoFlag exception handling from the standard. Exceptions for nonstandard operations (e.g. complex arithmetic and certain transcendental functions) are implementation-defined.","title":"Errors"},{"location":"spec/#constants","text":"The section describes the constants supported in StableHLO along with their syntax. Integer constants use decimal notation, e.g. 123 , or hexadecimal notation, e.g. ff . Negative numbers can be used with signed integer types, but not with unsigned integer types. Boolean constants true and false are both valid constants of the pred type. Floating-point constants use decimal notation, e.g. 123.421 , exponential notation, e.g. 1.23421e+2 , or a more precise hexadecimal notation, e.g. 0x42f6d78d . Complex constants Complex constants are represented as a pair of floating-point constants of f32 or f64 types, e.g. (12.34, 56.78) , where the first constant is the real part, and the second constant is the imaginary part. Tensor constants use NumPy notation. For example, [[1, 2, 3], [4, 5, 6]] is a constant of type tensor<2x3xf32> with the following mapping from indices to elements: {0, 0} => 1 , {0, 1} => 2 , {0, 2} => 3 , {1, 0} => 4 , {1, 1} => 5 , {1, 2} => 6 . String constants String constants are represented as a sequence of bytes enclosed in double quotation mark symbols, e.g. \"foo123?\" (in ASCII encoding) or \"\\18\\A3\" (in hex encoding).","title":"Constants"},{"location":"spec/#index-of-ops","text":"abs add after_all all_gather all_reduce all_to_all and atan2 batch_norm_grad batch_norm_inference batch_norm_training bitcast_convert broadcast_in_dim case cbrt ceil cholesky clamp collective_permute compare complex concatenate constant convert convolution cosine count_leading_zeros custom_call divide dot_general dynamic_slice dynamic_update_slice exponential exponential_minus_one fft floor gather get_tuple_element if imag infeed iota is_finite log log_plus_one logistic map maximum minimum multiply negate not optimization_barrier or outfeed pad partition_id popcnt power real recv reduce reduce_precision reduce_scatter reduce_window remainder replica_id reshape reverse rng rng_bit_generator round_nearest_afz round_nearest_even rsqrt scatter select select_and_scatter send shift_left shift_right_arithmetic shift_right_logical sign sine slice sort sqrt subtract tanh transpose triangular_solve tuple while xor","title":"Index of Ops"},{"location":"spec/#stablehloabs","text":"","title":"stablehlo.abs"},{"location":"spec/#semantics","text":"Performs element-wise absolute value of operand tensor and produces a result tensor. For floating-point element types, it implements the abs operation from the IEEE-754 specification. For n-bit signed integer, the absolute value of \\(-2^{n-1}\\) is implementation- defined and one of the following: Saturation to \\(2^{n-1}-1\\) \\(-2^{n-1}\\)","title":"Semantics"},{"location":"spec/#inputs","text":"Name Type operand tensor of signed integer, floating-point, or complex type","title":"Inputs"},{"location":"spec/#outputs","text":"Name Type result tensor of signed integer, floating-point, or complex type","title":"Outputs"},{"location":"spec/#constraints","text":"(C1) operand and result have the same shape. (C2) operand and result have the same element type, except when the element type of the operand is complex type, in which case the element type of the result is the element type of the complex type (e.g. the element type of the result is f64 for operand type complex<f64> ).","title":"Constraints"},{"location":"spec/#examples","text":"// %operand: [-2, 0, 2] %result = \"stablehlo.abs\"(%operand) : (tensor<3xi32>) -> tensor<3xi32> // %result: [2, 0, 2] Back to Ops","title":"Examples"},{"location":"spec/#stablehloadd","text":"","title":"stablehlo.add"},{"location":"spec/#semantics_1","text":"Performs element-wise addition of two tensors lhs and rhs and produces a result tensor. For integer element types, if the element-wise sum has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For floating-point element types, it implements the addition operation from the IEEE-754 specification. For boolean element type, the behavior is same as stablehlo.or .","title":"Semantics"},{"location":"spec/#inputs_1","text":"Name Type lhs tensor of any supported type rhs tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_1","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_1","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_1","text":"// %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.add\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[6, 8], [10, 12]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehloafter_all","text":"","title":"stablehlo.after_all"},{"location":"spec/#semantics_2","text":"Ensures that the operations producing the inputs are executed before any operations that depend on result . Execution of this operation does nothing, it only exists to establish data dependencies from result to inputs .","title":"Semantics"},{"location":"spec/#inputs_2","text":"Name Type inputs variadic number of token","title":"Inputs"},{"location":"spec/#outputs_2","text":"Name Type result token","title":"Outputs"},{"location":"spec/#examples_2","text":"%result = \"stablehlo.after_all\"(%input0, %input1) : (!stablehlo.token, !stablehlo.token) -> !stablehlo.token Back to Ops","title":"Examples"},{"location":"spec/#stablehloall_gather","text":"","title":"stablehlo.all_gather"},{"location":"spec/#semantics_3","text":"Within each process group in the StableHLO grid, concatenates the values of the operand tensor from each process along all_gather_dim and produces a result tensor. The operation splits the StableHLO grid into process_groups as follows: channel_id <= 0 and use_global_device_ids = false , cross_replica(replica_groups) . channel_id > 0 and use_global_device_ids = false , cross_replica_and_partition(replica_groups) . channel_id > 0 and use_global_device_ids = true , flattened_ids(replica_groups) . Afterwards, within each process_group : operands@receiver = [operand@sender for sender in process_group] for all receiver in process_group . result@process = concatenate(operands@process, all_gather_dim) for all process in process_group .","title":"Semantics"},{"location":"spec/#inputs_3","text":"Name Type operand tensor of any supported type all_gather_dim constant of type si64 replica_groups 2-dimensional tensor constant of type si64 channel_id constant of type si64 use_global_device_ids constant of type boolean","title":"Inputs"},{"location":"spec/#outputs_3","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_2","text":"(C1) all_gather_dim \\(\\in\\) [0, rank( operand )). (C2) All values in replica_groups are unique. (C3) size(replica_groups) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_replica_and_partition , num_replicas . If flattened_ids , num_processes . (C4) \\(0 \\le\\) replica_groups [i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C5) If use_global_device_ids = true , then channel_id > 0 . todo (C6) type(result) = type(operand) except: dim(result, all_gather_dim) = dim(operand, all_gather_dim) * dim(process_groups, 1) .","title":"Constraints"},{"location":"spec/#examples_3","text":"// num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [[1.0, 2.0], [3.0, 4.0]] // %operand@(1, 0): [[5.0, 6.0], [7.0, 8.0]] %result = \"stablehlo.all_gather\"(%operand) { all_gather_dim = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> // use_global_device_ids = false } : (tensor<2x2xf32>) -> tensor<2x4xf32> // %result@(0, 0): [[1.0, 2.0, 5.0, 6.0], [3.0, 4.0, 7.0, 8.0]] // %result@(1, 0): [[1.0, 2.0, 5.0, 6.0], [3.0, 4.0, 7.0, 8.0]] Back to Ops","title":"Examples"},{"location":"spec/#stablehloall_reduce","text":"","title":"stablehlo.all_reduce"},{"location":"spec/#semantics_4","text":"Within each process group in the StableHLO grid, applies a reduction function computation to the values of the operand tensor from each process and produces a result tensor. The operation splits the StableHLO grid into process groups as follows: channel_id <= 0 and use_global_device_ids = false , cross_replica(replica_groups) . channel_id > 0 and use_global_device_ids = false , cross_replica_and_partition(replica_groups) . channel_id > 0 and use_global_device_ids = true , flattened_ids(replica_groups) . Afterwards, within each process_group : operands@receiver = [operand@sender for sender in process_group] for all receiver in process_group . result@process[i0, i1, ..., iR-1] = reduce_without_init( inputs=operands@process[:][i0, i1, ..., iR-1], dimensions=[0], body=computation ) where reduce_without_init works exactly like reduce , except that its schedule doesn't include init values.","title":"Semantics"},{"location":"spec/#inputs_4","text":"Name Type operand tensor of any supported type replica_groups variadic number of 1-dimensional tensor constants of type si64 channel_id constant of type si64 use_global_device_ids constant of type boolean computation function","title":"Inputs"},{"location":"spec/#outputs_4","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_3","text":"(C1) All values in replica_groups are unique. (C2) size(replica_groups) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_replica_and_partition , num_replicas . If flattened_ids , num_processes . (C3) \\(0 \\le\\) replica_groups [i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C4) If use_global_device_ids = true , then channel_id > 0 . todo (C5) computation has type (tensor<E>, tensor<E>) -> (tensor<E>) where E = element_type(operand) . (C6) type( result ) \\(=\\) type( operand ).","title":"Constraints"},{"location":"spec/#examples_4","text":"// num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [1.0, 2.0, 3.0, 4.0] // %operand@(1, 0): [5.0, 6.0, 7.0, 8.0] %result = \"stablehlo.all_reduce\"(%operand) ({ ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32> \"stablehlo.return\"(%0) : (tensor<f32>) -> () }) { replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> // use_global_device_ids = false } : (tensor<4xf32>) -> tensor<4xf32> // %result@(0, 0): [6.0, 8.0, 10.0, 12.0] // %result@(1, 0): [6.0, 8.0, 10.0, 12.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloall_to_all","text":"","title":"stablehlo.all_to_all"},{"location":"spec/#semantics_5","text":"Within each process group in the StableHLO grid, splits the values of the operand tensor along split_dimension into parts, scatters the split parts between the processes, concatenates the scattered parts along concat_dimension and produces a result tensor. The operation splits the StableHLO grid into process groups using the cross_replica(replica_groups) strategy. Afterwards, within each process_group : split_parts@sender = [ slice( operand=operand@sender, start_indices=[s0, s1, ..., sR-1], # where # - sj = 0 if j != split_dimension # - sj = i * dim(operand, j) / split_count, if j == split_dimension # - R = rank(operand) limit_indices=[l0, l1, ..., lR-1], # where # - lj = dim(operand, j) if j != split_dimension # - lj = (i + 1) * dim(operand, j) / split_count, if j == split_dimension strides=[1, ..., 1] ) for i in range(split_count) ] for all sender in process_group . scattered_parts@receiver = [split_parts@sender[receiver_index] for sender in process_group] where receiver_index = index_of(receiver, process_group) . result@process = concatenate(scattered_parts@process, concat_dimension) .","title":"Semantics"},{"location":"spec/#inputs_5","text":"Name Type operand tensor of any supported type split_dimension constant of type si64 concat_dimension constant of type si64 split_count constant of type si64 replica_groups 2-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_5","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_4","text":"(C1) split_dimension \\(\\in\\) [0, rank( operand )). (C2) dim( operand , split_dimension ) % split_count \\(=\\) 0. (C3) concat_dimension \\(\\in\\) [0, rank( operand )). (C4) split_count \\(\\gt\\) 0. (C5) All values in replica_groups are unique. (C6) size(replica_groups) = num_replicas . (C7) \\(0 \\le\\) replica_groups [i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C8) type(result) = type(operand) except: dim(result, split_dimension) = dim(operand, split_dimension) / split_count . dim(result, concat_dimension) = dim(operand, concat_dimension) * split_count .","title":"Constraints"},{"location":"spec/#examples_5","text":"// num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [ // [1.0, 2.0, 3.0, 4.0], // [5.0, 6.0, 7.0, 8.0] // ] // %operand@(1, 0): [ // [9.0, 10.0, 11.0, 12.0], // [13.0, 14.0, 15.0, 16.0] // ] %result = \"stablehlo.all_to_all\"(%operand) { split_dimension = 1 : i64, concat_dimension = 0 : i64, split_count = 2 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64> } : (tensor<2x4xf32>) -> tensor<4x2xf32> // %result@(0, 0): [ // [1.0, 2.0], // [5.0, 6.0], // [9.0, 10.0], // [13.0, 14.0] // ] // %result@(1, 0): [ // [3.0, 4.0], // [7.0, 8.0], // [11.0, 12.0], // [15.0, 16.0] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloand","text":"","title":"stablehlo.and"},{"location":"spec/#semantics_6","text":"Performs element-wise bitwise or logical AND of two tensors lhs and rhs and produces a result tensor. For integer tensors, computes the bitwise operation. For boolean tensors, computes the logical operation.","title":"Semantics"},{"location":"spec/#inputs_6","text":"Name Type lhs tensor of integer or boolean type rhs tensor of integer or boolean type","title":"Inputs"},{"location":"spec/#outputs_6","text":"Name Type result tensor of integer or boolean type","title":"Outputs"},{"location":"spec/#constraints_5","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_6","text":"// %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.and\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[1, 2], [3, 0]] Back to Ops","title":"Examples"},{"location":"spec/#stablehloatan2","text":"","title":"stablehlo.atan2"},{"location":"spec/#semantics_7","text":"Performs element-wise atan2 operation on lhs and rhs tensor and produces a result tensor, implementing the atan2 operation from the IEEE-754 specification. For complex element types, it computes a complex atan2 function with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_7","text":"Name Type lhs tensor of floating-point or complex type rhs tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_7","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_6","text":"(C1) lhs , rhs , and result have the same type.","title":"Constraints"},{"location":"spec/#examples_7","text":"// %lhs: [0.0, 1.0, -1.0] // %rhs: [0.0, 0.0, 0.0] %result = \"stablehlo.atan2\"(%lhs, %rhs) : (tensor<3xf32>, tensor<3xf32>) -> tensor<3xf32> // %result: [0.0, 1.57079637, -1.57079637] // [0.0, pi/2, -pi/2] Back to Ops","title":"Examples"},{"location":"spec/#stablehlobatch_norm_grad","text":"","title":"stablehlo.batch_norm_grad"},{"location":"spec/#semantics_8","text":"Computes gradients of several inputs of batch_norm_training backpropagating from grad_output , and produces grad_operand , grad_scale and grad_offset tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows: def compute_sum ( operand , feature_index ): ( sum ,) = reduce ( inputs = [ operand ], init_values = [ 0.0 ], dimensions = [ i for i in range ( rank ( operand )) if i != feature_index ], body = lambda x , y : add ( x , y )) return sum def compute_mean ( operand , feature_index ): sum = compute_sum ( operand , feature_index ) divisor = constant ( num_elements ( operand ) / dim ( operand , feature_index )) divisor_bcast = broadcast_in_dim ( divisor , [], shape ( sum )) return divide ( sum , divisor_bcast ) def batch_norm_grad ( operand , scale , mean , variance , grad_output , epsilon , feature_index ): # Broadcast inputs to shape(operand) scale_bcast = broadcast_in_dim ( scale , [ feature_index ], shape ( operand )) mean_bcast = broadcast_in_dim ( mean , [ feature_index ], shape ( operand )) variance_bcast = broadcast_in_dim ( variance , [ feature_index ], shape ( operand )) epsilon_bcast = broadcast_in_dim ( constant ( epsilon ), [], shape ( operand )) # Perform normalization using the provided `mean` and `variance` # Intermediate values will be useful for computing gradients centered_operand = subtract ( operand , mean_bcast ) stddev = sqrt ( add ( variance_bcast , epsilon_bcast )) normalized_operand = divide ( centered_operand , stddev ) # Use the implementation from batchnorm_expander.cc in XLA # Temporary variables have exactly the same names as in the C++ code elements_per_feature = constant ( divide ( size ( operand ), dim ( operand , feature_index ))) i1 = multiply ( grad_output , broadcast_in_dim ( elements_per_feature , [], shape ( operand ))) i2 = broadcast_in_dim ( compute_sum ( grad_output , feature_index ), [ feature_index ], shape ( operand )) i3 = broadcast_in_dim ( compute_sum ( multiply ( grad_output , centered_operand )), [ feature_index ], shape ( operand )) i4 = multiply ( i3 , centered_operand ) i5 = divide ( i4 , add ( variance_bcast , epsilon_bcast )) grad_operand = multiply ( divide ( divide ( scale_bcast , stddev ), elements_per_feature ), subtract ( subtract ( i1 , i2 ), i5 )) grad_scale = compute_sum ( multiply ( grad_output , normalized_operand ), feature_index ) grad_offset = compute_sum ( grad_output , feature_index ) return grad_operand , grad_scale , grad_offset","title":"Semantics"},{"location":"spec/#inputs_8","text":"Name Type operand tensor of floating-point type scale 1-dimensional tensor of floating-point type mean 1-dimensional tensor of floating-point type variance 1-dimensional tensor of floating-point type grad_output tensor of floating-point type epsilon constant of type f32 feature_index constant of type si64","title":"Inputs"},{"location":"spec/#outputs_8","text":"Name Type grad_operand tensor of floating-point type grad_scale 1-dimensional tensor of floating-point type grad_offset 1-dimensional tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_7","text":"(C1) 0 \\(\\le\\) feature_index \\(\\lt\\) rank( operand ). (C2) operand , scale , mean , variance , grad_output , grad_operand grad_scale and grad_offset have the same element type. (C3) operand , grad_output and grad_operand have the same shape. (C4) scale , mean , variance , grad_scale and grad_offset have the same shape. (C5) size( scale ) \\(=\\) dim(operand, feature_index) .","title":"Constraints"},{"location":"spec/#examples_8","text":"// %operand: [ // [[1.0, 2.0], [3.0, 4.0]], // [[3.0, 4.0], [1.0, 2.0]] // ] // %scale: [1.0, 1.0] // %mean: [2.0, 3.0] // %variance: [1.0, 1.0] // %grad_output: [ // [[0.1, 0.1], [0.1, 0.1]], // [[0.1, 0.1], [0.1, 0.1]] // ] %grad_operand, %grad_scale, %grad_offset = \"stablehlo.batch_norm_grad\"(%operand, %scale, %mean, %variance, %grad_output) { epsilon = 0.0 : f32, feature_index = 2 : i64 } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2x2x2xf32>) -> (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) // %grad_operand: [ // [[0.0, 0.0], [0.0, 0.0]], // [[0.0, 0.0], [0.0, 0.0]] // ] // %grad_scale: [0.0, 0.0] // %grad_offset: [0.4, 0.4] Back to Ops","title":"Examples"},{"location":"spec/#stablehlobatch_norm_inference","text":"","title":"stablehlo.batch_norm_inference"},{"location":"spec/#semantics_9","text":"Normalizes the operand tensor across all dimensions except for the feature_index dimension and produces a result tensor. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows: def batch_norm_inference ( operand , scale , offset , mean , variance , epsilon , feature_index ): # Broadcast inputs to shape(operand) scale_bcast = broadcast_in_dim ( scale , [ feature_index ], shape ( operand )) offset_bcast = broadcast_in_dim ( offset , [ feature_index ], shape ( operand )) mean_bcast = broadcast_in_dim ( mean , [ feature_index ], shape ( operand )) variance_bcast = broadcast_in_dim ( variance , [ feature_index ], shape ( operand )) epsilon_bcast = broadcast_in_dim ( constant ( epsilon ), [], shape ( operand )) # Perform normalization using the provided `mean` and `variance` instead of # computing them like `batch_norm_training` does. centered_operand = subtract ( operand , mean_bcast ) stddev = sqrt ( add ( variance_bcast , epsilon_bcast )) normalized_operand = divide ( centered_operand , stddev ) return add ( multiply ( scale_bcast , normalized_operand ), offset_bcast ) Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_9","text":"Name Type operand tensor of floating-point type scale 1-dimensional tensor of floating-point type offset 1-dimensional tensor of floating-point type mean 1-dimensional tensor of floating-point type variance 1-dimensional tensor of floating-point type epsilon constant of type f32 feature_index constant of type si64","title":"Inputs"},{"location":"spec/#outputs_9","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_8","text":"(C1) 0 \\(\\le\\) feature_index \\(\\lt\\) rank( operand ). (C2) operand , scale , offset , mean , variance and result have the same element type. (C3) size( scale ) \\(=\\) dim(operand, feature_index) . (C4) size( offset ) \\(=\\) dim(operand, feature_index) . (C5) size( mean ) \\(=\\) dim(operand, feature_index) . (C6) size( variance ) \\(=\\) dim(operand, feature_index) . (C7) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_9","text":"// %operand: [ // [[1.0, 2.0], [3.0, 4.0]], // [[3.0, 4.0], [1.0, 2.0]] // ] // %scale: [1.0, 1.0] // %offset: [1.0, 1.0] // %mean: [2.0, 3.0] // %variance: [1.0, 1.0] %result = \"stablehlo.batch_norm_inference\"(%operand, %scale, %offset, %mean, %variance) { epsilon = 0.0 : f32, feature_index = 2 : i64 } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>) -> tensor<2x2x2xf32> // %result: [ // [[0.0, 0.0], [2.0, 2.0]], // [[2.0, 2.0], [0.0, 0.0]] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlobatch_norm_training","text":"","title":"stablehlo.batch_norm_training"},{"location":"spec/#semantics_10","text":"Computes mean and variance across all dimensions except for the feature_index dimension and normalizes the operand tensor producing output , batch_mean and batch_var tensors. More formally, this operation can be expressed as a decomposition to existing StableHLO operations using Python-like syntax as follows: def compute_mean ( operand , feature_index ): ( sum ,) = reduce ( inputs = [ operand ], init_values = [ 0.0 ], dimensions = [ i for i in range ( rank ( operand )) if i != feature_index ], body = lambda x , y : add ( x , y )) divisor = constant ( num_elements ( operand ) / dim ( operand , feature_index )) divisor_bcast = broadcast_in_dim ( divisor , [], shape ( sum )) return divide ( sum , divisor_bcast ) def compute_variance ( operand , feature_index ): mean = compute_mean ( operand , feature_index ) mean_bcast = broadcast_in_dim ( mean , [ feature_index ], shape ( operand )) centered_operand = subtract ( operand , mean_bcast ) return compute_mean ( mul ( centered_operand , centered_operand ), feature_index ) def batch_norm_training ( operand , scale , offset , epsilon , feature_index ): mean = compute_mean ( operand , feature_index ) variance = compute_variance ( operand , feature_index ) return batch_norm_inference ( operand , scale , offset , mean , variance , epsilon , feature_index ) Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_10","text":"Name Type operand tensor of floating-point type scale 1-dimensional tensor of floating-point type offset 1-dimensional tensor of floating-point type epsilon constant of type f32 feature_index constant of type si64","title":"Inputs"},{"location":"spec/#outputs_10","text":"Name Type output tensor of floating-point type batch_mean 1-dimensional tensor of floating-point type batch_var 1-dimensional tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_9","text":"(C1) 0 \\(\\le\\) feature_index \\(\\lt\\) rank( operand ). (C2) operand , scale , offset , result , batch_mean and batch_var have the same element type. (C3) size( scale ) \\(=\\) dim(operand, feature_index) . (C4) size( offset ) \\(=\\) dim(operand, feature_index) . (C5) size( batch_mean ) \\(=\\) dim(operand, feature_index) . (C6) size( batch_var ) \\(=\\) dim(operand, feature_index) . (C7) operand and output have the same type.","title":"Constraints"},{"location":"spec/#examples_10","text":"// %operand: [ // [[1.0, 2.0], [3.0, 4.0]], // [[3.0, 4.0], [1.0, 2.0]] // ] // %scale: [1.0, 1.0] // %offset: [1.0, 1.0] %output, %batch_mean, %batch_var = \"stablehlo.batch_norm_training\"(%operand, %scale, %offset) { epsilon = 0.0 : f32, feature_index = 2 : i64 } : (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) -> (tensor<2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) // %output: [ // [[0.0, 0.0], [2.0, 2.0]], // [[2.0, 2.0], [0.0, 0.0]] // ] // %batch_mean: [2.0, 3.0] // %batch_var: [1.0, 1.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehlobitcast_convert","text":"","title":"stablehlo.bitcast_convert"},{"location":"spec/#semantics_11","text":"Performs a bitcast operation on operand tensor and produces a result tensor where the bits of the entire operand tensor are reinterpreted using the type of the result tensor. Let E and E' be the operand and result element type respectively, and R = rank(operand) : If num_bits(E') \\(=\\) num_bits(E) , bits(result[i0, ..., iR-1]) = bits(operand[i0, ..., iR-1]) . If num_bits(E') \\(\\lt\\) num_bits(E) , bits(result[i0, ..., iR-1, :]) = bits(operand[i0, ..., iR-1]) . If num_bits(E') \\(\\gt\\) num_bits(E) , bits(result[i0, ..., iR-2]) = bits(operand[i0, ..., iR-2, :]) . The behavior of bits is implementation-defined because the exact representation of tensors is implementation-defined, and the exact representation of element types is implementation-defined as well.","title":"Semantics"},{"location":"spec/#inputs_11","text":"Name Type operand tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_11","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_10","text":"(C1) Let E and E' be the operand and result element type, respectively and R = rank(operand) : If num_bits(E') \\(=\\) num_bits(E) , shape( result ) \\(=\\) shape( operand ). If num_bits(E') \\(\\lt\\) num_bits(E) : rank(result) = R+1 . dim( result , i ) \\(=\\) dim( operand , i ) for all i \\(\\in\\) [0, R -1]. dim(result, R) = num_bits(E)/num_bits(E') . If num_bits(E') \\(\\gt\\) num_bits(E) : rank(result) = R-1 . dim( result , i ) \\(=\\) dim( operand , i ) for all i \\(\\in\\) [0, R -1). dim(operand, R-1) = num_bits(E')/num_bits(E) . (C2) Conversion between complex and non-complex types is not permitted.","title":"Constraints"},{"location":"spec/#examples_11","text":"// %operand: [0.0, 1.0] %result = \"stablehlo.bitcast_convert\"(%operand) : (tensor<2xf32>) -> tensor<2x4xi8> // %result: [ // [0, 0, 0, 0], // [0, 0, -128, 63] // little-endian representation of 1.0 // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlobroadcast_in_dim","text":"","title":"stablehlo.broadcast_in_dim"},{"location":"spec/#semantics_12","text":"Expands the dimensions and/or rank of an input tensor by duplicating the data in the operand tensor and produces a result tensor. Formally, result[i0, i1, ..., iR-1] \\(=\\) operand[j0, j1, ..., jR'-1] such that jk \\(=\\) dim(operand, k) == 1 ? 0 : i[broadcast_dimensions[k]] for all dimensions k in operand .","title":"Semantics"},{"location":"spec/#inputs_12","text":"Name Type operand tensor of any supported type broadcast_dimensions 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_12","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_11","text":"(C1) operand and result have the same element type. (C2) size( broadcast_dimensions ) \\(=\\) rank( operand ). (C3) \\(0 \\le\\) broadcast_dimensions[i] \\(\\lt\\) rank( result ) for all dimensions i in operand . (C4) All dimensions in broadcast_dimensions are unique. (C5) For all dimensions j in operand : dim(operand, j) = 1 or dim(operand, j) = dim(result, i) where i = broadcast_dimensions[j] .","title":"Constraints"},{"location":"spec/#examples_12","text":"// %operand: [ // [1, 2, 3] // ] %result = \"stablehlo.broadcast_in_dim\"(%operand) { broadcast_dimensions = dense<[2, 1]>: tensor<2xi64> } : (tensor<1x3xi32>) -> tensor<2x3x2xi32> // %result: [ // [ // [1, 1], // [2, 2], // [3, 3] // ], // [ // [1, 1], // [2, 2], // [3, 3] // ] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlocase","text":"","title":"stablehlo.case"},{"location":"spec/#semantics_13","text":"Produces the output from executing exactly one function from branches depending on the value of index . Formally, if \\(0 \\le\\) index \\(\\lt\\) N-1 , output of branches[index] is returned, else, output of branches[N-1] is returned.","title":"Semantics"},{"location":"spec/#inputs_13","text":"Name Type index 1-dimensional tensor of type si32 branches variadic number of function","title":"Inputs"},{"location":"spec/#outputs_13","text":"Name Type results variadic number of tensors of any supported type or tokens","title":"Outputs"},{"location":"spec/#constraints_12","text":"(C1) branches have at least one function. (C2) All functions in branches have 0 inputs. (C3) All functions in branches have the same output types. (C4) For all i , type(results[i]) = type(branches[0]).outputs[i] .","title":"Constraints"},{"location":"spec/#examples_13","text":"// %result_branch0: 10 // %result_branch1: 11 // %index: 1 %result = \"stablehlo.case\"(%index) ({ \"stablehlo.return\"(%result_branch0) : (tensor<i32>) -> () }, { \"stablehlo.return\"(%result_branch1) : (tensor<i32>) -> () }) : (tensor<i32>) -> tensor<i32> // %result: 11 Back to Ops","title":"Examples"},{"location":"spec/#stablehlocbrt","text":"","title":"stablehlo.cbrt"},{"location":"spec/#semantics_14","text":"Performs element-wise cubic root operation on operand tensor and produces a result tensor, implementing the rootn(x, 3) operation from the IEEE-754 specification. For complex element types, it computes a complex cubic root, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_14","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_14","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_13","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_14","text":"// %operand: [0.0, 1.0, 8.0, 27.0] %result = \"stablehlo.cbrt\"(%operand) : (tensor<4xf32>) -> tensor<4xf32> // %result: [0.0, 1.0, 2.0, 3.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloceil","text":"","title":"stablehlo.ceil"},{"location":"spec/#semantics_15","text":"Performs element-wise ceil of operand tensor and produces a result tensor. Implements the roundToIntegralTowardPositive operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_15","text":"Name Type operand tensor of floating-point type","title":"Inputs"},{"location":"spec/#outputs_15","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_14","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_15","text":"// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0] %result = \"stablehlo.ceil\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-0.0, -0.0, 1.0, 1.0, 2.0] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlocholesky","text":"","title":"stablehlo.cholesky"},{"location":"spec/#semantics_16","text":"Computes the Cholesky decomposition of a batch of matrices. More formally, for all i , result[i0, ..., iR-3, :, :] is a Cholesky decomposition of a[i0, ..., iR-3, :, :] , in the form of either of a lower-triangular (if lower is true ) or upper-triangular (if lower is false ) matrix. The output values in the opposite triangle, i.e. the strict upper triangle or strict lower triangle correspondingly, are implementation-defined. If there exists i where the input matrix is not an Hermitian positive-definite matrix, then the behavior is undefined.","title":"Semantics"},{"location":"spec/#inputs_16","text":"Name Type a tensor of floating-point or complex type lower 0-dimensional tensor constant of type i1","title":"Inputs"},{"location":"spec/#outputs_16","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_15","text":"(C1) a and result have the same type. (C2) rank( a ) >= 2. (C3) dim( a , -2) = dim( a , -1).","title":"Constraints"},{"location":"spec/#examples_16","text":"// %a: [ // [1.0, 2.0, 3.0], // [2.0, 20.0, 26.0], // [3.0, 26.0, 70.0] // ] %result = \"stablehlo.cholesky\"(%a) { lower = true } : (tensor<3x3xf32>) -> tensor<3x3xf32> // %result: [ // [1.0, 0.0, 0.0], // [2.0, 4.0, 0.0], // [3.0, 5.0, 6.0] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloclamp","text":"","title":"stablehlo.clamp"},{"location":"spec/#semantics_17","text":"Clamps every element of the operand tensor between a minimum and maximum value and produces a result tensor. More formally, result[i0, ..., iR-1] = minimum(maximum(operand[i0, ..., iR-1], min_val), max_val) , where min_val = rank(min) == 0 ? min : min[i0, ..., iR-1] , max_val = rank(max) == 0 ? max : max[i0, ..., iR-1] , minimum and maximum operations correspond to stablehlo.minimum and stablehlo.maximum .","title":"Semantics"},{"location":"spec/#inputs_17","text":"Name Type min tensor of any supported type operand tensor of any supported type max tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_17","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_16","text":"(C1) Either rank(min) \\(=\\) 0 or shape(min) \\(=\\) shape(operand) . (C2) Either rank(max) \\(=\\) 0 or shape(max) \\(=\\) shape(operand) . (C3) min , operand , and max have the same element type. (C4) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_17","text":"// %min: [5, 10, 15] // %operand: [3, 13, 23] // %max: [10, 15, 20] %result = \"stablehlo.clamp\"(%min, %operand, %max) : (tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> tensor<3xi32> // %result: [5, 13, 20] Back to Ops","title":"Examples"},{"location":"spec/#stablehlocollective_permute","text":"","title":"stablehlo.collective_permute"},{"location":"spec/#semantics_18","text":"Within each process group in the StableHLO grid, sends the value of the operand tensor from the source process to the target process and produces a result tensor. The operation splits the StableHLO grid into process_groups as follows: channel_id <= 0 , cross_replica(replica_groups) . channel_id > 0 , cross_partition(replica_groups) . Afterwards, result@process is given by: operand@process_groups[i, 0] , if there exists an i such that process_groups[i, 1] = process . broadcast_in_dim(0, [], shape(result)) , otherwise.","title":"Semantics"},{"location":"spec/#inputs_18","text":"Name Type operand tensor of any supported type source_target_pairs 2-dimensional tensor constant of type si64 channel_id constant of type si64","title":"Inputs"},{"location":"spec/#outputs_18","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_17","text":"(C1) dim( source_target_pairs , 1) \\(=\\) 2. (C2) All values in source_target_pairs[:, 0] are unique. (C3) All values in source_target_pairs[:, 1] are unique. (C4) \\(0 \\le\\) source_target_pairs[i][0], source_target_pairs[i][1] \\(\\lt N\\) , where \\(N\\) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_partition , num_partitions . (C5) type( result ) \\(=\\) type( operand ).","title":"Constraints"},{"location":"spec/#examples_18","text":"// num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [[1, 2], [3, 4]] // %operand@(1, 0): [[5, 6], [7, 8]] %result = \"stablehlo.collective_permute\"(%operand) { source_target_pairs = dense<[[0, 1]]> : tensor<2x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> } : (tensor<2x2xf32>) -> tensor<2x2xf32> // // %result@(0, 0): [[0, 0], [0, 0]] // %result@(1, 0): [[1, 2], [3, 4]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlocompare","text":"","title":"stablehlo.compare"},{"location":"spec/#semantics_19","text":"Performs element-wise comparison of lhs and rhs tensors according to comparison_direction and compare_type , and produces a result tensor. The values of comparison_direction and compare_type have the following semantics: For integer and boolean element types: EQ : lhs \\(=\\) rhs . NE : lhs \\(\\ne\\) rhs . GE : lhs \\(\\ge\\) rhs . GT : lhs \\(\\gt\\) rhs . LE : lhs \\(\\le\\) rhs . LT : lhs \\(\\lt\\) rhs . For floating-point element types and compare_type = FLOAT , the op implements the following IEEE-754 operations: EQ : compareQuietEqual . NE : compareQuietNotEqual . GE : compareQuietGreaterEqual . GT : compareQuietGreater . LE : compareQuietLessEqual . LT : compareQuietLess . For floating-point element types and compare_type = TOTALORDER , the op uses the combination of totalOrder and compareQuietEqual operations from IEEE-754. For complex element types, lexicographic comparison of (real, imag) pairs is performed using the provided comparison_direction and compare_type .","title":"Semantics"},{"location":"spec/#inputs_19","text":"Name Type lhs tensor of any supported type rhs tensor of any supported type comparison_direction enum of EQ , NE , GE , GT , LE , and LT compare_type enum of FLOAT , TOTALORDER , SIGNED , and UNSIGNED","title":"Inputs"},{"location":"spec/#outputs_19","text":"Name Type result tensor of boolean type","title":"Outputs"},{"location":"spec/#constraints_18","text":"(C1) lhs and rhs have the same element type. (C2) lhs , rhs , and result have the same shape. (C3) Given E is the lhs element type, the following are legal values of compare_type : If E is signed integer type, compare_type = SIGNED . If E is unsigned integer or boolean type, compare_type = UNSIGNED . If E is floating-point type, compare_type \\(\\in\\) { FLOAT , TOTALORDER }. If E is complex type, compare_type = FLOAT .","title":"Constraints"},{"location":"spec/#examples_19","text":"// %lhs: [1.0, 3.0] // %rhs: [1.1, 2.9] %result = \"stablehlo.compare\"(%lhs, %rhs) { comparison_direction = #stablehlo<comparison_direction LT>, compare_type = #stablehlo<comparison_type FLOAT> } : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xi1> // %result: [true, false] Back to Ops","title":"Examples"},{"location":"spec/#stablehlocomplex","text":"","title":"stablehlo.complex"},{"location":"spec/#semantics_20","text":"Performs element-wise conversion to a complex value from a pair of real and imaginary values, lhs and rhs , and produces a result tensor.","title":"Semantics"},{"location":"spec/#inputs_20","text":"Name Type lhs tensor of type f32 or f64 rhs tensor of type f32 or f64","title":"Inputs"},{"location":"spec/#outputs_20","text":"Name Type result tensor of complex type","title":"Outputs"},{"location":"spec/#constraints_19","text":"(C1) lhs and rhs have the same type. (C2) shape( result ) \\(=\\) shape( lhs ). (C3) element_type( result ) = complex_type(element_type( lhs )).","title":"Constraints"},{"location":"spec/#examples_20","text":"// %lhs: [1.0, 3.0] // %rhs: [2.0, 4.0] %result = \"stablehlo.complex\"(%lhs, %rhs) : (tensor<2xf32>, tensor<2xf32>) -> tensor<2xcomplex<f32>> // %result: [(1.0, 2.0), (3.0, 4.0)] Back to Ops","title":"Examples"},{"location":"spec/#stablehloconcatenate","text":"","title":"stablehlo.concatenate"},{"location":"spec/#semantics_21","text":"Concatenates a variadic number of tensors in inputs along dimension dimension in the same order as the given arguments and produces a result tensor. More formally, result[i0, ..., id, ..., iR-1] = inputs[k][i0, ..., kd, ..., iR-1] , where: id = d0 + ... + dk-1 + kd . d is equal to dimension , and d0 , ... are d th dimension sizes of inputs .","title":"Semantics"},{"location":"spec/#inputs_21","text":"Name Type inputs variadic number of tensors of any supported type dimension constant of type si64","title":"Inputs"},{"location":"spec/#outputs_21","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_20","text":"(C1) All tensors in inputs have the same element type. (C2) All tensors in inputs have the same shape except for the size of the dimension th dimension. (C3) inputs have N tensors where N >= 1. (C4) 0 \\(\\le\\) dimension \\(\\lt\\) rank of inputs[0] . (C5) result has the same element type as the tensors in inputs . (C6) result has the same shape as the tensors in inputs except for the size of the dimension th dimension, which is calculated as a sum of the size of inputs[k][dimension] for all k in inputs .","title":"Constraints"},{"location":"spec/#examples_21","text":"// %input0: [[1, 2], [3, 4], [5, 6]] // %input1: [[7, 8]] %result = \"stablehlo.concatenate\"(%input0, %input1) { dimension = 0 : i64 } : (tensor<3x2xi32>, tensor<1x2xi32>) -> tensor<4x2xi32> // %result: [[1, 2], [3, 4], [5, 6], [7, 8]] Back to Ops","title":"Examples"},{"location":"spec/#stablehloconstant","text":"","title":"stablehlo.constant"},{"location":"spec/#semantics_22","text":"Produces an output tensor from a constant value .","title":"Semantics"},{"location":"spec/#inputs_22","text":"Name Type value constant of any supported type","title":"Inputs"},{"location":"spec/#outputs_22","text":"Name Type output tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_21","text":"(C1) value and output have the same type.","title":"Constraints"},{"location":"spec/#examples_22","text":"%output = \"stablehlo.constant\"() { value = dense<[[0.0, 1.0], [2.0, 3.0]]> : tensor<2x2xf32> } : () -> tensor<2x2xf32> // %output: [[0.0, 1.0], [2.0, 3.0]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehloconvert","text":"","title":"stablehlo.convert"},{"location":"spec/#semantics_23","text":"Performs an element-wise conversion from one element type to another on operand tensor and produces a result tensor. For conversions involving integer-to-integer , if there is an unsigned/signed overflow, the result is implementation-defined and one of the following: * mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . * saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For conversions involving floating-point-to-floating-point or integer-to-floating-point , if the source value can be exactly represented in the destination type, the result value is that exact representation. Otherwise, the behavior is TBD. Conversion involving complex-to-complex follows the same behavior of floating-point-to-floating-point conversions for converting real and imaginary parts. For conversions involving floating-point-to-complex or complex-to-floating-point , the destination imaginary value is zeroed or the source imaginary value is ignored, respectively. The conversion of the real part follows the floating-point-to-floating-point conversion. Conversions involving integer-to-complex follows the same behavior as integer-to-floating-point conversion while converting the source integer to destination real part. The destination imaginary part is zeroed. For conversions involving floating-point-to-integer , the fractional part is truncated. If the truncated value cannot be represented in the destination type, the behavior is TBD. Conversions involving complex-to-integer follows the same behavior while converting the source real part to destination integer. The source imaginary part is ignored. For boolean-to-any-supported-type conversions, the value false is converted to zero, and the value true is converted to one. For any-supported-type-to-boolean conversions, a zero value is converted to false and any non-zero value is converted to true .","title":"Semantics"},{"location":"spec/#inputs_23","text":"Name Type operand tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_23","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_22","text":"(C1) operand and result have the same shape.","title":"Constraints"},{"location":"spec/#examples_23","text":"// %operand: [1, 2, 3] %result = \"stablehlo.convert\"(%operand) : (tensor<3xi32>) -> tensor<3xcomplex<f32>> // %result: [(1.0, 0.0), (2.0, 0.0), (3.0, 0.0)] Back to Ops","title":"Examples"},{"location":"spec/#stablehloconvolution","text":"","title":"stablehlo.convolution"},{"location":"spec/#semantics_24","text":"Computes dot products between windows of lhs and slices of rhs and produces result . The following diagram shows how elements in result are computed from lhs and rhs using a concrete example. More formally, we start with reframing the inputs to the operation in terms of lhs in order to be able to express windows of lhs : lhs_window_dimensions = lhs_shape(dim(lhs, input_batch_dimension), dim(rhs, kernel_spatial_dimensions), dim(lhs, input_feature_dimension)) . lhs_window_strides = lhs_shape(1, window_strides, 1) . lhs_padding = lhs_shape([0, 0], padding, [0, 0]) . lhs_base_dilations = lhs_shape(1, lhs_dilation, 1) . lhs_window_dilations = lhs_shape(1, rhs_dilation, 1) . This reframing uses the following helper functions: lhs_shape(n, hw, c) = permute([n] + hw + [c], [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension]) . result_shape(n1, hw, c1) = permute([n1] + hw + [c1], [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension]) . If feature_group_count = 1 and batch_group_count = 1 , then for all output_spatial_index in the index space of dim(result, output_spatial_dimensions) , result[result_shape(:, output_spatial_index, :)] = dot_product where: padded_lhs = pad(lhs, 0, lhs_padding[:, 0], lhs_padding[:, 1], lhs_base_dilations) . lhs_window_start = lhs_shape(0, output_spatial_index, 0) * lhs_window_strides . lhs_window = slice(padded_lhs, lhs_window_start, lhs_window_start + lhs_window_dimensions, lhs_window_dilations) . reversed_lhs_window = reverse(lhs_window, [input_spatial_dimensions[dim] for dim in [0, size(window_reversal) and window_reversal[dim] = true]) . dot_product = dot_general(reversed_lhs_window, rhs, lhs_batching_dimensions=[], lhs_contracting_dimensions=input_spatial_dimensions + [input_feature_dimension], rhs_batching_dimensions=[], rhs_contracting_dimensions=kernel_spatial_dimensions + [kernel_input_feature_dimension]) . If feature_group_count > 1 : lhses = split(lhs, feature_group_count, input_feature_dimension) . rhses = split(rhs, feature_group_count, kernel_output_feature_dimension) . results[:] = convolution(lhses[:], rhses[:], ..., feature_group_count=1, ...) . result = concatenate(results, output_feature_dimension) . If batch_group_count > 1 : lhses = split(lhs, batch_group_count, input_batch_dimension) . rhses = split(rhs, batch_group_count, kernel_output_feature_dimension) . results[:] = convolution(lhses[:], rhses[:], ..., batch_group_count=1, ...) . result = concatenate(results, output_feature_dimension) .","title":"Semantics"},{"location":"spec/#inputs_24","text":"Name Type Constraints lhs tensor of any supported type (C1), (C2), (C11), (C12), (C26), (C27) rhs tensor of any supported type (C1), (C2), (C15), (C16), (C17), (C26) window_strides 1-dimensional tensor constant of type si64 (C3), (C4), (C26) padding 2-dimensional tensor constant of type si64 (C5), (C26) lhs_dilation 1-dimensional tensor constant of type si64 (C6), (C7), (C26) rhs_dilation 1-dimensional tensor constant of type si64 (C8), (C9), (C26) window_reversal 1-dimensional tensor constant of type boolean (C10) input_batch_dimension constant of type si64 (C11), (C14), (C26) input_feature_dimension constant of type si64 (C12), (C14) input_spatial_dimensions 1-dimensional tensor constant of type si64 (C13), (C14), (C26) kernel_input_feature_dimension constant of type si64 (C15), (C19) kernel_output_feature_dimension constant of type si64 (C16), (C17), (C19), (C26) kernel_spatial_dimensions 1-dimensional tensor constant of type si64 (C18), (C19), (C26) output_batch_dimension constant of type si64 (C21), (C26) output_feature_dimension constant of type si64 (C21), (C26) output_spatial_dimensions 1-dimensional tensor constant of type si64 (C20), (C21), (C26) feature_group_count constant of type si64 (C12), (C15), (C17), (C22), (C24) batch_group_count constant of type si64 (C11), (C16), (C23), (C24), (C26) precision_config variadic number of enum of DEFAULT , HIGH , and HIGHEST (C25)","title":"Inputs"},{"location":"spec/#outputs_24","text":"Name Type Constraints result tensor of any supported type (C26), (C27), (C28)","title":"Outputs"},{"location":"spec/#constraints_23","text":"(C1) \\(N =\\) rank( lhs ) \\(=\\) rank( rhs ). (C2) element_type( lhs ) \\(=\\) element_type( rhs ). (C3) size( window_strides ) \\(= N - 2\\) . (C4) window_strides[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_strides )). (C5) dim( padding , 0) \\(= N - 2\\) and dim( padding , 1) = 2. (C6) size( lhs_dilation ) \\(= N - 2\\) . (C7) lhs_dilation[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( lhs_dilation )). (C8) size( rhs_dilation ) \\(= N - 2\\) . (C9) rhs_dilation[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( rhs_dilation )). (C10) size( window_reversal ) \\(= N - 2\\) . (C11) dim(lhs, input_batch_dimension) % batch_group_count = 0 . (C12) `dim(lhs, input_feature_dimension) % feature_group_count = 0. (C13) size( input_spatial_dimensions ) \\(= N - 2\\) . (C14) Given input_dimensions = [input_batch_dimension] + input_spatial_dimensions + [input_feature_dimension] . All dimensions in input_dimensions are unique. For any i \\(\\in\\) input_dimensions , 0 \\(\\le\\) i \\(\\lt\\) N. (C15) dim(rhs, kernel_input_feature_dimension = dim(lhs, input_feature_dimension) / feature_group_count . (C16) dim(rhs, kernel_output_feature_dimension) % batch_group_count = 0 . (C17) dim(rhs, kernel_output_feature_dimension) % feature_group_count = 0 . (C18) size( kernel_spatial_dimensions ) \\(= N - 2\\) . (C19) Given kernel_dimensions = kernel_spatial_dimensions + [kernel_input_feature_dimension] + [kernel_output_feature_dimension] . All dimensions in kernel_dimensions are unique. For any i \\(\\in\\) kernel_dimensions , 0 \\(\\le\\) i \\(\\lt\\) N. (C20) size( output_spatial_dimensions ) \\(= N - 2\\) . (C21) Given output_dimensions = [output_batch_dimension] + output_spatial_dimensions + [output_feature_dimension] . All dimensions in output_dimensions are unique. For any i \\(\\in\\) output_dimensions , 0 \\(\\le\\) i \\(\\lt\\) N. (C22) feature_group_count > 0 . (C23) batch_group_count > 0 . (C24) feature_group_count \\(= 1\\) OR batch_group_count \\(= 1\\) . (C25) size( precision_config ) \\(=\\) 2. (C26) For result_dim \\(\\in\\) [0, N), dim(result, result_dim) is given by dim(lhs, input_batch_dimension) / batch_group_count , if result_dim = output_batch_dimension . dim(rhs, kernel_output_feature_dimension) , if result_dim = output_feature_dimension . num_windows otherwise, where: output_spatial_dimensions[spatial_dim] = result_dim . lhs_dim = input_spatial_dimensions[spatial_dim] . rhs_dim = kernel_spatial_dimensions[spatial_dim] . dilated_input_shape[lhs_dim] = dim(lhs, lhs_dim) == 0 ? 0 : (dim(lhs, lhs_dim) - 1) * lhs_dilation[spatial_dim] + 1 . padded_input_shape[lhs_dim] = padding[spatial_dim, 0] + dilated_input_shape[lhs_dim] + padding[spatial_dim, 1] . dilated_window_shape[lhs_dim] = dim(rhs, rhs_dim) == 0 ? 0 : (dim(rhs, rhs_dim) - 1) * rhs_dilation[spatial_dim] + 1 . num_windows = (padded_input_shape[lhs_dim] == 0 || dilated_window_shape[lhs_dim] > padded_input_shape[lhs_dim]) ? 0 : floor((padded_input_shape[lhs_dim] - dilated_window_shape[lhs_dim]) / window_strides[spatial_dim]) + 1 . (C27) element_type( result ) \\(=\\) element_type( lhs ). (C28) rank( result ) \\(= N\\) .","title":"Constraints"},{"location":"spec/#examples_24","text":"// %lhs: [[ // [ // [1], [2], [5], [6] // ], // [ // [3], [4], [7], [8] // ], // [ // [10], [11], [14], [15] // ], // [ // [12], [13], [16], [17] // ] // ]] // // %rhs : [ // [[[1]], [[1]], [[1]]], // [[[1]], [[1]], [[1]]], // [[[1]], [[1]], [[1]]] // ] %result = \"stablehlo.convolution\"(%lhs, %rhs) { window_strides = dense<4> : tensor<2xi64>, padding = dense<0> : tensor<2x2xi64>, lhs_dilation = dense<2> : tensor<2xi64>, rhs_dilation = dense<1> : tensor<2xi64>, window_reversal = dense<false> : tensor<2xi1>, // In the StableHLO dialect, dimension numbers are encoded via: // `[<input dimensions>]x[<kernel dimensions>]->[output dimensions]`. // \"b\" is batch dimenion, \"f\" is feature dimension, // \"i\" is input feature dimension, \"o\" is output feature dimension, // \"0/1/etc\" are spatial dimensions. dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>, feature_group_count = 1 : i64, batch_group_count = 1 : i64, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>] } : (tensor<1x4x4x1xi32>, tensor<3x3x1x1xi32>) -> tensor<1x2x2x1xi32> // %result: [[ // [[10], [26]], // [[46], [62]] // ]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlocosine","text":"","title":"stablehlo.cosine"},{"location":"spec/#semantics_25","text":"Performs element-wise cosine operation on operand tensor and produces a result tensor, implementing the cos operation from the IEEE-754 specification. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_25","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_25","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_24","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_25","text":"// %operand: [ // [0.0, 1.57079632], // [0, pi/2] // [3.14159265, 4.71238898] // [pi, 3pi/2] // ] %result = \"stablehlo.cosine\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 0.0], [-1.0, 0.0]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlocount_leading_zeros","text":"","title":"stablehlo.count_leading_zeros"},{"location":"spec/#semantics_26","text":"Performs element-wise count of the number of leading zero bits in the operand tensor and produces a result tensor.","title":"Semantics"},{"location":"spec/#inputs_26","text":"Name Type operand tensor of integer type","title":"Inputs"},{"location":"spec/#outputs_26","text":"Name Type result tensor of integer type","title":"Outputs"},{"location":"spec/#constraints_25","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_26","text":"// %operand: [[0, 1], [127, -1]] %result = \"stablehlo.count_leading_zeros\"(%operand) : (tensor<2x2xi8>) -> tensor<2x2xi8> // %result: [[8, 7], [1, 0]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlocustom_call","text":"","title":"stablehlo.custom_call"},{"location":"spec/#semantics_27","text":"Encapsulates an implementation-defined operation call_target_name that takes inputs and called_computations and produces results . has_side_effect , backend_config and api_version may be used to provide additional implementation-defined metadata.","title":"Semantics"},{"location":"spec/#inputs_27","text":"Name Type inputs variadic number of values of any supported type call_target_name constant of type string has_side_effect constant of type i1 backend_config constant of type string api_version constant of type si32 called_computations variadic number of function","title":"Inputs"},{"location":"spec/#outputs_27","text":"Name Type results variadic number of values of any supported type","title":"Outputs"},{"location":"spec/#examples_27","text":"%results = \"stablehlo.custom_call\"(%input0) { call_target_name = \"foo\", has_side_effect = false, backend_config = \"bar\", api_version = 1 : i32, called_computations = [@foo] } : (tensor<f32>) -> tensor<f32> Back to Ops","title":"Examples"},{"location":"spec/#stablehlodivide","text":"","title":"stablehlo.divide"},{"location":"spec/#semantics_28","text":"Performs element-wise division of dividend lhs and divisor rhs tensors and produces a result tensor. For floating-point element types, it implements the division operation from IEEE-754 specification. For integer element types, it implements integer division truncating any fractional part. For n-bit integer types, division overflow (division by zero or division of \\(-2^{n-1}\\) with \\(-1\\) ) produces an implementation-defined value.","title":"Semantics"},{"location":"spec/#inputs_28","text":"Name Type lhs tensor of integer, floating-point or complex type rhs tensor of integer, floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_28","text":"Name Type result tensor of integer, floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_26","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_28","text":"// %lhs: [17.1, -17.1, 17.1, -17.1] // %rhs: [3.0, 3.0, -3.0, -3.0] %result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32> // %result: [5.66666651, -5.66666651, -5.66666651, 5.66666651] // %lhs: [17, -17, 17, -17] // %rhs: [3, 3, -3, -3] %result = \"stablehlo.divide\"(%lhs, %rhs) : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32> // %result: [5, -5, -5, 5] Back to Ops","title":"Examples"},{"location":"spec/#stablehlodot_general","text":"","title":"stablehlo.dot_general"},{"location":"spec/#semantics_29","text":"Computes dot products between slices of lhs and slices of rhs and produces a result tensor. More formally, result[result_index] = dot_product , where: lhs_result_dimensions = [d for d in axes(lhs) and d not in lhs_batching_dimensions and d not in lhs_contracting_dimensions] . rhs_result_dimensions = [d for d in axes(rhs) and d not in rhs_batching_dimensions and d not in rhs_contracting_dimensions] . result_batching_index + result_lhs_index + result_rhs_index = result_index where size(result_batching_index) = size(lhs_batching_dimensions) , size(result_lhs_index) = size(lhs_result_dimensions) and size(result_rhs_index) = size(rhs_result_dimensions) . transposed_lhs = transpose(lhs, lhs_batching_dimensions + lhs_result_dimensions + lhs_contracting_dimensions) . transposed_lhs_slice = slice(result_batching_index + result_lhs_index + [:, ..., :]) . reshaped_lhs_slice = reshape(transposed_lhs_slice, dims(lhs, lhs_contracting_dimensions)) . transposed_rhs = transpose(rhs, rhs_batching_dimensions + rhs_result_dimensions + rhs_contracting_dimensions) . transposed_rhs_slice = slice(result_batching_index + result_rhs_index + [:, ..., :]) . reshaped_rhs_slice = reshape(transposed_rhs_slice, dims(rhs, rhs_contracting_dimensions)) . dot_product = reduce( inputs=[multiply(reshaped_lhs_slice, reshaped_rhs_slice)], init_values=[0], dimensions=[0, ..., size(lhs_contracting_dimensions) - 1], body=lambda x, y: add(x, y)) . precision_config controls the tradeoff between speed and accuracy for computations on accelerator backends. This can be one of the following: DEFAULT : Fastest calculation, but least accurate approximation to the original number. HIGH : Slower calculation, but more accurate approximation to the original number. HIGHEST : Slowest calculation, but most accurate approximation to the original number.","title":"Semantics"},{"location":"spec/#inputs_29","text":"Name Type lhs tensor of any supported type rhs tensor of any supported type lhs_batching_dimensions 1-dimensional tensor constant of type si64 rhs_batching_dimensions 1-dimensional tensor constant of type si64 lhs_contracting_dimensions 1-dimensional tensor constant of type si64 rhs_contracting_dimensions 1-dimensional tensor constant of type si64 precision_config variadic number of enum of DEFAULT , HIGH , and HIGHEST","title":"Inputs"},{"location":"spec/#outputs_29","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_27","text":"(C1) lhs and rhs have the same element type. (C2) size( lhs_batching_dimensions ) \\(=\\) size( rhs_batching_dimensions ). (C3) size( lhs_contracting_dimensions ) \\(=\\) size( rhs_contracting_dimensions ). (C4) lhs_batching_dimensions and lhs_contracting_dimensions combined are unique. (C5) rhs_batching_dimensions and rhs_contracting_dimensions combined are unique. (C6) 0 \\(\\le\\) lhs_batching_dimensions[i] \\(\\lt\\) rank( lhs ) for all i \\(\\in\\) [0, size( lhs_batching_dimensions )). (C7) 0 \\(\\le\\) lhs_contracting_dimensions[i] \\(\\lt\\) rank( lhs ) for all i \\(\\in\\) [0, size( lhs_contracting_dimensions )). (C8) 0 \\(\\le\\) rhs_batching_dimensions[d] \\(\\lt\\) rank( rhs ) for all i \\(\\in\\) [0, size( rhs_batching_dimensions )). (C9) 0 \\(\\le\\) rhs_contracting_dimensions[d] \\(\\lt\\) rank( rhs ) for all i \\(\\in\\) [0, size( rhs_contracting_dimensions )). (C10) dim( lhs , lhs_batching_dimensions[i] ) \\(=\\) dim( rhs , rhs_batching_dimensions[i] ) for all i \\(\\in\\) [0, size( lhs_batching_dimensions )). (C11) dim( lhs , lhs_contracting_dimensions[i] ) \\(=\\) dim( rhs , rhs_contracting_dimensions[i] ) for all i \\(\\in\\) [0, size( lhs_contracting_dimensions )). (C12) size( precision_config ) \\(=\\) 2. (C13) shape( result ) \\(=\\) dim( lhs , lhs_batching_dimensions ) + dim( lhs , lhs_result_dimensions ) + dim( rhs , rhs_result_dimensions ).","title":"Constraints"},{"location":"spec/#examples_29","text":"// %lhs: [ // [[1, 2], // [3, 4]], // [[5, 6], // [7, 8]] // ] // %rhs: [ // [[1, 0], // [0, 1]], // [[1, 0], // [0, 1]] // ] %result = \"stablehlo.dot_general\"(%lhs, %rhs) { dot_dimension_numbers = #stablehlo.dot< lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1] >, precision_config = [#stablehlo<precision DEFAULT>, #stablehlo<precision DEFAULT>] } : (tensor<2x2x2xi32>, tensor<2x2x2xi32>) -> tensor<2x2x2xi32> // %result: [ // [[1, 2], // [3, 4]], // [[5, 6], // [7, 8]] // ] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlodynamic_slice","text":"","title":"stablehlo.dynamic_slice"},{"location":"spec/#semantics_30","text":"Extracts a slice from the operand using dynamically-computed starting indices and produces a result tensor. start_indices contain the starting indices of the slice for each dimension subject to potential adjustment, and slice_sizes contain the sizes of the slice for each dimension. More formally, result[i0, ..., iR-1] = operand[j0, ..., jR-1] where: jd = adjusted_start_indices[d][] + id . adjusted_start_indices = clamp(0, start_indices, shape(operand) - slice_sizes) .","title":"Semantics"},{"location":"spec/#inputs_30","text":"Name Type operand tensor of any supported type start_indices variadic number of 0-dimensional tensors of integer type slice_sizes 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_30","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_28","text":"(C1) operand and result have the same element type. (C2) size( start_indices ) \\(=\\) size( slice_sizes ) \\(=\\) rank( operand ). (C3) All start_indices have the same type. (C4) slice_sizes[k] \\(\\in\\) [0, dim( operand , k )) for all k \\(\\in\\) [0, rank( operand )). (C5) shape( result ) \\(=\\) slice_sizes .","title":"Constraints"},{"location":"spec/#examples_30","text":"// %operand: [ // [0, 0, 1, 1], // [0, 0, 1, 1], // [0, 0, 0, 0], // [0, 0, 0, 0] // ] // %start_indices0: -1 // %start_indices1: 3 %result = \"stablehlo.dynamic_slice\"(%operand, %start_indices0, %start_indices1) { slice_sizes = dense<[2, 2]> : tensor<2xi64> } : (tensor<4x4xi32>, tensor<i64>, tensor<i64>) -> tensor<2x2xi32> // %result: [ // [1, 1], // [1, 1] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlodynamic_update_slice","text":"","title":"stablehlo.dynamic_update_slice"},{"location":"spec/#semantics_31","text":"Produces a result tensor which is equal to the operand tensor except that the slice starting at start_indices is updated with the values in update . More formally, result[i0, ..., iR-1] is defined as: update[j0, ..., jR-1] if jd = adjusted_start_indices[d][] + id where adjusted_start_indices = clamp(0, start_indices, shape(operand) - update) . operand[i0, ..., iR-1] otherwise.","title":"Semantics"},{"location":"spec/#inputs_31","text":"Name Type operand tensor of any supported type update tensor of any supported type start_indices variadic number of 0-dimensional tensors of integer type","title":"Inputs"},{"location":"spec/#outputs_31","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_29","text":"(C1) operand and result have the same type. (C2) element_type( update ) \\(=\\) element_type( operand ). (C3) rank( update ) \\(=\\) rank( operand ). (C4) size( start_indices ) \\(=\\) rank( operand ). (C5) All start_indices have the same type. (C6) dim( update , k ) \\(\\in\\) [0, dim( operand , k )] for all k \\(\\in\\) [0, rank( operand )).","title":"Constraints"},{"location":"spec/#examples_31","text":"// %operand: [ // [1, 1, 0, 0], // [1, 1, 0, 0], // [1, 1, 1, 1], // [1, 1, 1, 1] // ] // %update: [ // [1, 1], // [1, 1] // ] // %start_indices0: -1 // %start_indices1: 3 %result = \"stablehlo.dynamic_update_slice\"(%operand, %update, %start_indices0, %start_indices1) : (tensor<4x4xi32>, tensor<2x2xi32>, tensor<i64>, tensor<i64>) -> tensor<4x4xi32> // %result: [ // [1, 1, 1, 1], // [1, 1, 1, 1], // [1, 1, 1, 1], // [1, 1, 1, 1] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloexponential","text":"","title":"stablehlo.exponential"},{"location":"spec/#semantics_32","text":"Performs element-wise exponential operation on operand tensor and produces a result tensor. For floating-point element types, it implements the exp operation from the IEEE-754 specification. For complex element types, it computes a complex exponential, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_32","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_32","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_30","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_32","text":"// %operand: [[0.0, 1.0], [2.0, 3.0]] %result = \"stablehlo.exponential\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 2.71828183], [7.38905610, 20.08553692]] // %operand: (1.0, 2.0) %result = \"stablehlo.exponential\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: (-1.13120438, 2.47172667) Back to Ops","title":"Examples"},{"location":"spec/#stablehloexponential_minus_one","text":"","title":"stablehlo.exponential_minus_one"},{"location":"spec/#semantics_33","text":"Performs element-wise exponential minus one operation on operand tensor and produces a result tensor. For floating-point element types, it implements the expm1 operation from the IEEE-754 specification. For complex element types, it computes a complex exponential minus one, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_33","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_33","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_31","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_33","text":"// %operand: [0.0, 1.0] %result = \"stablehlo.exponential_minus_one\"(%operand) : (tensor<2xf32>) -> tensor<2xf32> // %result: [0.0, 1.71828187] Back to Ops","title":"Examples"},{"location":"spec/#stablehlofft","text":"","title":"stablehlo.fft"},{"location":"spec/#semantics_34","text":"Performs the forward and inverse Fourier transforms for real and complex inputs/outputs. fft_type is one of the following: FFT : Forward complex-to-complex FFT. IFFT : Inverse complex-to-complex FFT. RFFT : Forward real-to-complex FFT. IRFFT : Inverse real-to-complex FFT (i.e. takes complex, returns real). More formally, given the function fft which takes 1-dimensional tensors of complex types as input, produces 1-dimensional tensors of same types as output and computes the discrete Fourier transform: For fft_type = FFT , result is defined as the final result of a series of L computations where L = size(fft_length) . For example, for L = 3 : result1[i0, ..., :] = fft(operand[i0, ..., :]) for all i . result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1]) for all i . Furthermore, given the function ifft which has the same type signature and computes the inverse of fft : For fft_type = IFFT , result is defined as the inverse of the computations for fft_type = FFT . For example, for L = 3 : result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1]) for all i . result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :] = ifft(result2[i0, ..., :]) for all i . Furthermore, given the function rfft which takes 1-dimensional tensors of floating-point types, produces 1-dimensional tensors of complex types of the same floating-point semantics and works as follows: rfft(real_operand) = truncated_result where complex_operand[i] = (real_operand, 0) for all i . complex_result = fft(complex_operand) . truncated_result = complex_result[:(rank(complex_result) / 2 + 1)] . (When the discrete Fourier transform is computed for real operands, the first N/2 + 1 elements of the result unambiguously define the rest of the result, so the result of rfft is truncated to avoid computing redundant elements). For fft_type = RFFT , result is defined as the final result of a series of L computations where L = size(fft_length) . For example, for L = 3 : result1[i0, ..., :] = rfft(operand[i0, ..., :]) for all i . result2[i0, ..., :, iR-1] = fft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :, iR-2, iR-1] = fft(result2[i0, ..., :, iR-2, iR-1]) for all i . Finally, given the function irfft which has the same type signature and computes the inverse of rfft : For fft_type = IRFFT , result is defined as the inverse of the computations for fft_type = RFFT . For example, for L = 3 : result1[i0, ..., :, iR-2, iR-1] = ifft(operand[i0, ..., :, iR-2, iR-1]) for all i . result2[i0, ..., :, iR-1] = ifft(result1[i0, ..., :, iR-1]) for all i . result[i0, ..., :] = irfft(result2[i0, ..., :]) for all i .","title":"Semantics"},{"location":"spec/#inputs_34","text":"Name Type operand tensor of floating-point or complex type fft_type enum of FFT , IFFT , RFFT , and IRFFT fft_length 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_34","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_32","text":"(C1) rank(operand) \\(\\ge\\) size(fft_length) . (C2) The relationship between operand and result element types varies: If fft_type = FFT , element_type(operand) and element_type(result) have the same complex type. If fft_type = IFFT , element_type(operand) and element_type(result) have the same complex type. If fft_type = RFFT , element_type(operand) is a floating-point type and element_type(result) is a complex type of the same floating-point semantics. If fft_type = IRFFT , element_type(operand) is a complex type and element_type(result) is a floating-point type of the same floating-point semantics. (C3) 1 \\(\\le\\) size(fft_length) \\(\\le\\) 3. (C4) If among operand and result , there is a tensor real of a floating-type type, then dims(real)[-size(fft_length):] = fft_length . (C5) dim(result, d) = dim(operand, d) for all d , except for: If fft_type = RFFT , dim(result, -1) = dim(operand, -1) == 0 ? 0 : dim(operand, -1) / 2 + 1 . If fft_type = IRFFT , dim(operand, -1) = dim(result, -1) == 0 ? 0 : dim(result, -1) / 2 + 1 .","title":"Constraints"},{"location":"spec/#examples_34","text":"// %operand: [(1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)] %result = \"stablehlo.fft\"(%operand) { fft_type = #stablehlo<fft_type FFT>, fft_length = dense<4> : tensor<1xi64> } : (tensor<4xcomplex<f32>>) -> tensor<4xcomplex<f32>> // %result: [(1.0, 0.0), (1.0, 0.0), (1.0, 0.0), (1.0, 0.0)] Back to Ops","title":"Examples"},{"location":"spec/#stablehlofloor","text":"","title":"stablehlo.floor"},{"location":"spec/#semantics_35","text":"Performs element-wise floor of operand tensor and produces a result tensor. Implements the roundToIntegralTowardNegative operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_35","text":"Name Type operand tensor of floating-point type","title":"Inputs"},{"location":"spec/#outputs_35","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_33","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_35","text":"// %operand: [-0.8166, -0.2530, 0.2530, 0.8166, 2.0] %result = \"stablehlo.floor\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-1.0, -1.0, 0.0, 0.0, 2.0] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlogather","text":"","title":"stablehlo.gather"},{"location":"spec/#semantics_36","text":"Gathers slices from operand tensor from offsets specified in start_indices and produces a result tensor. The following diagram shows how elements in result map on elements in operand using a concrete example. The diagram picks a few example result indices and explains in detail which operand indices they correspond to. More formally, result[result_index] = operand[operand_index] where: batch_dims = [ d for d in axes(result) and d not in offset_dims ]. batch_index = [ result_index[d] for d in batch_dims ]. start_index = start_indices[bi0, ..., :, ..., biN] where bi are individual elements in batch_index and : is inserted at the index_vector_dim index, if index_vector_dim < rank(start_indices) . [start_indices[batch_index]] otherwise. For do in axes(operand) , full_start_index[do] = start_index[ds] if do = start_index_map[ds] . full_start_index[do] = 0 otherwise. offset_index = [ result_index[d] for d in offset_dims ]. full_offset_index = [oi0, ..., 0, ..., oiN] where oi are individual elements in offset_index , and 0 is inserted at indices from collapsed_slice_dims . operand_index = add(full_start_index, full_offset_index) . If operand_index is out of bounds for operand , then the behavior is implementation-defined. If indices_are_sorted is true then the implementation can assume that start_indices are sorted with respect to start_index_map , otherwise the behavior is undefined. More formally, for all id < jd from indices(result) , full_start_index(id) <= full_start_index(jd) .","title":"Semantics"},{"location":"spec/#inputs_36","text":"Name Type Constraints operand tensor of any supported type (C1), (C10), (C11), (C12), (C15) start_indices tensor of any supported integer type (C2), (C3), (C13) offset_dims 1-dimensional tensor constant of type si64 (C1), (C4), (C5), collapsed_slice_dims 1-dimensional tensor constant of type si64 (C1), (C6), (C7), (C8), (C13) start_index_map 1-dimensional tensor constant of type si64 (C3), (C9), (C10) index_vector_dim constant of type si64 (C2), (C3), (C13) slice_sizes 1-dimensional tensor constant of type si64 (C7), (C8), (C11), (C12), (C13) indices_are_sorted constant of type i1","title":"Inputs"},{"location":"spec/#outputs_36","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_34","text":"(C1) rank( operand ) \\(=\\) size( offset_dims ) \\(+\\) size( collapsed_slice_dims ). (C2) \\(0 \\le\\) index_vector_dim \\(\\le\\) rank( start_indices ). (C3) size( start_index_map ) \\(=\\) index_vector_dim \\(\\lt\\) rank( start_indices ) ? dim( start_indices , index_vector_dim ) : 1. (C4) All dimensions in offset_dims are unique and sorted in ascending order. (C5) \\(0 \\le\\) offset_dims [i] \\(\\lt\\) rank( result ) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( offset_dims ). (C6) All dimensions in collapsed_slice_dims are unique and sorted in ascending order. (C7) \\(0 \\le\\) collapsed_slice_dims [i] \\(\\lt\\) size( slice_sizes ) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( collapsed_slice_dims ). (C8) slice_sizes [i] \\(\\le\\) 1 \\(\\forall i \\in\\) collapsed_slice_dims . (C9) All dimensions in start_index_map are unique. (C10) \\(0 \\le\\) start_index_map [i] \\(\\lt\\) rank( operand ) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( start_index_map ). (C11) size( slice_sizes ) \\(=\\) rank( operand ). (C12) \\(0 \\le\\) slice_sizes [i] \\(\\le\\) dim( operand , i) \\(\\forall i\\) such that \\(0 \\le\\) i \\(\\lt\\) size( slice_sizes ). (C13) shape(result) \\(=\\) combine(batch_dim_sizes, offset_dim_sizes) where: batch_dim_sizes = shape(start_indices) except that the dimension size of start_indices corresponding to index_vector_dim is not included. offset_dim_sizes = shape(slice_sizes) except that the dimension sizes in slice_sizes corresponding to collapsed_slice_dims are not included. combine puts batch_dim_sizes at axes corresponding to batch_dims and offset_dim_sizes at axes corresponding to offset_dims . (C15) operand and result have the same element type.","title":"Constraints"},{"location":"spec/#examples_36","text":"// %operand: [ // [[1, 2], [3, 4], [5, 6], [7, 8]], // [[9, 10],[11, 12], [13, 14], [15, 16]], // [[17, 18], [19, 20], [21, 22], [23, 24]] // ] // %start_indices: [ // [[0, 0], [1, 0], [2, 1]], // [[0, 1], [1, 1], [0, 2]] // ] %result = \"stablehlo.gather\"(%operand, %start_indices) { dimension_numbers = #stablehlo.gather< offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [1, 0], index_vector_dim = 2>, slice_sizes = dense<[1, 2, 2]> : tensor<3xi64>, indices_are_sorted = false } : (tensor<3x4x2xi32>, tensor<2x3x2xi64>) -> tensor<2x3x2x2xi32> // %result: [ // [ // [[1, 2], [3, 4]], // [[3, 4], [5, 6]], // [[13, 14], [15, 16]] // ], // [ // [[9, 10], [11, 12]], // [[11, 12], [13, 14]], // [[17, 18], [19, 20]] // ] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloget_tuple_element","text":"","title":"stablehlo.get_tuple_element"},{"location":"spec/#semantics_37","text":"Extracts element at index position of the operand tuple and produces a result .","title":"Semantics"},{"location":"spec/#inputs_37","text":"Name Type operand tuple index constant of type si32","title":"Inputs"},{"location":"spec/#outputs_37","text":"Name Type result any supported type","title":"Outputs"},{"location":"spec/#constraints_35","text":"(C1) 0 \\(\\le\\) index \\(\\lt\\) size( operand ). (C2) type( operand[index] ) \\(=\\) type( result ).","title":"Constraints"},{"location":"spec/#examples_37","text":"// %operand: ([1.0, 2.0], (3)) %result = \"stablehlo.get_tuple_element\"(%operand) { index = 0 : i32 } : (tuple<tensor<2xf32>, tuple<tensor<i32>>>) -> tensor<2xf32> // %result: [1.0, 2.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloif","text":"","title":"stablehlo.if"},{"location":"spec/#semantics_38","text":"Produces the output from executing exactly one function from true_branch or false_branch depending on the value of pred . Formally, if pred is true , output of true_branch is returned, else if pred is false , output of false_branch is returned.","title":"Semantics"},{"location":"spec/#inputs_38","text":"Name Type pred 1-dimensional tensor constant of type i1 true_branch function false_branch function","title":"Inputs"},{"location":"spec/#outputs_38","text":"Name Type results variadic number of tensors of any supported type or tokens","title":"Outputs"},{"location":"spec/#constraints_36","text":"(C1) true_branch and false_branch have 0 inputs. (C2) true_branch and false_branch have the same output types. (C3) For all i , type(results[i]) = type(true_branch).outputs[i] .","title":"Constraints"},{"location":"spec/#examples_38","text":"// %result_true_branch: 10 // %result_false_branch: 11 // %pred: true %result = \"stablehlo.if\"(%pred) ({ \"stablehlo.return\"(%result_true_branch) : (tensor<i32>) -> () }, { \"stablehlo.return\"(%result_false_branch) : (tensor<i32>) -> () }) : (tensor<i1>) -> tensor<i32> // %result: 10 Back to Ops","title":"Examples"},{"location":"spec/#stablehloimag","text":"","title":"stablehlo.imag"},{"location":"spec/#semantics_39","text":"Extracts the imaginary part, element-wise, from the operand and produces a result tensor. More formally, for each element x : imag(x) = is_complex(x) ? x.imag : 0.0 .","title":"Semantics"},{"location":"spec/#inputs_39","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_39","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_37","text":"(C1) shape( result ) = shape( operand ). (C2) element_type( result ) \\(=\\) element_type( operand ) if it's a floating-point type. real_type(element_type( operand )) otherwise.","title":"Constraints"},{"location":"spec/#examples_39","text":"// %operand: [(1.0, 2.0), (3.0, 4.0)] %result = \"stablehlo.imag\"(%operand) : (tensor<2xcomplex<f32>>) -> tensor<2xf32> // %result: [2.0, 4.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloinfeed","text":"","title":"stablehlo.infeed"},{"location":"spec/#semantics_40","text":"Reads data from the infeed and produces results . Semantics of infeed_config is implementation-defined. results consist of payload values which come first and a token which comes last. The operation produces a token to reify the side effect of this operation as a value that other operations can take a data dependency on.","title":"Semantics"},{"location":"spec/#inputs_40","text":"Name Type token token infeed_config constant of type string","title":"Inputs"},{"location":"spec/#outputs_40","text":"Name Type results variadic number of tensors of any supported type or tokens","title":"Outputs"},{"location":"spec/#constraints_38","text":"(C1) size( results ) \\(\\ge\\) 1. (C2) type( results [-1]) \\(=\\) token . -- Verify layout in InfeedOp --","title":"Constraints"},{"location":"spec/#examples_40","text":"%results:2 = \"stablehlo.infeed\"(%token) { infeed_config = \"\" } : (!stablehlo.token) -> (tensor<3x3x3xi32>, !stablehlo.token) Back to Ops","title":"Examples"},{"location":"spec/#stablehloiota","text":"","title":"stablehlo.iota"},{"location":"spec/#semantics_41","text":"Fills an output tensor with values in increasing order starting from zero along the iota_dimension dimension. More formally, output[i0, ..., id, ..., iR-1] = id , where d is equal to iota_dimension . For integers, if the dimension size is larger than what the element type's maximum value can hold, an overflow occurs and the behavior is implementation- defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) for signed overflow and saturation to \\(2^n - 1\\) for unsigned overflow.","title":"Semantics"},{"location":"spec/#inputs_41","text":"Name Type iota_dimension si64","title":"Inputs"},{"location":"spec/#outputs_41","text":"Name Type output tensor of integer, floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_39","text":"(C1) 0 \\(\\le\\) iota_dimension \\(\\lt\\) R , where R is the rank of the output .","title":"Constraints"},{"location":"spec/#examples_41","text":"%output = \"stablehlo.iota\"() { iota_dimension = 0 : i64 } : () -> tensor<4x5xi32> // %output: [ // [0, 0, 0, 0, 0], // [1, 1, 1, 1, 1], // [2, 2, 2, 2, 2], // [3, 3, 3, 3, 3] // ] %output = \"stablehlo.iota\"() { iota_dimension = 1 : i64 } : () -> tensor<4x5xi32> // %output: [ // [0, 1, 2, 3, 4], // [0, 1, 2, 3, 4], // [0, 1, 2, 3, 4], // [0, 1, 2, 3, 4] // ] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlois_finite","text":"","title":"stablehlo.is_finite"},{"location":"spec/#semantics_42","text":"Performs element-wise check whether the value in x is finite (i.e. is neither +Inf, -Inf, nor NaN) and produces a y tensor. Implements the isFinite operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_42","text":"Name Type x tensor of floating-point type","title":"Inputs"},{"location":"spec/#outputs_42","text":"Name Type y tensor of boolean type","title":"Outputs"},{"location":"spec/#constraints_40","text":"(C1) x and y have the same shape.","title":"Constraints"},{"location":"spec/#examples_42","text":"// Logical values: -Inf, +Inf, NaN, ... // %x: [0xFF800000, 0x7F800000, 0x7FFFFFFF, -10.0, -0.0, 0.0, 10.0] %y = \"stablehlo.is_finite\"(%x) : (tensor<7xf32>) -> tensor<7xi1> // %y: [false, false, false, true, true, true, true] Back to Ops","title":"Examples"},{"location":"spec/#stablehlolog","text":"","title":"stablehlo.log"},{"location":"spec/#semantics_43","text":"Performs element-wise logarithm operation on operand tensor and produces a result tensor. For floating-point element types, it implements the log operation from the IEEE-754 specification. For complex element types, it computes a complex logarithm, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_43","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_43","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_41","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_43","text":"// %operand: [[1.0, 2.0], [3.0, 4.0]] %result = \"stablehlo.log\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 0.69314718], [1.09861229, 1.38629436]] // %operand: (1.0, 2.0) %result = \"stablehlo.log\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: (0.80471896, 1.10714871) Back to Ops","title":"Examples"},{"location":"spec/#stablehlolog_plus_one","text":"","title":"stablehlo.log_plus_one"},{"location":"spec/#semantics_44","text":"Performs element-wise log plus one operation on operand tensor and produces a result tensor. For floating-point element types, it implements the logp1 operation from the IEEE-754 specification. For complex element types, it computes a complex log plus one, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_44","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_44","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_42","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_44","text":"// %operand: [-2.0, -0.0, -0.999, 7.0, 6.38905621, 15.0] %result = \"stablehlo.log_plus_one\"(%operand) : (tensor<6xf32>) -> tensor<6xf32> // %result: [-nan, 0.0, -6.90776825, 2.07944155, 2.0, 2.77258873] Back to Ops","title":"Examples"},{"location":"spec/#stablehlologistic","text":"","title":"stablehlo.logistic"},{"location":"spec/#semantics_45","text":"Performs element-wise logistic (sigmoid) function on operand tensor and produces a result tensor. For floating-point element types, it implements: \\( \\(logistic(x) = division(1, addition(1, exp(-x)))\\) \\) where addition , division , and exp are operations from IEEE-754 specification. For complex element types, it computes a complex logistic function, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_45","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_45","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_43","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_45","text":"// %operand: [[0.0, 1.0], [2.0, 3.0]] %result = \"stablehlo.logistic\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.5, 0.73105858], [0.88079708, 0.95257413]] // %operand: (1.0, 2.0) %result = \"stablehlo.logistic\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: (1.02141536, 0.40343871) Back to Ops","title":"Examples"},{"location":"spec/#stablehlomap","text":"","title":"stablehlo.map"},{"location":"spec/#semantics_46","text":"Applies a map function computation to inputs along the dimensions and produces a result tensor. More formally, result[i0, ..., iR-1] = computation(inputs[0][i0, ..., iR-1], ..., inputs[N-1][i0, ..., iR-1]) .","title":"Semantics"},{"location":"spec/#inputs_46","text":"Name Type inputs variadic number of tensors of any supported type dimensions 1-dimensional tensor constant of type si64 computation function","title":"Inputs"},{"location":"spec/#outputs_46","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_44","text":"(C1) All inputs and result have the same shape. (C2) size( inputs ) \\(=\\) N \\(\\ge\\) 1. (C3) dimensions = [0, ..., R-1] , where R \\(=\\) rank( inputs[0] ). (C4) computation has type (tensor<E0>, ..., tensor<EN-1>) -> tensor<E'> where Ek \\(=\\) element_type( inputs[k] ) and E' \\(=\\) element_type( result ).","title":"Constraints"},{"location":"spec/#examples_46","text":"// %input0: [[0, 1], [2, 3]] // %input1: [[4, 5], [6, 7]] %result = \"stablehlo.map\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = stablehlo.multiply %arg0, %arg1 : tensor<i32> stablehlo.return %0 : tensor<i32> }) { dimensions = dense<[0, 1]> : tensor<2xi64> } : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[0, 5], [12, 21]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlomaximum","text":"","title":"stablehlo.maximum"},{"location":"spec/#semantics_47","text":"Performs element-wise max operation on tensors lhs and rhs and produces a result tensor. For floating-point element types, it implements the maximum operation from the IEEE-754 specification. For complex element types, it performs lexicographic comparison on the (real, imaginary) pairs with corner cases TBD. For boolean element type, the behavior is same as stablehlo.or .","title":"Semantics"},{"location":"spec/#inputs_47","text":"Name Type lhs tensor of any supported type rhs tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_47","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_45","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_47","text":"// %lhs: [[1, 2], [7, 8]] // %rhs: [[5, 6], [3, 4]] %result = \"stablehlo.maximum\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 6], [7, 8]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlominimum","text":"","title":"stablehlo.minimum"},{"location":"spec/#semantics_48","text":"Performs element-wise min operation on tensors lhs and rhs and produces a result tensor. For floating-point element types, it implements the minimum operation from the IEEE-754 specification. For complex element types, it performs lexicographic comparison on the (real, imaginary) pairs with corner cases TBD. For boolean element type, the behavior is same as stablehlo.and .","title":"Semantics"},{"location":"spec/#inputs_48","text":"Name Type lhs tensor of any supported type rhs tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_48","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_46","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_48","text":"// %lhs: [[1, 2], [7, 8]] // %rhs: [[5, 6], [3, 4]] %result = \"stablehlo.minimum\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[1, 2], [3, 4]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlomultiply","text":"","title":"stablehlo.multiply"},{"location":"spec/#semantics_49","text":"Performs element-wise product of two tensors lhs and rhs and produces a result tensor. For integer element types, if the element-wise product has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ \\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For floating-point element types, it implements the multiplication operation from the IEEE-754 specification. For complex element types, it computes a complex multiplication, with corner cases TBD. For boolean element type, the behavior is same as stablehlo.and .","title":"Semantics"},{"location":"spec/#inputs_49","text":"Name Type lhs tensor of any supported type rhs tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_49","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_47","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_49","text":"// %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.multiply\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 12], [21, 32]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlonegate","text":"","title":"stablehlo.negate"},{"location":"spec/#semantics_50","text":"Performs element-wise negation of operand tensor and produces a result tensor. For floating-point element types, it implements the negate operation from the IEEE-754 specification. For signed integer types, it performs the regular negation operation where the negation of \\(-2^{n-1}\\) is implementation- defined and one of the following: Saturation to \\(2^{n-1}-1\\) \\(-2^{n-1}\\) For unsigned integer types, it bitcasts to the corresponding signed integer type, performs the regular negation operation and bitcasts back to the original unsigned integer type.","title":"Semantics"},{"location":"spec/#inputs_50","text":"Name Type operand tensor of integer, floating-point, or complex type","title":"Inputs"},{"location":"spec/#outputs_50","text":"Name Type result tensor of integer, floating-point, or complex type","title":"Outputs"},{"location":"spec/#constraints_48","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_50","text":"// Negation operation with integer Tensors // %operand: [0, -2] %result = \"stablehlo.negate\"(%operand) : (tensor<2xi32>) -> tensor<2xi32> // %result: [0, 2] // Negation operation with with complex tensors // %operand: (2.5, 0.0) %result = \"stablehlo.negate\"(%operand) : (tensor<1xcomplex<f32>>) -> tensor<1xcomplex<f32>> // %result: [-2.5, -0.0] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlonot","text":"","title":"stablehlo.not"},{"location":"spec/#semantics_51","text":"Performs element-wise bitwise NOT of tensor operand of type integer and produces a result tensor. For boolean tensors, it computes the logical NOT.","title":"Semantics"},{"location":"spec/#arguments","text":"Name Type operand tensor of integer or boolean type","title":"Arguments"},{"location":"spec/#outputs_51","text":"Name Type result tensor of integer or boolean type","title":"Outputs"},{"location":"spec/#constraints_49","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_51","text":"// Bitwise operation with with integer tensors // %operand: [[1, 2], [3, 4]] %result = \"stablehlo.not\"(%operand) : (tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[-2, -3], [-4, -5]] // Bitwise operation with with boolean tensors // %operand: [true, false] %result = \"stablehlo.not\"(%operand) : (tensor<2xi1>) -> tensor<2xi1> // %result: [false, true] Back to Ops","title":"Examples"},{"location":"spec/#stablehlooptimization_barrier","text":"","title":"stablehlo.optimization_barrier"},{"location":"spec/#semantics_52","text":"Ensures that the operations that produce the operand are executed before any operations that depend on the result and prevents compiler transformations from moving operations across the barrier. Other than that, the operation is an identity, i.e. result = operand .","title":"Semantics"},{"location":"spec/#arguments_1","text":"Name Type operand variadic number of tensors of any supported type or tokens","title":"Arguments"},{"location":"spec/#outputs_52","text":"Name Type result variadic number of tensors of any supported type or tokens","title":"Outputs"},{"location":"spec/#constraints_50","text":"(C1) size( operand ) \\(=\\) size( result ). (C2) type( operand[i] ) \\(=\\) type( result[i] ) for all i.","title":"Constraints"},{"location":"spec/#examples_52","text":"// %operand0: 0.0 // %operand1: 1.0 %result0, %result1 = \"stablehlo.optimization_barrier\"(%operand0, %operand1) : (tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>) // %result0: 0.0 // %result1: 1.0 Back to Ops","title":"Examples"},{"location":"spec/#stablehloor","text":"","title":"stablehlo.or"},{"location":"spec/#semantics_53","text":"Performs element-wise bitwise OR of two tensors lhs and rhs of integer types and produces a result tensor. For boolean tensors, it computes the logical operation.","title":"Semantics"},{"location":"spec/#inputs_51","text":"Name Type lhs tensor of integer or boolean type rhs tensor of integer or boolean type","title":"Inputs"},{"location":"spec/#outputs_53","text":"Name Type result tensor of integer or boolean type","title":"Outputs"},{"location":"spec/#constraints_51","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_53","text":"// Bitwise operation with with integer tensors // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.or\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 6], [7, 12]] // Logical operation with with boolean tensors // %lhs: [[false, false], [true, true]] // %rhs: [[false, true], [false, true]] %result = \"stablehlo.or\"(%lhs, %rhs) : (tensor<2x2xi1>, tensor<2x2xi1>) -> tensor<2x2xi1> // %result: [[false, true], [true, true]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlooutfeed","text":"","title":"stablehlo.outfeed"},{"location":"spec/#semantics_54","text":"Writes inputs to the outfeed and produces a result token. Semantics of outfeed_config is implementation-defined. The operation takes a token and produces a token to reify its side effects as a value that other operations can take a data dependency on.","title":"Semantics"},{"location":"spec/#inputs_52","text":"Name Type inputs variadic number of tensors of any supported type token token outfeed_config constant of type string","title":"Inputs"},{"location":"spec/#outputs_54","text":"Name Type result token","title":"Outputs"},{"location":"spec/#examples_54","text":"%result = \"stablehlo.outfeed\"(%input0, %token) { outfeed_config = \"\" } : (tensor<3x3x3xi32>, !stablehlo.token) -> !stablehlo.token Back to Ops","title":"Examples"},{"location":"spec/#stablehlopad","text":"","title":"stablehlo.pad"},{"location":"spec/#semantics_55","text":"Expands operand by padding around the tensor as well as between the elements of the tensor with the given padding_value . edge_padding_low and edge_padding_high specify the amount of padding added at the low-end (next to index 0) and the high-end (next to the highest index) of each dimension respectively. The amount of padding can be negative, where the absolute value of negative padding indicates the number of elements to remove from the specified dimension. interior_padding specifies the amount of padding added between any two elements in each dimension which may not be negative. Interior padding occurs before edge padding such that negative edge padding will remove elements from the interior-padded operand. More formally, result[i0, ..., iR-1] is equal to: operand[j0, ..., jR-1] if id = edge_padding_low[d] + jd * (interior_padding[d] + 1) . padding_value[] otherwise.","title":"Semantics"},{"location":"spec/#inputs_53","text":"Name Type operand tensor of any supported type padding_value 0-dimensional tensor of any supported type edge_padding_low 1-dimensional tensor constant of type si64 edge_padding_high 1-dimensional tensor constant of type si64 interior_padding 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_55","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_52","text":"(C1) operand , padding_value , result have the same element type. (C2) edge_padding_low , edge_padding_high , interior_padding have the size equal to operand 's rank. (C3) 0 \\(\\le\\) interior_padding[i] for all i values in interior_padding . (C4) 0 \\(\\le\\) dim(result, i) for all i th dimension of operand , where dim(result, i) = di + max(di - 1, 0) * interior_padding[i] + edge_padding_low[i] + edge_padding_high[i] and di = dim(operand, i) .","title":"Constraints"},{"location":"spec/#examples_55","text":"// %operand: [ // [1, 2, 3], // [4, 5, 6] // ] // %padding_value: 0 %result = \"stablehlo.pad\"(%operand, %padding_value) { edge_padding_low = dense<[0, 1]> : tensor<2xi64>, edge_padding_high = dense<[2, 1]> : tensor<2xi64>, interior_padding = dense<[1, 2]> : tensor<2xi64> } : (tensor<2x3xi32>, tensor<i32>) -> tensor<5x9xi32> // %result: [ // [0, 1, 0, 0, 2, 0, 0, 3, 0], // [0, 0, 0, 0, 0, 0, 0, 0, 0], // [0, 4, 0, 0, 5, 0, 0, 6, 0], // [0, 0, 0, 0, 0, 0, 0, 0, 0], // [0, 0, 0, 0, 0, 0, 0, 0, 0] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlopartition_id","text":"","title":"stablehlo.partition_id"},{"location":"spec/#semantics_56","text":"Produces partition_id of the current process.","title":"Semantics"},{"location":"spec/#outputs_56","text":"Name Type result 0-dimensional tensor of type ui32","title":"Outputs"},{"location":"spec/#examples_56","text":"%result = \"stablehlo.partition_id\"() : () -> tensor<ui32> Back to Ops","title":"Examples"},{"location":"spec/#stablehlopopcnt","text":"","title":"stablehlo.popcnt"},{"location":"spec/#semantics_57","text":"Performs element-wise count of the number of bits set in the operand tensor and produces a result tensor.","title":"Semantics"},{"location":"spec/#inputs_54","text":"Name Type operand tensor of integer type","title":"Inputs"},{"location":"spec/#outputs_57","text":"Name Type result tensor of integer type","title":"Outputs"},{"location":"spec/#constraints_53","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_57","text":"// %operand: [0, 1, 2, 127] %result = \"stablehlo.popcnt\"(%operand) : (tensor<4xi8>) -> tensor<4xi8> // %result: [0, 1, 1, 7] Back to Ops","title":"Examples"},{"location":"spec/#stablehlopower","text":"","title":"stablehlo.power"},{"location":"spec/#semantics_58","text":"Performs element-wise exponentiation of lhs tensor by rhs tensor and produces a result tensor. For integer element types, if the exponentiation has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For an integer, x , raised to a negative power, y , the behaviour is as follows: If abs(x) \\(\\gt\\) 1, then result is 0. If abs(x) \\(=\\) 1, then result is equivalent to x^abs(y) . If abs(x) \\(=\\) 0, then behaviour is implementation-defined. For floating-point element types, it implements the pow operation from the IEEE-754 specification. For complex element types, it computes complex exponentiation, with corner cases TBD. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_55","text":"Name Type lhs tensor of integer, floating-point, or complex type rhs tensor of integer, floating-point, or complex type","title":"Inputs"},{"location":"spec/#outputs_58","text":"Name Type result tensor of integer, floating-point, or complex type","title":"Outputs"},{"location":"spec/#constraints_54","text":"(C1) lhs , rhs , and result have the same type.","title":"Constraints"},{"location":"spec/#examples_58","text":"// %lhs: [-2.0, -0.0, -36.0, 5.0, 3.0, 10000.0] // %rhs: [2.0, 2.0, 1.1, 2.0, -1.0, 10.0] %result = \"stablehlo.power\"(%lhs, %rhs) : (tensor<6xf32>, tensor<6xf32>) -> tensor<6xf32> // %result: [4.0, 0.0, -nan, 25.0, 0.333333343, inf] Back to Ops","title":"Examples"},{"location":"spec/#stablehloreal","text":"","title":"stablehlo.real"},{"location":"spec/#semantics_59","text":"Extracts the real part, element-wise, from the operand and produces a result tensor. More formally, for each element x : real(x) = is_complex(x) ? x.real : x .","title":"Semantics"},{"location":"spec/#inputs_56","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_59","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_55","text":"(C1) shape( result ) = shape( operand ). (C2) element_type( result ) \\(=\\) element_type( operand ) if it's a floating-point type. real_type(element_type( operand )) otherwise.","title":"Constraints"},{"location":"spec/#examples_59","text":"// %operand: [(1.0, 2.0), (3.0, 4.0)] %result = \"stablehlo.real\"(%operand) : (tensor<2xcomplex<f32>>) -> tensor<2xf32> // %result: [1.0, 3.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehlorecv","text":"","title":"stablehlo.recv"},{"location":"spec/#semantics_60","text":"Receives data from a channel with channel_id and produces results . If is_host_transfer is true , then the operation transfers data from the host. Otherwise, it transfers data from another device. What this means is implementation-defined. results consist of payload values which come first and a token which comes last. The operation produces a token to reify its side effects as a value that other operations can take a data dependency on.","title":"Semantics"},{"location":"spec/#inputs_57","text":"Name Type token token channel_id constant of type si64 channel_type enum of DEVICE_TO_DEVICE and HOST_TO_DEVICE is_host_transfer constant of type i1","title":"Inputs"},{"location":"spec/#outputs_60","text":"Name Type results variadic number of tensors of any supported type or token","title":"Outputs"},{"location":"spec/#constraints_56","text":"(C1) todo channel_type must be HOST_TO_DEVICE , if is_host_transfer \\(=\\) true , DEVICE_TO_DEVICE , otherwise. (C2) size( results ) \\(\\ge\\) 1. (C3) type( results [-1]) \\(=\\) token .","title":"Constraints"},{"location":"spec/#examples_60","text":"%results:2 = \"stablehlo.recv\"(%token) { // channel_id = 5 : i64, // channel_type = #stablehlo<channel_type HOST_TO_DEVICE>, channel_handle = #stablehlo.channel_handle<handle = 5, type = 3>, is_host_transfer = true } : (!stablehlo.token) -> (tensor<3x4xi32>, !stablehlo.token) Back to Ops","title":"Examples"},{"location":"spec/#stablehloreduce","text":"","title":"stablehlo.reduce"},{"location":"spec/#semantics_61","text":"Applies a reduction function body to inputs and init_values along the dimensions and produces a result tensor. The order of reductions is implementation-defined, which means that body and init_values must form a monoid to guarantee that the operation produces the same results for all inputs on all implementations. However, this condition doesn't hold for many popular reductions. E.g. floating-point addition for body and zero for init_values don't actually form a monoid because floating-point addition is not associative. What this means for numeric precision is implementation-defined. More formally, results[:][j0, ..., jR-1] = reduce(input_slices) where: input_slices = inputs[:][j0, ..., :, ..., jR-1] , where : are inserted at dimensions . reduce(input_slices) = exec(schedule) for some binary tree schedule where: exec(node) = body(exec(node.left), exec(node.right)) . exec(leaf) = leaf.value . schedule is an implementation-defined full binary tree whose in-order traversal consists of: input_slices[:][index] values, for all index in the index space of input_slices , in the ascending lexicographic order of index . Interspersed with an implementation-defined amount of init_values at implementation-defined positions.","title":"Semantics"},{"location":"spec/#inputs_58","text":"Name Type inputs variadic number of tensors of any supported type init_values variadic number of 0-dimensional tensors of any supported type dimensions 1-dimensional tensor constant of type si64 body function","title":"Inputs"},{"location":"spec/#outputs_61","text":"Name Type results variadic number of tensors of any supported type","title":"Outputs"},{"location":"spec/#constraints_57","text":"(C1) All inputs have the same shape. (C2) element_type( inputs[k] ) \\(=\\) element_type( init_values[k] ) \\(=\\) element_type( results[k] ) for all k \\(\\in\\) [0, N). (C3) size( inputs ) \\(=\\) size( init_values ) \\(=\\) size( results ) \\(=\\) N where N >= 1. (C4) 0 \\(\\le\\) dimensions[d] \\(\\lt\\) rank( inputs[0][d] ) for all dimension d . (C5) All dimensions in dimensions are unique. (C6) body has type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) where Ek = element_type(inputs[k]) . (C7) shape( results[k] ) \\(=\\) shape( inputs[k] ) except that the dimension sizes of inputs[k] corresponding to dimensions are not included.","title":"Constraints"},{"location":"spec/#examples_61","text":"// %input = [[0, 1, 2, 3, 4, 5]] // %init_value = 0 %result = \"stablehlo.reduce\"(%input, %init_value) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) { dimensions = dense<1> : tensor<1xi64> } : (tensor<1x6xi32>, tensor<i32>) -> tensor<1xi32> // %result = [15] Back to Ops","title":"Examples"},{"location":"spec/#stablehloreduce_precision","text":"","title":"stablehlo.reduce_precision"},{"location":"spec/#semantics_62","text":"Performs element-wise conversion of operand to another floating-point type that uses exponent_bits and mantissa_bits and back to the original floating-point type and produces a result tensor. More formally: * The mantissa bits of the original value are updated to round the original value to the nearest value representable with mantissa_bits using roundToIntegralTiesToEven semantics. * Then, if mantissa_bits are smaller than the number of mantissa bits of the original value, the mantissa bits are truncated to mantissa_bits . * Then, if the exponent bits of the intermediate result don't fit into the range provided by exponent_bits , the intermediate result overflows to infinity using the original sign or underflows to zero using the original sign.","title":"Semantics"},{"location":"spec/#inputs_59","text":"Name Type operand tensor of floating-point type exponent_bits constant of type si32 mantissa_bits constant of type si32","title":"Inputs"},{"location":"spec/#outputs_62","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_58","text":"(C1) operand and result have the same type. (C2) exponent_bits \\(\\ge\\) 1. (C3) mantissa_bits \\(\\ge\\) 0.","title":"Constraints"},{"location":"spec/#examples_62","text":"// Logical values: -Inf, +Inf, NaN, ... // %operand: [0xFF800000, 0x7F800000, 0x7FFFFFFF, 0.0, 1000.0, 1000000.0] %result = \"stablehlo.reduce_precision\"(%operand) { exponent_bits = 5 : i32, mantissa_bits = 2 : i32 } : (tensor<6xf32>) -> tensor<6xf32> // Logical values: -Inf, +Inf, NaN, NaN, 0.0, 1024.0, +Inf // %result: [0xFF800000, 0x7F800000, 0x7FFFFFFF, 0.0, 1024.0, 0x7F800000] Back to Ops","title":"Examples"},{"location":"spec/#stablehloreduce_scatter","text":"","title":"stablehlo.reduce_scatter"},{"location":"spec/#semantics_63","text":"Within each process group in the StableHLO grid, performs reduction, using computations , over the values of the operand tensor from each process, splits the reduction result along scatter_dimension into parts, and scatters the split parts between the processes to produce the result . The operation splits the StableHLO grid into process_groups as follows: channel_id <= 0 and use_global_device_ids = false , cross_replica(replica_groups) . channel_id > 0 and use_global_device_ids = false , cross_replica_and_partition(replica_groups) . channel_id > 0 and use_global_device_ids = true , flattened_ids(replica_groups) . Afterwards, within each process_group : reduced_value = all_reduce(operand, replica_groups, channel_id, use_global_device_ids, computation) . parts@sender = split(reduced_value@sender, dim(process_groups, 1), split_dimension) . result@receiver = parts@sender[receiver_index] for any sender in process_group, where receiver_index = index_of(receiver, process_group) .","title":"Semantics"},{"location":"spec/#inputs_60","text":"Name Type operand tensor of any supported type scatter_dimension constant of type si64 replica_groups 2-dimensional tensor constant of type si64 channel_id constant of type si64 use_global_device_ids constant of type boolean computation function","title":"Inputs"},{"location":"spec/#outputs_63","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_59","text":"(C1) dim( operand , scatter_dimension ) % dim( process_groups , 1) \\(=\\) 0. (C2) scatter_dimension \\(\\in\\) [0, rank( operand )). (C3) All values in replica_groups are unique. (C4) size(replica_groups) depends on the process grouping strategy: If cross_replica , num_replicas . If cross_replica_and_partition , num_replicas . If flattened_ids , num_processes . (C5) \\(0 \\le\\) replica_groups[i] \\(\\lt\\) size( replica_groups ) \\(\\forall i\\) in indices(replica_groups) . (C6) If use_global_device_ids = true , then channel_id > 0 . todo (C7) computation has type (tensor<E>, tensor<E>) -> (tensor<E>) where E = element_type(operand) . (C8) type(result) = type(operand) except: dim(result, scatter_dimension) = dim(operand, scatter_dimension) / dim(process_groups, 1) .","title":"Constraints"},{"location":"spec/#examples_63","text":"// num_replicas: 2 // num_partitions: 1 // %operand@(0, 0): [ // [1.0, 2.0, 3.0, 4.0], // [5.0, 6.0, 7.0, 8.0] // ] // %operand@(1, 0): [ // [9.0, 10.0, 11.0, 12.0], // [13.0, 14.0, 15.0, 16.0] // ] %result = \"stablehlo.reduce_scatter\"(%operand) ({ ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<f32>, tensor<f32>) -> tensor<f32> \"stablehlo.return\"(%0) : (tensor<f32>) -> () }) { scatter_dimension = 1 : i64, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>, // channel_id = 0 channel_handle = #stablehlo.channel_handle<handle = 0, type = 0> // use_global_device_ids = false } : (tensor<2x4xf32>) -> tensor<2x2xf32> // // %result@(0, 0): [ // [10.0, 12.0], // [18.0, 20.0] // ] // %result@(1, 0): [ // [14.0, 16.0], // [22.0, 24.0] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloreduce_window","text":"","title":"stablehlo.reduce_window"},{"location":"spec/#semantics_64","text":"Applies a reduction function body to windows of inputs and init_values and produces results . The following diagram shows how elements in results[k] are computed from inputs[k] using a concrete example. More formally, results[:][result_index] = reduce(windows, init_values, axes(inputs[:]), body) where: padded_inputs = pad(inputs[:], init_values[:], padding[:, 0], padding[:, 1], base_dilations) . window_start = result_index * window_strides . windows = slice(padded_inputs[:], window_start, window_start + window_dimensions, window_dilations) .","title":"Semantics"},{"location":"spec/#inputs_61","text":"Name Type Constraints inputs variadic number of tensors of any supported type (C1-C4), (C6), (C8), (C10), (C12), (C13), (C15) init_values variadic number of 0-dimensional tensors of any supported type (C1), (C13), (C16) window_dimensions 1-dimensional tensor constant of type si64 (C4), (C5), (C15) window_strides 1-dimensional tensor constant of type si64 (C6), (C7), (C15) base_dilations 1-dimensional tensor constant of type si64 (C8), (C9), (C15) window_dilations 1-dimensional tensor constant of type si64 (C10), (C11), (C15) padding 2-dimensional tensor constant of type si64 (C12), (C15) body function (C13)","title":"Inputs"},{"location":"spec/#outputs_64","text":"Name Type Constraints results variadic number of tensors of any supported type (C1), (C14-C16)","title":"Outputs"},{"location":"spec/#constraints_60","text":"(C1) size( inputs ) \\(=\\) size( init_values ) \\(=\\) size( results ) \\(=\\) N and N \\(\\ge\\) 1. (C2) All inputs have the same shape. (C3) element_type(inputs[k]) = element_type(init_values[k]) for any k \\(\\in\\) [0, N). (C4) size( window_dimensions ) \\(=\\) rank( inputs[0] ). (C5) window_dimensions[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_dimensions )). (C6) size( window_strides ) \\(=\\) rank( inputs[0] ). (C7) window_strides[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_strides )). (C8) size( base_dilations ) \\(=\\) rank( inputs[0] ). (C9) base_dilations[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( base_dilations )). (C10) size( window_dilations ) \\(=\\) rank( inputs[0] ). (C11) window_dilations[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size( window_dilations )). (C12) dim( padding , 0) \\(=\\) rank( inputs[0] ) and dim( padding , 1) = 2. (C13) body has type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) where Ek = element_type(inputs[0]) . (C14) All results have the same shape. (C15) shape(results[0]) = num_windows dilated_input_shape = shape(inputs[0]) == 0 ? 0 : (shape(inputs[0]) - 1) * base_dilations + 1 . padded_input_shape = padding[:, 0] + dilated_input_shape + padding[:, 1] . dilated_window_shape = window_dimensions == 0 ? 0 : (window_dimensions - 1) * window_dilations + 1 . num_windows = (padded_input_shape == 0 || dilated_window_shape > padded_input_shape) ? 0 : floor((padded_input_shape - dilated_window_shape) / window_strides) + 1 . (C16) element_type(results[k]) = element_type(init_values[k]) for any k \\(\\in\\) [0, N).","title":"Constraints"},{"location":"spec/#examples_64","text":"// %input = [[1, 2], [3, 4], [5, 6]] // %init_value = 0 %result = \"stablehlo.reduce_window\"(%input, %init_value) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) { window_dimensions = dense<[2, 1]> : tensor<2xi64>, window_strides = dense<[4, 1]> : tensor<2xi64>, base_dilations = dense<[2, 1]> : tensor<2xi64>, window_dilations = dense<[3, 1]> : tensor<2xi64>, padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<3x2xi32>, tensor<i32>) -> tensor<2x2xi32> // %result = [[0, 0], [3, 4]] Back to Ops","title":"Examples"},{"location":"spec/#stablehloremainder","text":"","title":"stablehlo.remainder"},{"location":"spec/#semantics_65","text":"Performs element-wise remainder of dividend lhs and divisor rhs tensors and produces a result tensor. The sign of the result is taken from the dividend, and the absolute value of the result is always less than the divisor's absolute value. The remainder is calculated as lhs - d * rhs , where d = stablehlo.divide . For floating-point element types, this is in contrast with the remainder operation from IEEE-754 specification where d is an integral value nearest to the exact value of lhs/rhs with ties to even. For floating-point types, the corner cases are TBD. For n-bit integer, division overflow (remainder by zero or remainder of \\(-2^{n-1}\\) with \\(-1\\) ) produces an implementation-defined value.","title":"Semantics"},{"location":"spec/#inputs_62","text":"Name Type lhs tensor of integer, floating-point or complex type rhs tensor of integer, floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_65","text":"Name Type result tensor of integer, floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_61","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_65","text":"// %lhs: [17.1, -17.1, 17.1, -17.1] // %rhs: [3.0, 3.0, -3.0, -3.0] %result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32> // %result: [2.1, -2.1, 2.1, -2.1] // %lhs: [17, -17, 17, -17] // %rhs: [3, 3, -3, -3] %result = \"stablehlo.remainder\"(%lhs, %rhs) : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32> // %result: [2, -2, 2, -2] Back to Ops","title":"Examples"},{"location":"spec/#stablehloreplica_id","text":"","title":"stablehlo.replica_id"},{"location":"spec/#semantics_66","text":"Produces replica_id of the current process.","title":"Semantics"},{"location":"spec/#outputs_66","text":"Name Type result 0-dimensional tensor of type ui32","title":"Outputs"},{"location":"spec/#examples_66","text":"%result = \"stablehlo.replica_id\"() : () -> tensor<ui32> Back to Ops","title":"Examples"},{"location":"spec/#stablehloreshape","text":"","title":"stablehlo.reshape"},{"location":"spec/#semantics_67","text":"Performs reshape of operand tensor to a result tensor. Conceptually, it amounts to keeping the same canonical representation but potentially changing the shape, e.g. from tensor<2x3xf32> to tensor<3x2xf32> or tensor<6xf32> . More formally, result[i0, ..., iR-1] = operand[j0, ..., jR'-1] where i and j have the same position in the lexicographic ordering of the index spaces of result and operand .","title":"Semantics"},{"location":"spec/#inputs_63","text":"Name Type operand tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_67","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_62","text":"(C1) operand and result have the same element type. (C2) operand and result have the same number of elements.","title":"Constraints"},{"location":"spec/#examples_67","text":"// %operand: [[1, 2, 3], [4, 5, 6]]] %result = \"stablehlo.reshape\"(%operand) : (tensor<2x3xi32>) -> tensor<3x2xi32> // %result: [[1, 2], [3, 4], [5, 6]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehloreverse","text":"","title":"stablehlo.reverse"},{"location":"spec/#semantics_68","text":"Reverses the order of elements in the operand along the specified dimensions and produces a result tensor. More formally, result[i0, ..., ik,..., iR-1] = operand[i0, ..., ik',..., iR-1] where ik + ik' = dk - 1 for all dimensions k in dimensions .","title":"Semantics"},{"location":"spec/#inputs_64","text":"Name Type operand tensor of any supported type dimensions 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_68","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_63","text":"(C1) operand and result have the same type. (C2) All dimensions in dimensions are unique. (C3) For all dimensions k in dimensions , 0 \\(\\le\\) dimensions[k] \\(\\lt\\) R , where R is the rank of the result .","title":"Constraints"},{"location":"spec/#examples_68","text":"// Reverse along dimension 0 // %operand = [[1, 2], [3, 4], [5, 6]] %result = \"stablehlo.reverse\"(%operand) { dimensions = dense<0> : tensor<i64> } : (tensor<3x2xi32>) -> tensor<3x2xi32> // %result: [[5, 6], [3, 4], [1, 2]] // Reverse along dimension 1 // %operand = [[1, 2], [3, 4], [5, 6]] %result = \"stablehlo.reverse\"(%operand) { dimensions = dense<1> : tensor<i64> } : (tensor<3x2xi32>) -> tensor<3x2xi32> // %result: [[2, 1], [4, 3], [6, 5]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlorng","text":"","title":"stablehlo.rng"},{"location":"spec/#semantics_69","text":"Generates random numbers using the rng_distribution algorithm and produces a result tensor of a given shape shape . If rng_distribution \\(=\\) UNIFORM , then the random numbers are generated following the uniform distribution over the interval [ a , b ). If a \\(\\ge\\) b , the behavior is undefined. If rng_distribution \\(=\\) NORMAL , then the random numbers are generated following the normal distribution with mean = a and standard deviation = b . If b \\(\\lt\\) 0, the behavior is undefined. The exact way how random numbers are generated is implementation-defined. For example, they may or may not be deterministic, and they may or may not use hidden state.","title":"Semantics"},{"location":"spec/#inputs_65","text":"Name Type a 0-dimensional tensor of integer, boolean, or floating-point type b 0-dimensional tensor of integer, boolean, or floating-point type shape 1-dimensional tensor constant of type si64 rng_distribution enum of UNIFORM and NORMAL","title":"Inputs"},{"location":"spec/#outputs_69","text":"Name Type result tensor of integer, boolean, or floating-point type","title":"Outputs"},{"location":"spec/#constraints_64","text":"(C1) a , b , and result have the same element type. (C2) If rng_distribution = NORMAL , a , b , and result have the same floating-point element type. (C3) shape( result ) = shape .","title":"Constraints"},{"location":"spec/#examples_69","text":"// %a = 0 // %b = 2 // %shape = [3, 3] %result = \"stablehlo.rng\"(%a, %b, %shape) { rng_distribution = #stablehlo<rng_distribution UNIFORM> } : (tensor<i32>, tensor<i32>, tensor<2xi64>) -> tensor<3x3xi32> // %result: [ // [1, 0, 1], // [1, 1, 1], // [0, 0, 0] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlorng_bit_generator","text":"","title":"stablehlo.rng_bit_generator"},{"location":"spec/#semantics_70","text":"Returns an output filled with uniform random bits and an updated output state output_state given an initial state initial_state using the pseudorandom number generator algorithm rng_algorithm . The output is guaranteed to be deterministic function of initial_state , but it is not guaranteed to be deterministic between implementations. rng_algorithm is one of the following: DEFAULT : Implementation-defined algorithm. THREE_FRY : Implementation-defined variant of the Threefry algorithm.* PHILOX : Implementation-defined variant of the Philox algorithm.* * See: Salmon et al. SC 2011. Parallel random numbers: as easy as 1, 2, 3.","title":"Semantics"},{"location":"spec/#inputs_66","text":"Name Type initial_state 1-dimensional tensor of type ui64 rng_algorithm enum of DEFAULT , THREE_FRY , and PHILOX","title":"Inputs"},{"location":"spec/#outputs_70","text":"Name Type output_state 1-dimensional tensor of type ui64 output tensor of integer or floating-point type","title":"Outputs"},{"location":"spec/#constraints_65","text":"(C1) type( initial_state ) \\(=\\) type( output_state ). (C2) size( initial_state ) depends on rng_algorithm : DEFAULT : implementation-defined. THREE_FRY : 2 . PHILOX : 2 or 3 .","title":"Constraints"},{"location":"spec/#examples_70","text":"// %initial_state: [1, 2] %output_state, %output = \"stablehlo.rng_bit_generator\"(%initial_state) { rng_algorithm = #stablehlo<rng_algorithm THREE_FRY> } : (tensor<2xui64>) -> (tensor<2xui64>, tensor<2x2xui64>) // %output_state: [1, 6] // %output: [ // [9236835810183407956, 16087790271692313299], // [18212823393184779219, 2658481902456610144] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloround_nearest_afz","text":"","title":"stablehlo.round_nearest_afz"},{"location":"spec/#semantics_71","text":"Performs element-wise rounding towards the nearest integer, breaking ties away from zero, on the operand tensor and produces a result tensor. Implements the roundToIntegralTiesToAway operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_67","text":"Name Type operand tensor of floating-point type","title":"Inputs"},{"location":"spec/#outputs_71","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_66","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_71","text":"// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5] %result = \"stablehlo.round_nearest_afz\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-3.0, 0.0, 1.0, 1.0, 3.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloround_nearest_even","text":"","title":"stablehlo.round_nearest_even"},{"location":"spec/#semantics_72","text":"Performs element-wise rounding towards the nearest integer, breaking ties towards the even integer, on the operand tensor and produces a result tensor. Implements the roundToIntegralTiesToEven operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_68","text":"Name Type operand tensor of floating-point type","title":"Inputs"},{"location":"spec/#outputs_72","text":"Name Type result tensor of floating-point type","title":"Outputs"},{"location":"spec/#constraints_67","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_72","text":"// %operand = [-2.5, 0.4, 0.5, 0.6, 2.5] %result = \"stablehlo.round_nearest_even\"(%operand) : (tensor<5xf32>) -> tensor<5xf32> // %result: [-2.0, 0.0, 0.0, 1.0, 2.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehlorsqrt","text":"","title":"stablehlo.rsqrt"},{"location":"spec/#semantics_73","text":"Performs element-wise reciprocal square root operation on operand tensor and produces a result tensor, implementing the rSqrt operation from the IEEE-754 specification. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_69","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_73","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_68","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_73","text":"// %operand: [[1.0, 4.0], [9.0, 25.0]] %result = \"stablehlo.rsqrt\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[1.0, 0.5], [0.33333343, 0.2]] // %operand: [(1.0, 2.0)] %result = \"stablehlo.rsqrt\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: [(0.56886448, -0.35157758)] Back to Ops","title":"Examples"},{"location":"spec/#stablehloscatter","text":"","title":"stablehlo.scatter"},{"location":"spec/#semantics_74","text":"Produces results tensors which are equal to inputs tensors except that several slices specified by scatter_indices are updated with the values updates using update_computation . The following diagram shows how elements in updates[k] map on elements in results[k] using a concrete example. The diagram picks a few example updates[k] indices and explains in detail which results[k] indices they correspond to. More formally, for all update_index from the index space of updates[0] : update_scatter_dims = [ d for d in axes(updates[0]) and d not in update_window_dims ]. update_scatter_index = [ update_index[d] for d in update_scatter_dims ]. start_index = scatter_indices[si0, ..., :, ..., siN] where si are individual elements in update_scatter_index and : is inserted at the index_vector_dim index, if index_vector_dim < rank(scatter_indices) . [scatter_indices[update_scatter_index]] otherwise. For do in axes(inputs[0]) , full_start_index[do] = start_index[ds] if do = scatter_dims_to_operand_dims[ds] . full_start_index[do] = 0 otherwise. update_window_index = [ update_index[d] for d in update_window_dims ]. full_window_index = [oi0, ..., 0, ..., oiN] where oi are individual elements in update_window_index , and 0 is inserted at indices from inserted_window_dims . result_index = add(full_start_index, full_window_index) . Using this mapping between update_index and result_index , we define results = exec(schedule, inputs) , where: schedule is an implementation-defined permutation of the index space of updates[0] . exec([update_index, ...], results) = exec([...], updated_results) where: updated_values = update_computation(results[:][result_index], updates[:][update_index]) . updated_results is a copy of results with results[:][result_index] set to updated_values[:] . If result_index is out of bounds for shape(results[:]) , the behavior is implementation-defined. exec([], results) = results . If indices_are_sorted is true then the implementation can assume that scatter_indices are sorted with respect to scatter_dims_to_operand_dims , otherwise the behavior is undefined. More formally, for all id < jd from indices(result) , full_start_index(id) <= full_start_index(jd) . If unique_indices is true then the implementation can assume that all result_index indices being scattered to are unique. If unique_indices is true but the indices being scattered to are not unique then the behavior is undefined.","title":"Semantics"},{"location":"spec/#inputs_70","text":"Name Type Constraints inputs variadic number of tensors of any supported types (C1), (C2), (C4), (C5), (C6), (C10), (C13), (C15), (C16) scatter_indices tensor of any supported integer type (C4), (C11), (C14) updates variadic number of tensors of any supported types (C3), (C4), (C5), (C6), (C8) update_window_dims 1-dimensional tensor constant of type si64 (C2), (C4), (C7), (C8) inserted_window_dims 1-dimensional tensor constant of type si64 (C2), (C4), (C9), (C10) scatter_dims_to_operand_dims 1-dimensional tensor constant of type si64 (C11),(C12), (C13) index_vector_dim constant of type si64 (C4), (C11), (C14) indices_are_sorted constant of type i1 unique_indices constant of type i1 update_computation function (C15)","title":"Inputs"},{"location":"spec/#outputs_74","text":"Name Type results variadic number of tensors of any supported types","title":"Outputs"},{"location":"spec/#constraints_69","text":"(C1) All inputs have the same shape. (C2) rank( inputs [0]) = size( update_window_dims ) + size( inserted_window_dims ). (C3) All updates have the same shape. (C4) shape(updates[0]) \\(=\\) combine(update_scatter_dim_sizes, update_window_dim_sizes) where: update_scatter_dim_sizes = shape(scatter_indices) except that the dimension size of scatter_indices corresponding to index_vector_dim is not included. update_window_dim_sizes \\(\\le\\) shape(inputs[0]) except that the dimension sizes in inputs[0] corresponding to inserted_window_dims are not included. combine puts update_scatter_dim_sizes at axes corresponding to update_scatter_dims and update_window_dim_sizes at axes corresponding to update_window_dims . (C5) N \\(=\\) size( inputs ) = size( updates ) and N \\(\\ge\\) 1. (C6) element_type(updates[k]) = element_type(inputs[k]) for any k \\(\\in\\) [0, N). (C7) All dimensions in update_window_dims are unique and sorted. (C8) For all i \\(\\in\\) [0, size( update_window_dims )), \\(0 \\le\\) update_window_dims [i] \\(\\lt\\) rank( updates [0]). (C9) All dimensions in inserted_window_dims are unique and sorted. (C10) For all i \\(\\in\\) [0, size( inserted_window_dims )), \\(0 \\le\\) inserted_window_dims [i] \\(\\lt\\) rank( inputs [0]). (C11) size( scatter_dims_to_operand_dims ) \\(=\\) index_vector_dim \\(\\lt\\) rank( scatter_indices ) ? dim( scatter_indices , index_vector_dim ) : 1. (C12) All dimensions in scatter_dims_to_operand_dims are unique. (C13) For all i \\(\\in\\) [0, size( scatter_dims_to_operand_dims )), \\(0 \\le\\) scatter_dims_to_operand_dims [i] \\(\\lt\\) rank( inputs [0]). (C14) \\(0 \\le\\) index_vector_dim \\(\\le\\) rank( scatter_indices ). (C15) update_computation has type (tensor<E0>, ..., tensor<EN-1>, tensor<E0>, ..., tensor<EN-1>) -> (tensor<E0>, ..., tensor<EN-1>) where Ek = element_type(inputs[k]) for any k \\(\\in\\) [0, N). (C16) inputs[k] and result[k] have the same type for any k \\(\\in\\) [0, N).","title":"Constraints"},{"location":"spec/#examples_74","text":"// %input: [ // [[1, 2], [3, 4], [5, 6], [7, 8]], // [[9, 10], [11, 12], [13, 14], [15, 16]], // [[17, 18], [19, 20], [21, 22], [23, 24]] // ] // %scatter_indices: [[[0, 2], [1, 0], [2, 1]], [[0, 1], [1, 0], [2, 0]]] // %update: [ // [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 1], [1, 1]]], // [[[1, 1], [1, 1]], [[1, 1], [1, 1]], [[1, 1], [1, 1]]] // ] %result = \"stablehlo.scatter\"(%input, %scatter_indices, %update) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %arg1) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0) : (tensor<i32>) -> () }) { scatter_dimension_numbers = #stablehlo.scatter< update_window_dims = [2,3], inserted_window_dims = [0], scatter_dims_to_operand_dims = [1, 0], index_vector_dim = 2>, indices_are_sorted = false, unique_indices = false } : (tensor<3x4x2xi32>, tensor<2x3x2xi64>, tensor<2x3x2x2xi32>) -> tensor<3x4x2xi32> // %result: [ // [[1, 2], [5, 6], [8, 9], [8, 9]], // [[10, 11], [12, 13], [14, 15], [16, 17]], // [[18, 19], [20, 21], [21, 22], [23, 24]] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehloselect","text":"","title":"stablehlo.select"},{"location":"spec/#semantics_75","text":"Produces a result tensor where each element is selected from on_true or on_false tensor based on the value of the corresponding element of pred . More formally, result[i0, ..., iR-1] = pred_val ? on_true[i0, ..., iR-1] : on_false[i0, ..., iR-1] , where pred_val = rank(pred) == 0 ? pred : pred[i0, ..., iR-1] .","title":"Semantics"},{"location":"spec/#inputs_71","text":"Name Type pred tensor of type i1 on_true tensor of any supported type on_false tensor of any supported type","title":"Inputs"},{"location":"spec/#outputs_75","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_70","text":"(C1) Either rank(pred) \\(=\\) 0 or shape(pred) \\(=\\) shape(on_true) . (C2) on_true , on_false and result have same type.","title":"Constraints"},{"location":"spec/#examples_75","text":"// %pred: [[false, true], [true, false]] // %on_true: [[1, 2], [3, 4]] // %on_false: [[5, 6], [7, 8]] %result = \"stablehlo.select\"(%pred, %on_true, %on_false) : (tensor<2x2xi1>, tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[5, 2], [3, 8]] Back to Ops","title":"Examples"},{"location":"spec/#stablehloselect_and_scatter","text":"","title":"stablehlo.select_and_scatter"},{"location":"spec/#semantics_76","text":"Scatters the values from the source tensor using scatter based on the outcome of reduce_window of the input tensor using select and produces a result tensor. The following diagram shows how elements in result are computed from operand and source using a concrete example. More formally: selected_values = reduce_window_without_init(...) with the following inputs: inputs \\(=\\) [ operand ]. window_dimensions , window_strides , and padding which are used as is. base_dilations \\(=\\) windows_dilations \\(=\\) [1, ..., 1] . body defined as: ( tensor < E > arg0 , tensor < E > arg1 ) -> tensor < E > { return select ( arg0 , arg1 ) ? arg0 : arg1 ; } where E = element_type(operand) . where reduce_window_without_init works exactly like reduce_window , except that the schedule of the underlying reduce doesn't include init values. result[result_index] = reduce([source_values], [init_value], [0], scatter) where: source_values \\(=\\) [ source[source_index] for source_index in source_indices ]. source_indices \\(=\\) [ source_index for source_index in indices(source) if selected_index(source_index) = result_index ]. selected_index(source_index) = operand_index if selected_values[source_index] has the operand element from operand_index .","title":"Semantics"},{"location":"spec/#inputs_72","text":"Name Type Constraints operand tensor of any supported type (C1-C5), (C7), (C9), (C10-C12) source tensor of any supported type (C2), (C3) init_value 0-dimensional tensor of any supported type (C4) window_dimensions 1-dimensional tensor constant type si64 (C1), (C3), (C5), (C6) window_strides 1-dimensional tensor constant type si64 (C3), (C7), (C8) padding 2-dimensional tensor constant type si64 (C3), (C9) select function (C10) scatter function (C11)","title":"Inputs"},{"location":"spec/#outputs_76","text":"Name Type Constraints result tensor of any supported type (C12)","title":"Outputs"},{"location":"spec/#constraints_71","text":"(C1) rank( operand ) \\(=\\) size( window_dimensions ). (C2) operand and source have the same element type. (C3) shape(source) = (padded_operand_shape == 0 || window_dimensions > padded_operand_shape) ? 0 : floor((padded_operand_shape - window_dimensions) / window_strides) + 1: padded_operand_shape = padding[:, 0] + shape(operand) + padding[:, 1] . (C4) element_type( init_value ) \\(=\\) element_type( operand ). (C5) size( window_dimensions ) \\(=\\) rank( operand ). (C6) window_dimensions[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size(window_dimensions)). (C7) size( window_strides ) \\(=\\) rank( operand ). (C8) window_strides[i] \\(\\gt 0\\) for all i \\(\\in\\) [0, size(window_strides)). (C9) dim( padding , 0) \\(=\\) rank( operand ) and dim( padding , 1) = 2. (C10) select has type (tensor<E>, tensor<E>) -> tensor<i1> where E = element_type(operand) . (C11) scatter has type (tensor<E>, tensor<E>) -> tensor<E> where E = element_type(operand) . (C12) type( operand ) \\(=\\) type( result ).","title":"Constraints"},{"location":"spec/#examples_76","text":"// %operand: [[1, 5], [2, 5], [3, 6], [4, 4]] // %source: [[5, 6], [7, 8]] // %init_value: 0 %result = \"stablehlo.select_and_scatter\"(%operand, %source, %init_value) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = stablehlo.compare GE, %arg0, %arg1 : (tensor<i32>, tensor<i32>) -> tensor<i1> stablehlo.return %0 : tensor<i1> }, { ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = stablehlo.add %arg0, %arg1 : tensor<i32> stablehlo.return %0 : tensor<i32> }) { window_dimensions = dense<[3, 1]> : tensor<2xi64>, window_strides = dense<[2, 1]> : tensor<2xi64>, padding = dense<[[0, 1], [0, 0]]> : tensor<2x2xi64> } : (tensor<4x2xi32>, tensor<2x2xi32>, tensor<i32>) -> tensor<4x2xi32> // %result: [[0, 0], [0, 0], [5, 14], [7, 0]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlosend","text":"","title":"stablehlo.send"},{"location":"spec/#semantics_77","text":"Sends inputs to a channel channel_id and produces a result token. The operation takes a token and produces a token to reify its side effects as a value that other operations can take a data dependency on. If is_host_transfer is true , then the operation transfers data to the host. Otherwise, it transfers data to another device. What this means is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_73","text":"Name Type inputs variadic number of tensors of any supported type token token channel_id constant of type si64 channel_type enum of DEVICE_TO_DEVICE and DEVICE_TO_HOST is_host_transfer constant of type i1","title":"Inputs"},{"location":"spec/#outputs_77","text":"Name Type result token","title":"Outputs"},{"location":"spec/#constraints_72","text":"(C1) todo channel_type must be DEVICE_TO_HOST , if is_host_transfer \\(=\\) true , DEVICE_TO_DEVICE , otherwise.","title":"Constraints"},{"location":"spec/#examples_77","text":"%result = \"stablehlo.send\"(%operand, %token) { // channel_id = 5 : i64, // channel_type = #stablehlo<channel_type DEVICE_TO_HOST>, channel_handle = #stablehlo.channel_handle<handle = 5, type = 2>, is_host_transfer = true } : (tensor<3x4xi32>, !stablehlo.token) -> !stablehlo.token Back to Ops","title":"Examples"},{"location":"spec/#stablehloshift_left","text":"","title":"stablehlo.shift_left"},{"location":"spec/#semantics_78","text":"Performs element-wise left-shift operation on the lhs tensor by rhs number of bits and produces a result tensor.","title":"Semantics"},{"location":"spec/#inputs_74","text":"Name Type lhs tensor of integer type rhs tensor of integer type","title":"Inputs"},{"location":"spec/#outputs_78","text":"Name Type result tensor of integer type","title":"Outputs"},{"location":"spec/#constraints_73","text":"(C1) lhs , rhs , and result have the same type.","title":"Constraints"},{"location":"spec/#examples_78","text":"// %lhs: [-1, -2, 3, 4, 7, 7] // %rhs: [1, 2, 3, 6, 7, 8] %result = \"stablehlo.shift_left\"(%lhs, %rhs): (tensor<6xi8>, tensor<6xi8>) -> tensor<6xi8> // %result: [-2, -8, 24, 0, -128, 0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloshift_right_arithmetic","text":"","title":"stablehlo.shift_right_arithmetic"},{"location":"spec/#semantics_79","text":"Performs element-wise arithmetic right-shift operation on the lhs tensor by rhs number of bits and produces a result tensor.","title":"Semantics"},{"location":"spec/#inputs_75","text":"Name Type lhs tensor of integer type rhs tensor of integer type","title":"Inputs"},{"location":"spec/#outputs_79","text":"Name Type result tensor of integer type","title":"Outputs"},{"location":"spec/#constraints_74","text":"(C1) lhs , rhs , and result have the same type.","title":"Constraints"},{"location":"spec/#examples_79","text":"// %lhs: [-1, -128, -36, 5, 3, 7] // %rhs: [1, 2, 3, 2, 1, 3] %result = \"stablehlo.shift_right_arithmetic\"(%lhs, %rhs): (tensor<6xi8>, tensor<6xi8>) -> tensor<6xi8> // %result: [-1, -32, -5, 1, 1, 0] Back to Ops","title":"Examples"},{"location":"spec/#stablehloshift_right_logical","text":"","title":"stablehlo.shift_right_logical"},{"location":"spec/#semantics_80","text":"Performs element-wise logical right-shift operation on the lhs tensor by rhs number of bits and produces a result tensor.","title":"Semantics"},{"location":"spec/#inputs_76","text":"Name Type lhs tensor of integer type rhs tensor of integer type","title":"Inputs"},{"location":"spec/#outputs_80","text":"Name Type result tensor of integer type","title":"Outputs"},{"location":"spec/#constraints_75","text":"(C1) lhs , rhs , and result have the same type.","title":"Constraints"},{"location":"spec/#examples_80","text":"// %lhs: [-1, -128, -36, 5, 3, 7] // %rhs: [1, 2, 3, 2, 1, 3] %result = \"stablehlo.shift_right_logical\"(%lhs, %rhs): (tensor<6xi8>, tensor<6xi8>) -> tensor<6xi8> // %result: [127, 32, 27, 1, 1, 0] Back to Ops","title":"Examples"},{"location":"spec/#stablehlosign","text":"","title":"stablehlo.sign"},{"location":"spec/#semantics_81","text":"Returns the sign of the operand element-wise and produces a result tensor. More formally, for each element x , the semantics can be expressed using Python-like syntax as follows: def sign ( x ): if is_integer ( x ): if compare ( x , 0 , LT , SIGNED ): return - 1 if compare ( x , 0 , EQ , SIGNED ): return 0 if compare ( x , 0 , GT , SIGNED ): return 1 elif is_float ( x ): if x is NaN : return NaN else : if compare ( x , 0.0 , LT , FLOAT ): return - 1.0 if compare ( x , - 0.0 , EQ , FLOAT ): return - 0.0 if compare ( x , + 0.0 , EQ , FLOAT ): return + 0.0 if compare ( x , 0.0 , GT , FLOAT ): return 1.0 elif is_complex ( x ): if x . real is NaN or x . imag is NaN : return NaN else : return divide ( x , abs ( x ))","title":"Semantics"},{"location":"spec/#inputs_77","text":"Name Type operand tensor of signed integer, floating-point, or complex type","title":"Inputs"},{"location":"spec/#outputs_81","text":"Name Type result tensor of signed integer, floating-point, or complex type","title":"Outputs"},{"location":"spec/#constraints_76","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_81","text":"// Logical values: -Inf, +Inf, NaN, ... // %operand: [0xFF800000, 0x7F800000, 0x7FFFFFFF, -10.0, -0.0, 0.0, 10.0] %result = \"stablehlo.sign\"(%operand) : (tensor<7xf32>) -> tensor<7xf32> // %result: [-1.0, 1.0, 0x7FFFFFFF, -1.0, -0.0, 0.0, 1.0] Back to Ops","title":"Examples"},{"location":"spec/#stablehlosine","text":"","title":"stablehlo.sine"},{"location":"spec/#semantics_82","text":"Performs element-wise sine operation on operand tensor and produces a result tensor, implementing the sin operation from the IEEE-754 specification. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_78","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_82","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_77","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_82","text":"// %operand: [ // [0.0, 1.57079632], // [0, pi/2] // [3.14159265, 4.71238898] // [pi, 3pi/2] // ] %result = \"stablehlo.sine\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 1.0], [0.0, -1.0]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehloslice","text":"","title":"stablehlo.slice"},{"location":"spec/#semantics_83","text":"Extracts a slice from the operand using statically-computed starting indices and produces a result tensor. start_indices contain the starting indices of the slice for each dimension, limit_indices contain the ending indices (exclusive) for the slice for each dimension, and strides contain the strides for each dimension. More formally, result[i0, ..., iR-1] = operand[j0, ..., jR-1] where jd = start_indices[d] + id * strides[d] .","title":"Semantics"},{"location":"spec/#inputs_79","text":"Name Type operand tensor of any supported type start_indices 1-dimensional tensor constant of type si64 limit_indices 1-dimensional tensor constant of type si64 strides 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_83","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_78","text":"(C1) operand and result have the same element type. (C2) size( start_indices ) = size( limit_indices ) = size( strides ) = rank( operand ). (C3) 0 \\(\\le\\) start_indices[d] \\(\\le\\) limit_indices[d] \\(\\le\\) dim(operand, d) for all dimension d . (C4) 0 \\(\\lt\\) strides[d] for all dimension d . (C5) dim(result, d) = \\(\\lceil\\) (limit_indices[d]-start_indices[d])/stride[d] \\(\\rceil\\) for all dimension d in operand .","title":"Constraints"},{"location":"spec/#examples_83","text":"// 1-dimensional slice // %operand: [0, 1, 2, 3, 4] %result = \"stablehlo.slice\"(%operand) { start_indices = dense<2> : tensor<1xi64>, limit_indices = dense<4> : tensor<1xi64>, strides = dense<1> : tensor<1xi64> } : (tensor<5xi64>) -> tensor<2xi64> // %result: [2, 3] // 2-dimensional slice // %operand: [ // [0, 0, 0, 0], // [0, 0, 1, 1], // [0, 0, 1, 1] // ] %result = \"stablehlo.slice\"(%operand) { start_indices = dense<[1, 2]> : tensor<2xi64>, limit_indices = dense<[3, 4]> : tensor<2xi64>, strides = dense<1> : tensor<2xi64> } : (tensor<3x4xi64>) -> tensor<2x2xi64> // % result: [ // [1, 1], // [1, 1] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlosort","text":"","title":"stablehlo.sort"},{"location":"spec/#semantics_84","text":"Sorts a variadic number of tensors in inputs together, according to a custom comparator , along the given dimension and produces a variadic number of tensors as results . If is_stable is true, then the sorting is stable, that is, relative order of elements considered to be equal by the comparator is preserved. Two elements e1 and e2 are considered to be equal by the comparator if and only if comparator(e1, e2) = comparator(e2, e1) = false . More formally, for all 0 <= id < jd < dim(inputs[0], d) , either compare_i_j = compare_j_i = false or compare_i_j = true , where: compare_i_j \\(=\\) comparator(inputs[0][i], inputs[0][j], inputs[1][i], inputs[1][j], ...) . For all indices i = [i0, ..., iR-1] and j = [j0, ..., jR-1] . Where i \\(=\\) j everywhere except for the d th dimension. Where d \\(=\\) dimension >= 0 ? dimension : rank(inputs[0]) + dimension .","title":"Semantics"},{"location":"spec/#inputs_80","text":"Name Type inputs variadic number of tensors of any supported type dimension constant of type si64 is_stable constant of type i1 comparator function","title":"Inputs"},{"location":"spec/#outputs_84","text":"Name Type results variadic number of tensors of any supported type","title":"Outputs"},{"location":"spec/#constraints_79","text":"(C1) inputs have at least 1 tensor. (C2) For all i , type(inputs[i]) = type(results[i]) . (C3) All tensors in inputs and results have the same shape. (C4) -R \\(\\le\\) dimension \\(\\lt\\) R , where R is rank of inputs[0] . (C5) comparator has type (tensor<E1>, tensor<E1>, ..., tensor<EN-1>, tensor<EN-1>) -> tensor<i1> , where Ei is element type of inputs[i] .","title":"Constraints"},{"location":"spec/#examples_84","text":"// Sort along dimension 0 // %input0 = [[1, 2, 3], [3, 2, 1]] // %input1 = [[3, 2, 1], [1, 2, 3]] %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<i32>): %predicate = \"stablehlo.compare\"(%arg0, %arg1) { comparison_direction = #stablehlo<comparison_direction GT> } : (tensor<i32>, tensor<i32>) -> tensor<i1> \"stablehlo.return\"(%predicate) : (tensor<i1>) -> () }) { dimension = 0 : i64, is_stable = true } : (tensor<2x3xi32>, tensor<2x3xi32>) -> (tensor<2x3xi32>, tensor<2x3xi32>) // %result0 = [[3, 2, 3], [1, 2, 1]] // %result1 = [[1, 2, 1], [3, 2, 3]] // Sort along dimension 1 // %input0 = [[1, 2, 3], [3, 2, 1]] // %input1 = [[3, 2, 1], [1, 2, 3]] %result0, %result1 = \"stablehlo.sort\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<i32>): %predicate = \"stablehlo.compare\"(%arg0, %arg1) { comparison_direction = #stablehlo<comparison_direction GT> } : (tensor<i32>, tensor<i32>) -> tensor<i1> \"stablehlo.return\"(%predicate) : (tensor<i1>) -> () }) { dimension = 1 : i64, is_stable = true } : (tensor<2x3xi32>, tensor<2x3xi32>) -> (tensor<2x3xi32>, tensor<2x3xi32>) // %result0 = [[3, 2, 1], [3, 2, 1]] // %result1 = [[1, 2, 3], [1, 2, 3]] Back to Ops","title":"Examples"},{"location":"spec/#stablehlosqrt","text":"","title":"stablehlo.sqrt"},{"location":"spec/#semantics_85","text":"Performs element-wise square root operation on operand tensor and produces a result tensor, implementing the squareRoot operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_81","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_85","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_80","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_85","text":"// %operand: [[0.0, 1.0], [4.0, 9.0]] %result = \"stablehlo.sqrt\"(%operand) : (tensor<2x2xf32>) -> tensor<2x2xf32> // %result: [[0.0, 1.0], [2.0, 3.0]] // %operand: [(1.0, 2.0)] %result = \"stablehlo.sqrt\"(%operand) : (tensor<complex<f32>>) -> tensor<complex<f32>> // %result: [(1.27201965, 0.78615138)] Back to Ops","title":"Examples"},{"location":"spec/#stablehlosubtract","text":"","title":"stablehlo.subtract"},{"location":"spec/#semantics_86","text":"Performs element-wise subtraction of two tensors lhs and rhs and produces a result tensor. For integer element types, if the element-wise difference has an unsigned/signed overflow, the result is implementation-defined and one of the following: mathematical result modulo \\(2^n\\) , where n is the bit width of the result, for unsigned overflow. For signed integer overflow, wraps the result around the representable range \\([-2^{n-1},\\ 2^{n-1} - 1]\\) . saturation to \\(2^{n-1} - 1\\) (or \\(-2^{n-1}\\) ) for signed overflow and saturation to \\(2^n - 1\\) (or \\(0\\) ) for unsigned overflow. For floating-point element types, it implements the subtraction operation from the IEEE-754 specification.","title":"Semantics"},{"location":"spec/#inputs_82","text":"Name Type lhs tensor of integer, floating-point, or complex type rhs tensor of integer, floating-point, or complex type","title":"Inputs"},{"location":"spec/#outputs_86","text":"Name Type result tensor of integer, floating-point, or complex type","title":"Outputs"},{"location":"spec/#constraints_81","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_86","text":"// %lhs: [[6, 8], [10, 12]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.subtract\"(%lhs, %rhs) : (tensor<2x2xf32>, tensor<2x2xf32>) -> (tensor<2x2xf32>) // %result: [[1, 2], [3, 4]] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlotanh","text":"","title":"stablehlo.tanh"},{"location":"spec/#semantics_87","text":"Performs element-wise tanh operation on operand tensor and produces a result tensor, implementing the tanh operation from the IEEE-754 specification. Numeric precision is implementation-defined.","title":"Semantics"},{"location":"spec/#inputs_83","text":"Name Type operand tensor of floating-point or complex type","title":"Inputs"},{"location":"spec/#outputs_87","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_82","text":"(C1) operand and result have the same type.","title":"Constraints"},{"location":"spec/#examples_87","text":"// %operand: [-1.0, 0.0, 1.0] %result = \"stablehlo.tanh\"(%operand) : (tensor<3xf32>) -> tensor<3xf32> // %result: [-0.76159416, 0.0, 0.76159416] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlotranspose","text":"","title":"stablehlo.transpose"},{"location":"spec/#semantics_88","text":"Permutes the dimensions of operand tensor using permutation and produces a result tensor. More formally, result[i0, ..., iR-1] = operand[j0, ..., jR-1] where i[d] = j[permutation[d]] .","title":"Semantics"},{"location":"spec/#inputs_84","text":"Name Type operand tensor of any supported type permutation 1-dimensional tensor constant of type si64","title":"Inputs"},{"location":"spec/#outputs_88","text":"Name Type result tensor of any supported type","title":"Outputs"},{"location":"spec/#constraints_83","text":"(C1) operand and result have the same element type. (C2) permutation is a permutation of [0, 1, ..., R-1] where R is the rank of operand . (C3) For all dimensions i in operand , dim(operand, i) = dim(result, j) where j = permutation[i] .","title":"Constraints"},{"location":"spec/#examples_88","text":"// %operand: [ // [[1,2], [3,4], [5,6]], // [[7,8], [9,10], [11,12]] // ] %result = \"stablehlo.transpose\"(%operand) { permutation = dense<[2, 1, 0]> : tensor<3xi64> } : (tensor<2x3x2xi32>) -> tensor<2x3x2xi32> // %result: [ // [[1,7], [3,9], [5,11]], // [[2,8], [4,10], [6,12]] // ] More Examples Back to Ops","title":"Examples"},{"location":"spec/#stablehlotriangular_solve","text":"","title":"stablehlo.triangular_solve"},{"location":"spec/#semantics_89","text":"Solves batches of systems of linear equations with lower or upper triangular coefficient matrices. More formally, given a and b , result[i0, ..., iR-3, :, :] is the solution to op(a[i0, ..., iR-3, :, :]) * x = b[i0, ..., iR-3, :, :] when left_side is true or x * op(a[i0, ..., iR-3, :, :]) = b[i0, ..., iR-3, :, :] when left_side is false , solving for the variable x where op(a) is determined by transpose_a , which can be one of the following: NO_TRANSPOSE : Perform operation using a as-is. TRANSPOSE : Perform operation on transpose of a . ADJOINT : Perform operation on conjugate transpose of a . Input data is read only from the lower triangle of a , if lower is true or upper triangle of a , otherwise. Output data is returned in the same triangle; the values in the other triangle are implementation-defined. If unit_diagonal is true, then the implementation can assume that the diagonal elements of a are equal to 1, otherwise the behavior is undefined.","title":"Semantics"},{"location":"spec/#inputs_85","text":"Name Type a tensor of floating-point or complex type b tensor of floating-point or complex type left_side constant of type i1 lower constant of type i1 unit_diagonal constant of type i1 transpose_a enum of NO_TRANSPOSE , TRANSPOSE , and ADJOINT","title":"Inputs"},{"location":"spec/#outputs_89","text":"Name Type result tensor of floating-point or complex type","title":"Outputs"},{"location":"spec/#constraints_84","text":"(C1) a and b have the same element type (C2) rank( a ) \\(=\\) rank( b ) \\(\\ge\\) 2. (C3) The relationship between shape( a ) and shape( b ) is as follows: For all i \\(\\in\\) [0, R-3], dim( a , i ) \\(=\\) dim( b , i ). dim(a, R-2) \\(=\\) dim(a, R-1) \\(=\\) dim(b, left_side ? R-2 : R-1) . (C4) b and result have the same type.","title":"Constraints"},{"location":"spec/#examples_89","text":"// %a = [ // [1.0, 0.0, 0.0], // [2.0, 4.0, 0.0], // [3.0, 5.0, 6.0] // ] // %b = [ // [2.0, 0.0, 0.0], // [4.0, 8.0, 0.0], // [6.0, 10.0, 12.0] // ] %result = \"stablehlo.triangular_solve\"(%a, %b) { left_side = true, lower = true, unit_diagonal = false, transpose_a = #stablehlo<transpose NO_TRANSPOSE> } : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<3x3xf32> // %result: [ // [2.0, 0.0, 0.0], // [0.0, 2.0, 0.0], // [0.0, 0.0, 2.0] // ] Back to Ops","title":"Examples"},{"location":"spec/#stablehlotuple","text":"","title":"stablehlo.tuple"},{"location":"spec/#semantics_90","text":"Produces a result tuple from values val .","title":"Semantics"},{"location":"spec/#inputs_86","text":"Name Type val variadic number of values of any supported type","title":"Inputs"},{"location":"spec/#outputs_90","text":"Name Type result tuple","title":"Outputs"},{"location":"spec/#constraints_85","text":"(C1) size( val ) \\(=\\) size( result ) \\(=\\) N. (C2) type(val[i]) \\(=\\) type(result[i]) , for all i \\(\\in\\) range [0, N).","title":"Constraints"},{"location":"spec/#examples_90","text":"// %val0: [1.0, 2.0] // %val1: (3) %result = \"stablehlo.tuple\"(%val0, %val1) : (tensor<2xf32>, tuple<tensor<i32>>) -> tuple<tensor<2xf32>, tuple<tensor<i32>>> // %result: ([1.0, 2.0], (3)) Back to Ops","title":"Examples"},{"location":"spec/#stablehlowhile","text":"","title":"stablehlo.while"},{"location":"spec/#semantics_91","text":"Produces the output from executing body function 0 or more times while the cond function outputs true . More formally, the semantics can be expressed using Python-like syntax as follows: internal_state = operands while cond ( internal_state ) == True : internal_state = body ( internal_state ) results = internal_state The behavior of an infinite loop is TBD.","title":"Semantics"},{"location":"spec/#inputs_87","text":"Name Type operands variadic number of tensors of any supported type or tokens cond function body function","title":"Inputs"},{"location":"spec/#outputs_91","text":"Name Type results variadic number of tensors of any supported type or tokens","title":"Outputs"},{"location":"spec/#constraints_86","text":"(C1) cond has type (T0, ..., TN-1) -> tensor<i1> , where Ti = type(operands[i]) . (C2) body has type (T0, ..., TN-1) -> (T0, ..., TN-1) , where Ti = type(operands[i]) . (C3) For all i , type(results[i]) = type(operands[i]) .","title":"Constraints"},{"location":"spec/#examples_91","text":"// %constant0: 1 // %input0: 0 // %input1: 10 %results:2 = \"stablehlo.while\"(%input0, %input1) ({ ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.compare\"(%arg0, %arg1) { comparison_direction = #stablehlo<comparison_direction LT> } : (tensor<i32>, tensor<i32>) -> tensor<i1> \"stablehlo.return\"(%0) : (tensor<i1>) -> () }, { ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>): %0 = \"stablehlo.add\"(%arg0, %constant0) : (tensor<i32>, tensor<i32>) -> tensor<i32> \"stablehlo.return\"(%0, %arg1) : (tensor<i32>, tensor<i32>) -> () }) : (tensor<i32>, tensor<i32>) -> (tensor<i32>, tensor<i32>) // %results#0: 10 // %results#1: 10 Back to Ops","title":"Examples"},{"location":"spec/#stablehloxor","text":"","title":"stablehlo.xor"},{"location":"spec/#semantics_92","text":"Performs element-wise bitwise XOR of two tensors lhs and rhs of integer types and produces a result tensor. For boolean tensors, it computes the logical operation.","title":"Semantics"},{"location":"spec/#inputs_88","text":"Name Type lhs tensor of integer or boolean type rhs tensor of integer or boolean type","title":"Inputs"},{"location":"spec/#outputs_92","text":"Name Type result tensor of integer or boolean type","title":"Outputs"},{"location":"spec/#constraints_87","text":"(C1) lhs , rhs and result have the same type.","title":"Constraints"},{"location":"spec/#examples_92","text":"// Bitwise operation with with integer tensors // %lhs: [[1, 2], [3, 4]] // %rhs: [[5, 6], [7, 8]] %result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor<2x2xi32>, tensor<2x2xi32>) -> tensor<2x2xi32> // %result: [[4, 4], [4, 12]] // Logical operation with with boolean tensors // %lhs: [[false, false], [true, true]] // %rhs: [[false, true], [false, true]] %result = \"stablehlo.xor\"(%lhs, %rhs) : (tensor<2x2xi1>, tensor<2x2xi1>) -> tensor<2x2xi1> // %result: [[false, true], [true, false]] Back to Ops","title":"Examples"},{"location":"spec_checklist/","text":"StableHLO Specification Checklist In this document, we summarize the guidelines for reviewing changes to the specification. At the moment, these changes typically involve checking multiple things in multiple sources, so this document summarizes them all to simplify reviews: Check that the \"Specification\" column in status.md says \"yes\". Check if the section title matches the op's mnemonic in the ODS . Check if the \"Semantics\" section matches XLA's Operation Semantics . Check whether the \"Inputs\" and \"Outputs\" sections: List the same items as the ODS. List the same items as HloInstruction::CreateFromProto . Are ordered exactly like ODS. If there are any mismatches, check that there are corresponding tickets. Check whether the \"Constraints\" section: Matches XLA's shape_inference.cc . Matches XLA's hlo_verifier.cc . Matches the ODS. Matches StablehloOps.cpp . If there are any mismatches, check that there are corresponding tickets. Link all those tickets in the spec, in locations which are as specific as possible (e.g. if a ticket is about a constraint that hasn't been implemented, link the ticket right in that constraint). If the corresponding parts of the ODS and StablehloOps.cpp match the spec, check that the \"Verification\" and \"Type Inference\" columns in status.md say \"yes\". Check whether the \"Examples\" section: Only has one example. (In the future, we'll link to more examples from the StableHLO interpreter test suite). Uses valid MLIR syntax by running stablehlo-opt on code examples. Uses generic MLIR syntax which can be obtained by running stablehlo-opt -mlir-print-op-generic (we stick to generic syntax in the spec to avoid having to change the spec on prettyprinter changes). Check that the description in op's ODS: Includes the first sentence of the spec. Then links to the corresponding section of the spec. Then uses the same example as the spec but via pretty syntax which can be obtaining by running stablehlo-opt .","title":"Checklist"},{"location":"spec_checklist/#stablehlo-specification-checklist","text":"In this document, we summarize the guidelines for reviewing changes to the specification. At the moment, these changes typically involve checking multiple things in multiple sources, so this document summarizes them all to simplify reviews: Check that the \"Specification\" column in status.md says \"yes\". Check if the section title matches the op's mnemonic in the ODS . Check if the \"Semantics\" section matches XLA's Operation Semantics . Check whether the \"Inputs\" and \"Outputs\" sections: List the same items as the ODS. List the same items as HloInstruction::CreateFromProto . Are ordered exactly like ODS. If there are any mismatches, check that there are corresponding tickets. Check whether the \"Constraints\" section: Matches XLA's shape_inference.cc . Matches XLA's hlo_verifier.cc . Matches the ODS. Matches StablehloOps.cpp . If there are any mismatches, check that there are corresponding tickets. Link all those tickets in the spec, in locations which are as specific as possible (e.g. if a ticket is about a constraint that hasn't been implemented, link the ticket right in that constraint). If the corresponding parts of the ODS and StablehloOps.cpp match the spec, check that the \"Verification\" and \"Type Inference\" columns in status.md say \"yes\". Check whether the \"Examples\" section: Only has one example. (In the future, we'll link to more examples from the StableHLO interpreter test suite). Uses valid MLIR syntax by running stablehlo-opt on code examples. Uses generic MLIR syntax which can be obtained by running stablehlo-opt -mlir-print-op-generic (we stick to generic syntax in the spec to avoid having to change the spec on prettyprinter changes). Check that the description in op's ODS: Includes the first sentence of the spec. Then links to the corresponding section of the spec. Then uses the same example as the spec but via pretty syntax which can be obtaining by running stablehlo-opt .","title":"StableHLO Specification Checklist"},{"location":"status/","text":"About When bootstrapping StableHLO from MHLO, we have inherited MHLO's implementation of many things, including prettyprinting, verification and shape inference. Thanks to that, we already have significant coverage of the opset, but there's still plenty to do to review the existing implementations for completeness and provide new implementations where none exist. This live document is for the developers and the users to track the progress on various aspects of the opset - specification, verification, type inference, pretty printing, interpreter, etc. How to use it The progress of a StableHLO op, as mentioned in the corresponding row, on a particular aspect, as mentioned in the corresponding column, is tracked using one of the following tracking labels. Generic labels yes : there is a comprehensive implementation. no : there is no implementation, but working on that is part of the roadmap . Note that Verifier can never be labeled as \"no\" because the ODS already implements some verification. Customized labels for Verifier and Type Inference yes : there is an implementation, and it's in sync with StableHLO semantics . yes* : there is an implementation, and it's in sync with XLA semantics . Since XLA semantics is oftentimes underdocumented, we are using hlo_verifier.cc and shape_inference.cc as the reference. revisit : there is an implementation, but it doesn't fall under \"yes\" or \"yes*\" - either because we haven't audited it yet, or because we have and found issues. infeasible : there is no implementation, because it's infeasible. For example, because the result type of an op cannot be inferred from its operands and attributes. Status StableHLO Op Specification Verification Type Inference Pretty Printing Interpreter abs yes yes yes yes no add yes yes yes yes yes after_all yes yes yes yes no all_gather yes revisit no no no all_reduce yes revisit yes no no all_to_all yes revisit yes no no and yes yes yes yes yes atan2 yes revisit yes yes no batch_norm_grad yes revisit yes no no batch_norm_inference yes revisit yes no no batch_norm_training yes revisit yes no no bitcast_convert yes yes infeasible yes no broadcast no yes* yes* yes no broadcast_in_dim yes yes infeasible yes no case yes revisit yes no no cbrt yes revisit yes yes no ceil yes yes yes yes yes cholesky yes yes yes yes no clamp yes revisit yes yes no collective_permute yes revisit yes no no compare yes yes yes yes no complex yes yes yes yes no compute_reshape_shape no revisit no yes no concatenate yes yes yes yes no constant yes yes yes yes yes convert yes yes infeasible yes no convolution revisit yes revisit revisit no cosine yes yes yes yes yes count_leading_zeros yes yes yes yes no create_token no yes* yes* yes no cross-replica-sum no revisit yes* no no cstr_reshapable no revisit no yes no custom_call yes yes infeasible yes no divide yes yes yes yes no dot no revisit revisit yes no dot_general yes revisit revisit no no dynamic_broadcast_in_dim no revisit infeasible no no dynamic_conv no revisit no no no dynamic_gather no revisit revisit no no dynamic_iota no revisit infeasible yes no dynamic_pad no revisit no yes no dynamic_reshape no revisit infeasible yes no dynamic_slice yes revisit yes yes no dynamic_update_slice yes yes yes yes no einsum no revisit no yes no exponential yes yes yes yes no exponential_minus_one yes yes yes yes no fft yes revisit yes yes no floor yes yes yes yes yes gather yes yes yes no no get_dimension_size no yes* yes* yes no get_tuple_element yes yes yes yes no if yes revisit yes no no imag yes yes yes yes no infeed yes revisit infeasible no no iota yes yes infeasible yes yes is_finite yes yes yes yes no log yes yes yes yes no log_plus_one yes yes yes yes no logistic yes yes yes yes no map yes revisit yes no no maximum yes yes yes yes yes minimum yes yes yes yes yes multiply yes yes yes yes yes negate yes yes yes yes yes not yes yes yes yes yes optimization_barrier yes yes yes yes no or yes yes yes yes yes outfeed yes yes yes no no pad yes yes yes yes no partition_id yes yes yes yes no popcnt yes yes yes yes no power yes revisit yes yes no real yes yes yes yes no real_dynamic_slice no revisit no yes no recv yes revisit infeasible no no reduce yes revisit yes revisit no reduce_precision yes yes yes yes no reduce_scatter yes revisit no no no reduce_window yes revisit yes no no remainder yes yes yes yes no replica_id yes yes yes yes no reshape yes yes infeasible yes yes return no revisit yes yes no reverse yes revisit yes yes no rng yes yes yes yes no rng_bit_generator yes revisit infeasible yes no round_nearest_afz yes yes yes yes no round_nearest_even yes yes yes yes no rsqrt yes yes yes yes no scatter yes revisit yes no no select yes yes yes yes no select_and_scatter yes revisit yes no no send yes revisit yes no no set_dimension_size no yes* yes* yes no shift_left yes revisit yes yes no shift_right_arithmetic yes revisit yes yes no shift_right_logical yes revisit yes yes no sign yes yes yes yes no sine yes yes yes yes yes slice yes yes yes no no sort yes yes yes no no sqrt yes yes yes yes no subtract yes yes yes yes yes tanh yes yes yes yes yes torch_index_select no revisit no no no trace no revisit no yes no transpose yes yes yes yes yes triangular_solve yes revisit yes no no tuple yes yes yes yes no unary_einsum no revisit no yes no uniform_dequantize no yes* yes* yes no uniform_quantize no yes* infeasible yes no while yes revisit yes revisit no xor yes yes yes yes yes","title":"Status"},{"location":"status/#about","text":"When bootstrapping StableHLO from MHLO, we have inherited MHLO's implementation of many things, including prettyprinting, verification and shape inference. Thanks to that, we already have significant coverage of the opset, but there's still plenty to do to review the existing implementations for completeness and provide new implementations where none exist. This live document is for the developers and the users to track the progress on various aspects of the opset - specification, verification, type inference, pretty printing, interpreter, etc.","title":"About"},{"location":"status/#how-to-use-it","text":"The progress of a StableHLO op, as mentioned in the corresponding row, on a particular aspect, as mentioned in the corresponding column, is tracked using one of the following tracking labels. Generic labels yes : there is a comprehensive implementation. no : there is no implementation, but working on that is part of the roadmap . Note that Verifier can never be labeled as \"no\" because the ODS already implements some verification. Customized labels for Verifier and Type Inference yes : there is an implementation, and it's in sync with StableHLO semantics . yes* : there is an implementation, and it's in sync with XLA semantics . Since XLA semantics is oftentimes underdocumented, we are using hlo_verifier.cc and shape_inference.cc as the reference. revisit : there is an implementation, but it doesn't fall under \"yes\" or \"yes*\" - either because we haven't audited it yet, or because we have and found issues. infeasible : there is no implementation, because it's infeasible. For example, because the result type of an op cannot be inferred from its operands and attributes.","title":"How to use it"},{"location":"status/#status","text":"StableHLO Op Specification Verification Type Inference Pretty Printing Interpreter abs yes yes yes yes no add yes yes yes yes yes after_all yes yes yes yes no all_gather yes revisit no no no all_reduce yes revisit yes no no all_to_all yes revisit yes no no and yes yes yes yes yes atan2 yes revisit yes yes no batch_norm_grad yes revisit yes no no batch_norm_inference yes revisit yes no no batch_norm_training yes revisit yes no no bitcast_convert yes yes infeasible yes no broadcast no yes* yes* yes no broadcast_in_dim yes yes infeasible yes no case yes revisit yes no no cbrt yes revisit yes yes no ceil yes yes yes yes yes cholesky yes yes yes yes no clamp yes revisit yes yes no collective_permute yes revisit yes no no compare yes yes yes yes no complex yes yes yes yes no compute_reshape_shape no revisit no yes no concatenate yes yes yes yes no constant yes yes yes yes yes convert yes yes infeasible yes no convolution revisit yes revisit revisit no cosine yes yes yes yes yes count_leading_zeros yes yes yes yes no create_token no yes* yes* yes no cross-replica-sum no revisit yes* no no cstr_reshapable no revisit no yes no custom_call yes yes infeasible yes no divide yes yes yes yes no dot no revisit revisit yes no dot_general yes revisit revisit no no dynamic_broadcast_in_dim no revisit infeasible no no dynamic_conv no revisit no no no dynamic_gather no revisit revisit no no dynamic_iota no revisit infeasible yes no dynamic_pad no revisit no yes no dynamic_reshape no revisit infeasible yes no dynamic_slice yes revisit yes yes no dynamic_update_slice yes yes yes yes no einsum no revisit no yes no exponential yes yes yes yes no exponential_minus_one yes yes yes yes no fft yes revisit yes yes no floor yes yes yes yes yes gather yes yes yes no no get_dimension_size no yes* yes* yes no get_tuple_element yes yes yes yes no if yes revisit yes no no imag yes yes yes yes no infeed yes revisit infeasible no no iota yes yes infeasible yes yes is_finite yes yes yes yes no log yes yes yes yes no log_plus_one yes yes yes yes no logistic yes yes yes yes no map yes revisit yes no no maximum yes yes yes yes yes minimum yes yes yes yes yes multiply yes yes yes yes yes negate yes yes yes yes yes not yes yes yes yes yes optimization_barrier yes yes yes yes no or yes yes yes yes yes outfeed yes yes yes no no pad yes yes yes yes no partition_id yes yes yes yes no popcnt yes yes yes yes no power yes revisit yes yes no real yes yes yes yes no real_dynamic_slice no revisit no yes no recv yes revisit infeasible no no reduce yes revisit yes revisit no reduce_precision yes yes yes yes no reduce_scatter yes revisit no no no reduce_window yes revisit yes no no remainder yes yes yes yes no replica_id yes yes yes yes no reshape yes yes infeasible yes yes return no revisit yes yes no reverse yes revisit yes yes no rng yes yes yes yes no rng_bit_generator yes revisit infeasible yes no round_nearest_afz yes yes yes yes no round_nearest_even yes yes yes yes no rsqrt yes yes yes yes no scatter yes revisit yes no no select yes yes yes yes no select_and_scatter yes revisit yes no no send yes revisit yes no no set_dimension_size no yes* yes* yes no shift_left yes revisit yes yes no shift_right_arithmetic yes revisit yes yes no shift_right_logical yes revisit yes yes no sign yes yes yes yes no sine yes yes yes yes yes slice yes yes yes no no sort yes yes yes no no sqrt yes yes yes yes no subtract yes yes yes yes yes tanh yes yes yes yes yes torch_index_select no revisit no no no trace no revisit no yes no transpose yes yes yes yes yes triangular_solve yes revisit yes no no tuple yes yes yes yes no unary_einsum no revisit no yes no uniform_dequantize no yes* yes* yes no uniform_quantize no yes* infeasible yes no while yes revisit yes revisit no xor yes yes yes yes yes","title":"Status"},{"location":"type_inference/","text":"Type Inference StableHLO has been originally bootstrapped from the MHLO dialect , including inheriting the implementation of type inference. The implementation progress is tracked in status.md . To implement high-quality verifiers and shape functions for StableHLO ops, these guidelines are proposed below to follow: Proposal These proposals apply to both revisiting existing implementations, and achieving new ops until a comprehensive coverage. (P1) Use the StableHLO spec as the source of truth. The spec is the source of truth for all verifiers and shape functions of the StableHLO ops. The existing verifiers and shape functions of every op need revisited to be fully aligned with the specification. Note that the specification document keeps evolving, in cases that the spec for an op is not available, the XLA implementation should be used as the source of truth instead: including xla/service/shape_inference.cc and xla/service/hlo_verifier.cc . XLA implementation doesn't cover unbounded dynamism, so for unbounded dynamism we'll apply common sense until the dynamism RFC is available. (P2) Make the most of ODS ODS files (like StablehloOps.td ) define ops with traits and types for each operands/attributes/results and will do the verifications. Thus NO verification code needed in the verifiers or shape functions for the properties which are already guaranteed by the ODS. Remove the verification code if duplicated with ODS, as they will never be triggered. Do we need adding tests for the constraints from the ODS? Please see \u201cEstablish testing guidelines\u201d below. (P3) Maintain verification code in verifiers and shape functions Both - verifiers : implemented by Op::verify() , and - shape functions : implemented by InferTypeOpInterface like Op::inferReturnTypes() or Op::inferReturnTypeComponents may have verification code to check operands/attributes/results. An initial split would be that: let the verifiers check the operands/attributes, then let shape functions only calculate inferred result types and check the compatibility against the real result types. However, in reality this split has a few problems: The shape function can be called by the autogenerated build() functions, without calling the verifier first. So the related inputs must be verified in shape function as well. Duplicated code: for example in verifiers we do some processing on the operands then verify some intermediate results, then in shape functions these intermediate results are useful to infer the final results. These intermediate results have to be calculated twice. Maintenance burden: as verifications of an op are contained in two different methods. The solution is as follows: For most ops without regions (like PadOp ): Put all the verification code into the shape functions, and discard verifiers totally. For ops with regions (like ReduceOp/IfOp , a full list is here ): the autogenerated builders don't take regions as parameters, so if these builders involve type inference, then the shape function will be called with empty regions, for example . If the regions are not needed for type inference (like ReduceOp ), put the region related verification logic in verifiers instead of the shape functions. Duplicate some code if it is inevitable. If the regions are needed for type inference ( IfOp/CaseOp/MapOp ), then additionally the shape function must verify the regions are not empty explicitly, even though the ODS may already guarantee its existence in the Op definition. (P4) Establish testing guidelines Do we need to add/maintain tests for verifications that are covered by ODS? We do not. The tests should focus on the verifiers and shape functions, while changes to ODS need a revisit of this op. But stay careful about the missing pieces: for example, if the op contains the trait SameOperandsAndResultShape which checks only shapes but not element type, then the verification for element types of operands/results still need tests. Where do we put tests for verifiers and type inference? ops_stablehlo.mlir contains the positive cases of ops, and (at least) 1 negative test for every verification error. It is also able to check the inferred return type is compatible (not the same!) as the real result type. infer_stablehlo.mlir verifies the existence of the shape function of an op by line with hlo_test_infer.get_return_type_components\"(%x):... and the inferred type matches exactly as expected. One positive test per op in general. What to do When implementing or revisiting the verifier and/or shape function of an op: 1. Put all positive cases and negative cases in ops_stablehlo.mlir . 2. Add a single positive test in infer_stablehlo.mlir to test the interface. 3. (Optional) If an op is complicated and could contain a lot of tests, consider adding a separate test file named verify_<op_name>.mlir or verify_<your_topic>.mlir within the same folder. Note: For now, the tests for new bounded dynamism / sparsity are also put in infer_stablehlo.mlir .","title":"Type inference"},{"location":"type_inference/#type-inference","text":"StableHLO has been originally bootstrapped from the MHLO dialect , including inheriting the implementation of type inference. The implementation progress is tracked in status.md . To implement high-quality verifiers and shape functions for StableHLO ops, these guidelines are proposed below to follow:","title":"Type Inference"},{"location":"type_inference/#proposal","text":"These proposals apply to both revisiting existing implementations, and achieving new ops until a comprehensive coverage.","title":"Proposal"},{"location":"type_inference/#p1-use-the-stablehlo-spec-as-the-source-of-truth","text":"The spec is the source of truth for all verifiers and shape functions of the StableHLO ops. The existing verifiers and shape functions of every op need revisited to be fully aligned with the specification. Note that the specification document keeps evolving, in cases that the spec for an op is not available, the XLA implementation should be used as the source of truth instead: including xla/service/shape_inference.cc and xla/service/hlo_verifier.cc . XLA implementation doesn't cover unbounded dynamism, so for unbounded dynamism we'll apply common sense until the dynamism RFC is available.","title":"(P1) Use the StableHLO spec as the source of truth."},{"location":"type_inference/#p2-make-the-most-of-ods","text":"ODS files (like StablehloOps.td ) define ops with traits and types for each operands/attributes/results and will do the verifications. Thus NO verification code needed in the verifiers or shape functions for the properties which are already guaranteed by the ODS. Remove the verification code if duplicated with ODS, as they will never be triggered. Do we need adding tests for the constraints from the ODS? Please see \u201cEstablish testing guidelines\u201d below.","title":"(P2) Make the most of ODS"},{"location":"type_inference/#p3-maintain-verification-code-in-verifiers-and-shape-functions","text":"Both - verifiers : implemented by Op::verify() , and - shape functions : implemented by InferTypeOpInterface like Op::inferReturnTypes() or Op::inferReturnTypeComponents may have verification code to check operands/attributes/results. An initial split would be that: let the verifiers check the operands/attributes, then let shape functions only calculate inferred result types and check the compatibility against the real result types. However, in reality this split has a few problems: The shape function can be called by the autogenerated build() functions, without calling the verifier first. So the related inputs must be verified in shape function as well. Duplicated code: for example in verifiers we do some processing on the operands then verify some intermediate results, then in shape functions these intermediate results are useful to infer the final results. These intermediate results have to be calculated twice. Maintenance burden: as verifications of an op are contained in two different methods. The solution is as follows: For most ops without regions (like PadOp ): Put all the verification code into the shape functions, and discard verifiers totally. For ops with regions (like ReduceOp/IfOp , a full list is here ): the autogenerated builders don't take regions as parameters, so if these builders involve type inference, then the shape function will be called with empty regions, for example . If the regions are not needed for type inference (like ReduceOp ), put the region related verification logic in verifiers instead of the shape functions. Duplicate some code if it is inevitable. If the regions are needed for type inference ( IfOp/CaseOp/MapOp ), then additionally the shape function must verify the regions are not empty explicitly, even though the ODS may already guarantee its existence in the Op definition.","title":"(P3) Maintain verification code in verifiers and shape functions"},{"location":"type_inference/#p4-establish-testing-guidelines","text":"Do we need to add/maintain tests for verifications that are covered by ODS? We do not. The tests should focus on the verifiers and shape functions, while changes to ODS need a revisit of this op. But stay careful about the missing pieces: for example, if the op contains the trait SameOperandsAndResultShape which checks only shapes but not element type, then the verification for element types of operands/results still need tests. Where do we put tests for verifiers and type inference? ops_stablehlo.mlir contains the positive cases of ops, and (at least) 1 negative test for every verification error. It is also able to check the inferred return type is compatible (not the same!) as the real result type. infer_stablehlo.mlir verifies the existence of the shape function of an op by line with hlo_test_infer.get_return_type_components\"(%x):... and the inferred type matches exactly as expected. One positive test per op in general.","title":"(P4) Establish testing guidelines"},{"location":"type_inference/#what-to-do","text":"When implementing or revisiting the verifier and/or shape function of an op: 1. Put all positive cases and negative cases in ops_stablehlo.mlir . 2. Add a single positive test in infer_stablehlo.mlir to test the interface. 3. (Optional) If an op is complicated and could contain a lot of tests, consider adding a separate test file named verify_<op_name>.mlir or verify_<your_topic>.mlir within the same folder. Note: For now, the tests for new bounded dynamism / sparsity are also put in infer_stablehlo.mlir .","title":"What to do"}]}